{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for vast ai - enter in terminal\n",
    "!python3 -m pip install ipykernel -U --user --force-reinstall && apt update && apt install -y python3-pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip3 install llama-index llama-parse llama-index-embeddings-huggingface llama-index-llms-huggingface dspy-ai openpyxl langchain chromadb\n",
    "!pip3 install sentencepiece protobuf evaluate rouge_score absl-py tensorboardX bitsandbytes peft accelerate\n",
    "!cp /workspace/repos/agentic-ai/MASTER\\ -\\ PYTHON\\ -\\ SCORING\\ MODEL\\ -\\ MCG\\ MADISON\\ RIDGE\\ DST\\ -\\ v2.0.xlsx /workspace/data\n",
    "!cp /workspace/repos/agentic-ai/PPM\\ -\\ MCG\\ MADISON\\ RIDGE\\ DST.pdf /workspace/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip3 uninstall -y torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "import dspy\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.datasets.hotpotqa import HotPotQA\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, StorageContext\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from llama_index.readers.file import PandasExcelReader\n",
    "CHROMA_COLLECTION_NAME = \"blockchain_and_ai\"\n",
    "CHROMADB_DIR = \"/workspace/data/db/\"\n",
    "\n",
    "from typing import List, Any, Callable, Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from dspy.retrieve.chromadb_rm import ChromadbRM\n",
    "\n",
    "from train_utils import get_csv_string, randomize_row_values, operators_dict, range_description_json, split_df_by_empty_columns, split_df_by_empty_rows, print_trainable_parameters\n",
    "from models import SpreadSheetAnalyzer\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "filepath = \"/workspace/data/MASTER - PYTHON - SCORING MODEL - MCG MADISON RIDGE DST - v2.0.xlsx\"\n",
    "disposition_inputs = [\n",
    "  \"Selling Costs\",\n",
    "  \"Disposition Fee\",\n",
    "  \"Net Operating Income\",\n",
    "  \"Loan Assumption/Payoff\",\n",
    "  \"Return of Forecasted Reserves\",\n",
    "  \"CF Y 11\",\n",
    "  \"Return of Maximum Offering Amount\",\n",
    "  \"Projected Terminal Cap Rate\",\n",
    "  \"Cash Flows\"\n",
    "]\n",
    "dfs = pd.read_excel(filepath, sheet_name=\"5 - Disposition Analysis\", header=None)\n",
    "# Splitting the DataFrame by empty columns\n",
    "sub_dfs_by_columns = split_df_by_empty_columns(dfs)\n",
    "\n",
    "# Splitting each sub-DataFrame by empty rows\n",
    "final_split_dfs = []\n",
    "for sub_df in sub_dfs_by_columns:\n",
    "    split_sub_dfs = split_df_by_empty_rows(sub_df)\n",
    "    final_split_dfs.extend([get_csv_string(x) for x in split_sub_dfs if not x.empty])\n",
    "\n",
    "dfs.dropna(axis=0, how='all', inplace=True)\n",
    "dfs.dropna(axis=1, how='all', inplace=True)\n",
    "fee_columns = ['Disposition Fee', 'Selling Costs']\n",
    "cashflow_columns = [1,2,3,4,5,6,7,8,9]\n",
    "ground_truth = dfs[dfs[1].isin(disposition_inputs+cashflow_columns)].iloc[:, :2] # Get only the necessary columns\n",
    "ground_truth.drop(labels=[16, 17], axis=0, inplace=True) # drop the duplicate Selling and Disposition Costs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first model load...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting model...\n",
      "reloading model...\n"
     ]
    }
   ],
   "source": [
    "access_token = os.getenv('HF_TOKEN')\n",
    "llama_api_key = os.getenv('LLAMA_API_KEY')\n",
    "\n",
    "print('first model load...')\n",
    "# model_name = \"EleutherAI/gpt-neo-125m\"\n",
    "# model_name = \"microsoft/Phi-3-mini-128k-instruct\" # 128K context window\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\" # 8K context window\n",
    "# model_name = \"clibrain/mamba-2.8b-instruct-openhermes\" # 8K context window\n",
    "model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.3\" # 32K context window\n",
    "llm = dspy.HFModel(model=model_name, hf_device_map='auto', token=access_token)\n",
    "llm.kwargs['max_new_tokens']=30\n",
    "# llm.kwargs['repetition_penalty']=1.1\n",
    "llm.kwargs['temperature']=None\n",
    "llm.kwargs['do_sample']=False\n",
    "llm.kwargs['top_k']=None\n",
    "# llm.kwargs['typical_p']=0.9\n",
    "\n",
    "print('deleting model...')\n",
    "llm.model=None\n",
    "gc.collect()\n",
    "print('reloading model...')\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "llm.model=AutoModelForCausalLM.from_pretrained(model_name, quantization_config=None, \n",
    "                                               trust_remote_code=True, device_map=\"auto\", \n",
    "                                               attn_implementation=\"flash_attention_2\",  \n",
    "                                               torch_dtype=torch.bfloat16)\n",
    "\n",
    "\n",
    "# from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=32,\n",
    "#     target_modules=[\"k_proj\", \"v_proj\", \"q_proj\", \"o_proj\"], # Mistral param names\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\", #\"none\", \"all\", \"lora_only\"\n",
    "#     task_type=\"CAUSAL_LM\", \n",
    "    \n",
    "# )\n",
    "\n",
    "# llm.model = prepare_model_for_kbit_training(llm.model)\n",
    "# llm.model = get_peft_model(llm.model, config)\n",
    "# print_trainable_parameters(llm.model)\n",
    "\n",
    "if model_name == 'mistralai/Mistral-7B-Instruct-v0.3':\n",
    "    llm.model.generation_config.pad_token_id = llm.tokenizer.eos_token_id\n",
    "    llm.tokenizer.pad_token_id = llm.tokenizer.eos_token_id\n",
    "\n",
    "# dspy.settings.configure(lm=llm)\n",
    "\n",
    "######## RAG model\n",
    "# chroma_client = chromadb.PersistentClient(path=CHROMADB_DIR)\n",
    "# collection = chroma_client.get_or_create_collection(name=CHROMA_COLLECTION_NAME)\n",
    "# # text_splitter = SentenceTransformersTokenTextSplitter(tokens_per_chunk=100)\n",
    "\n",
    "# ids = []\n",
    "# documents = []\n",
    "# metadatas = []\n",
    "# # dfs_str = get_csv_string(dfs)\n",
    "# # chunks = text_splitter.create_documents([dfs_str], )\n",
    "# for chunk_no, chunk in enumerate(final_split_dfs):\n",
    "#     ids.append(f\"{chunk_no}\")\n",
    "#     documents.append(chunk)\n",
    "#     # metadatas.append({\"title\":})\n",
    "# if ids:\n",
    "#     collection.upsert(ids=ids, documents=documents)#, metadatas=metadatas)\n",
    "\n",
    "# retriever = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "# default_ef = embedding_functions.HuggingFaceEmbeddingFunction(model_name='colbert-ir/colbertv2.0', api_key=access_token)\n",
    "# default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "# retriever = ChromadbRM(CHROMA_COLLECTION_NAME, CHROMADB_DIR, default_ef, k=3)\n",
    "\n",
    "# dspy.settings.configure(lm=llm, rm=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.settings.configure(lm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = os.getenv('HF_TOKEN')\n",
    "llama_api_key = os.getenv('LLAMA_API_KEY')\n",
    "parser = LlamaParse(\n",
    "    api_key=llama_api_key,\n",
    "        result_type=\"text\",\n",
    "        language=\"en\",\n",
    "        varbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!rm -rf /root/.cache/huggingface/hub/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip3 install llama-index-embeddings-text-embeddings-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents created\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "TRY LlamaIndexRM\n",
    "\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.3\" # 32K context window\n",
    "# model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "# tokenizer_name = model_name\n",
    "# rm_llm = HuggingFaceLLM(model=dspy_llm.llm, tokenizer_name=tokenizer_name, is_chat_model=True, device_map='auto', max_new_tokens=50, context_window=8000)\n",
    "# rm_llm = HuggingFaceLLM(model_name=model_name, tokenizer_name=tokenizer_name, is_chat_model=True, device_map='auto', max_new_tokens=50, context_window=8000)\n",
    "filepath = \"/workspace/data/MASTER - PYTHON - SCORING MODEL - MCG MADISON RIDGE DST - v2.0.xlsx\"\n",
    "documents = PandasExcelReader(sheet_name=\"5 - Disposition Analysis\").load_data(filepath)\n",
    "# documents = parser.load_data(\"/workspace/data/PPM - MCG MADISON RIDGE DST.pdf\")\n",
    "print(\"Documents created\")\n",
    "\n",
    "# Settings.llm = rm_llm\n",
    "Settings.chunk_size = 300\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "embed_model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "# embed_model_name = \"Alibaba-NLP/gte-Qwen1.5-7B-instruct\"\n",
    "embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "\n",
    "# embed_model.num_workers = 1\n",
    "\n",
    "# KeywordTableSimpleRetriever\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "index.storage_context.persist(persist_dir=\"/workspace/data/storage/alpha\")\n",
    "query_engine = index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "# Settings.embed_model = embed_model\n",
    "\n",
    "# index.set_index_id(\"vector_index\")\n",
    "# index.storage_context.persist(\"/workspace/data/storage\")\n",
    "# storage_context = StorageContext.from_defaults(persist_dir=\"/workspace/data/storage\")\n",
    "# index = load_index_from_storage(storage_context, index_id=\"vector_index\")\n",
    "# query_engine = index.as_query_engine(response_mode=\"tree_summarize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dspy_llm = DspyLlamaIndexWrapper(rm_llm, model_type='chat', max_new_tokens=30)\n",
    "dspy.settings.configure(lm=dspy_llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: Get the value for Return of Maximum Offering Amount.\n",
    "# Extracted values: Return of Maximum Offering Amount: 44386706.96773932\n",
    "# Question: What is the return on maximum offering amount? Please provide a floating point number less than zero.\n",
    "# Extracted values: Return of Maximum Offering Amount: -77670566.54709445"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_collect = {}\n",
    "for row,col in ground_truth.iterrows():\n",
    "    if isinstance(col.values[0], int):\n",
    "        name = f\"Cashflows {col.values[0]}\"\n",
    "    else:\n",
    "        name = col.values[0]\n",
    "    value = col.values[1]\n",
    "    gt_collect[name] = str(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# dfs_str = get_csv_string(dfs)\n",
    "num_rounds = 10\n",
    "train_data = []\n",
    "for _ in range(num_rounds):\n",
    "    # TODO: gradually increase n_samples, random fill in of values in range\n",
    "    # dfs_aug = randomize_row_values(dfs, ground_truth=ground_truth, n_samples=15)\n",
    "    # dfs_str = get_csv_string(dfs_aug)\n",
    "    # dfs_str = get_csv_string(dfs)\n",
    "    \n",
    "    for value_to_extract in gt_collect:\n",
    "\n",
    "        question = f\"Extract the value for the variable name '{value_to_extract}'?\"\n",
    "        answer = f\"{value_to_extract}: {gt_collect[value_to_extract]}\"\n",
    "        train_data.append(dspy.Example(question=question, answer=answer).with_inputs('question'))\n",
    "    \n",
    "random.shuffle(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_data = \"\"\n",
    "for string in final_split_dfs:\n",
    "    input_data += string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train_utils import operators_dict, range_description_json\n",
    "from models_testing import SpreadSheetAnalyzer\n",
    "spreadsheeet_ananlyst = SpreadSheetAnalyzer(range_description_json, operators_dict, query_engine=query_engine, num_passages=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using a vanilla teacher. Are you sure you want to use BootstrapFinetune without a compiled teacher?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/118 [00:00<?, ?it/s]\u001b[2m2024-07-18T08:40:26.213636Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mFailed to run or to evaluate example Example({'question': \"Extract the value for the variable name 'Return of Forecasted Reserves'?\", 'answer': 'Return of Forecasted Reserves: 0'}) (input_keys={'question'}) with <function answer_exact_match at 0x72e558b6fac0> due to 'HuggingFaceEmbedding' object has no attribute '_model'.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.teleprompt.bootstrap\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mbootstrap.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m222\u001b[0m\n",
      "\u001b[2m2024-07-18T08:40:26.215127Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mFailed to run or to evaluate example Example({'question': \"Extract the value for the variable name 'Cashflows 7'?\", 'answer': 'Cashflows 7: 3717508.8156313607'}) (input_keys={'question'}) with <function answer_exact_match at 0x72e558b6fac0> due to 'HuggingFaceEmbedding' object has no attribute '_model'.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.teleprompt.bootstrap\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mbootstrap.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m222\u001b[0m\n",
      "\u001b[2m2024-07-18T08:40:26.216372Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mFailed to run or to evaluate example Example({'question': \"Extract the value for the variable name 'Cashflows 8'?\", 'answer': 'Cashflows 8: 3767450.5411334764'}) (input_keys={'question'}) with <function answer_exact_match at 0x72e558b6fac0> due to 'HuggingFaceEmbedding' object has no attribute '_model'.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.teleprompt.bootstrap\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mbootstrap.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m222\u001b[0m\n",
      "\u001b[2m2024-07-18T08:40:26.217615Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mFailed to run or to evaluate example Example({'question': \"Extract the value for the variable name 'Return of Forecasted Reserves'?\", 'answer': 'Return of Forecasted Reserves: 0'}) (input_keys={'question'}) with <function answer_exact_match at 0x72e558b6fac0> due to 'HuggingFaceEmbedding' object has no attribute '_model'.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.teleprompt.bootstrap\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mbootstrap.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m222\u001b[0m\n",
      "  3%|▎         | 4/118 [00:00<00:00, 638.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok 1\n",
      "ok 2\n",
      "ok 3\n",
      "ok 4\n",
      "ok 4.1\n",
      "ok 4.2\n",
      "ok 4.3\n",
      "Example({'question': \"Extract the value for the variable name 'Return of Forecasted Reserves'?\", 'answer': 'Return of Forecasted Reserves: 0'}) (input_keys={'question'})\n",
      "False\n",
      "ok 1\n",
      "ok 2\n",
      "ok 3\n",
      "ok 4\n",
      "ok 4.1\n",
      "ok 4.2\n",
      "ok 4.3\n",
      "Example({'question': \"Extract the value for the variable name 'Cashflows 7'?\", 'answer': 'Cashflows 7: 3717508.8156313607'}) (input_keys={'question'})\n",
      "False\n",
      "ok 1\n",
      "ok 2\n",
      "ok 3\n",
      "ok 4\n",
      "ok 4.1\n",
      "ok 4.2\n",
      "ok 4.3\n",
      "Example({'question': \"Extract the value for the variable name 'Cashflows 8'?\", 'answer': 'Cashflows 8: 3767450.5411334764'}) (input_keys={'question'})\n",
      "False\n",
      "ok 1\n",
      "ok 2\n",
      "ok 3\n",
      "ok 4\n",
      "ok 4.1\n",
      "ok 4.2\n",
      "ok 4.3\n",
      "Example({'question': \"Extract the value for the variable name 'Return of Forecasted Reserves'?\", 'answer': 'Return of Forecasted Reserves: 0'}) (input_keys={'question'})\n",
      "False\n",
      "ok 1\n",
      "ok 2\n",
      "ok 3\n",
      "ok 4\n",
      "ok 4.1\n",
      "ok 4.2\n",
      "ok 4.3\n",
      "Example({'question': \"Extract the value for the variable name 'CF Y 11'?\", 'answer': 'CF Y 11: 3870554.3880146043'}) (input_keys={'question'})\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'HuggingFaceEmbedding' object has no attribute '_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m finetune_optimizer \u001b[38;5;241m=\u001b[39m BootstrapFinetune(metric\u001b[38;5;241m=\u001b[39mmetric)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# finetune_optimizer = BootstrapFewShot(metric=metric)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m finetune_program \u001b[38;5;241m=\u001b[39m \u001b[43mfinetune_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspreadsheeet_ananlyst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mnum_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# finetune_program = finetune_optimizer.compile(spreadsheeet_ananlyst, trainset=train_data[:22])\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# finetune_program = spreadsheeet_ananlyst\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#     p.lm = LM\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#     p.activated = False\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/dspy/teleprompt/finetune.py:93\u001b[0m, in \u001b[0;36mBootstrapFinetune.compile\u001b[0;34m(self, student, teacher, trainset, valset, target, bsize, accumsteps, lr, epochs, bf16, int8, peft, path_prefix)\u001b[0m\n\u001b[1;32m     89\u001b[0m finetune_data \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m teacher \u001b[38;5;129;01min\u001b[39;00m teachers:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# Dummy compilation to get bootstraps.\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     compiled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mteleprompter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mteacher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     multitask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultitask\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# Prepare finetune <prompt, completion> pairs.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/dspy/teleprompt/bootstrap.py:84\u001b[0m, in \u001b[0;36mBootstrapFewShot.compile\u001b[0;34m(self, student, teacher, trainset)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_student_and_teacher(student, teacher)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_predictor_mappings()\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bootstrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudent\u001b[38;5;241m.\u001b[39m_compiled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/dspy/teleprompt/bootstrap.py:154\u001b[0m, in \u001b[0;36mBootstrapFewShot._bootstrap\u001b[0;34m(self, max_bootstraps)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m example_idx \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m bootstrapped:\n\u001b[0;32m--> 154\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bootstrap_one_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mround_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    157\u001b[0m         bootstrapped[example_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/dspy/teleprompt/bootstrap.py:221\u001b[0m, in \u001b[0;36mBootstrapFewShot._bootstrap_one_example\u001b[0;34m(self, example, round_idx)\u001b[0m\n\u001b[1;32m    219\u001b[0m         current_error_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_count\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m current_error_count \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_errors:\n\u001b[0;32m--> 221\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    222\u001b[0m     dspy\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to run or to evaluate example \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m due to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/dspy/teleprompt/bootstrap.py:198\u001b[0m, in \u001b[0;36mBootstrapFewShot._bootstrap_one_example\u001b[0;34m(self, example, round_idx)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok 4.3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mprint\u001b[39m(example)\n\u001b[0;32m--> 198\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mteacher\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok 4.4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    200\u001b[0m trace \u001b[38;5;241m=\u001b[39m dsp\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mtrace\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/dspy/primitives/program.py:26\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/repos/agentic-ai/models_testing.py:89\u001b[0m, in \u001b[0;36mSpreadSheetAnalyzer.forward\u001b[0;34m(self, question, verbose)\u001b[0m\n\u001b[1;32m     86\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretriever(query_or_queries\u001b[38;5;241m=\u001b[39mretriever_question)\u001b[38;5;241m.\u001b[39mpassages\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m     retrieved_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     data \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mget_content() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m retrieved_data]\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    228\u001b[0m )\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/base/base_retriever.py:243\u001b[0m, in \u001b[0;36mBaseRetriever.retrieve\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mas_trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    240\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mRETRIEVE,\n\u001b[1;32m    241\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str},\n\u001b[1;32m    242\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m retrieve_event:\n\u001b[0;32m--> 243\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_recursive_retrieval(query_bundle, nodes)\n\u001b[1;32m    245\u001b[0m         retrieve_event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[1;32m    246\u001b[0m             payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mNODES: nodes},\n\u001b[1;32m    247\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    228\u001b[0m )\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/indices/vector_store/retrievers/retriever.py:97\u001b[0m, in \u001b[0;36mVectorIndexRetriever._retrieve\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39mis_embedding_query:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_bundle\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(query_bundle\u001b[38;5;241m.\u001b[39membedding_strs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     96\u001b[0m         query_bundle\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 97\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_agg_embedding_from_queries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m                \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_strs\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m         )\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_nodes_with_embeddings(query_bundle)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/base/embeddings/base.py:184\u001b[0m, in \u001b[0;36mBaseEmbedding.get_agg_embedding_from_queries\u001b[0;34m(self, queries, agg_fn)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_agg_embedding_from_queries\u001b[39m(\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    180\u001b[0m     queries: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    181\u001b[0m     agg_fn: Optional[Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Embedding]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    182\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Embedding:\n\u001b[1;32m    183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get aggregated embedding from multiple queries.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     query_embeddings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_query_embedding(query) \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries]\n\u001b[1;32m    185\u001b[0m     agg_fn \u001b[38;5;241m=\u001b[39m agg_fn \u001b[38;5;129;01mor\u001b[39;00m mean_agg\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m agg_fn(query_embeddings)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/base/embeddings/base.py:184\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_agg_embedding_from_queries\u001b[39m(\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    180\u001b[0m     queries: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    181\u001b[0m     agg_fn: Optional[Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Embedding]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    182\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Embedding:\n\u001b[1;32m    183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get aggregated embedding from multiple queries.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     query_embeddings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_query_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries]\n\u001b[1;32m    185\u001b[0m     agg_fn \u001b[38;5;241m=\u001b[39m agg_fn \u001b[38;5;129;01mor\u001b[39;00m mean_agg\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m agg_fn(query_embeddings)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    228\u001b[0m )\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/base/embeddings/base.py:133\u001b[0m, in \u001b[0;36mBaseEmbedding.get_query_embedding\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    125\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    126\u001b[0m     EmbeddingStartEvent(\n\u001b[1;32m    127\u001b[0m         model_dict\u001b[38;5;241m=\u001b[39mmodel_dict,\n\u001b[1;32m    128\u001b[0m     )\n\u001b[1;32m    129\u001b[0m )\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    131\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mEMBEDDING, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mSERIALIZED: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict()}\n\u001b[1;32m    132\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m--> 133\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_query_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[1;32m    136\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    137\u001b[0m             EventPayload\u001b[38;5;241m.\u001b[39mCHUNKS: [query],\n\u001b[1;32m    138\u001b[0m             EventPayload\u001b[38;5;241m.\u001b[39mEMBEDDINGS: [query_embedding],\n\u001b[1;32m    139\u001b[0m         },\n\u001b[1;32m    140\u001b[0m     )\n\u001b[1;32m    141\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    142\u001b[0m     EmbeddingEndEvent(\n\u001b[1;32m    143\u001b[0m         chunks\u001b[38;5;241m=\u001b[39m[query],\n\u001b[1;32m    144\u001b[0m         embeddings\u001b[38;5;241m=\u001b[39m[query_embedding],\n\u001b[1;32m    145\u001b[0m     )\n\u001b[1;32m    146\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/instrumentation/dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    228\u001b[0m )\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/embeddings/huggingface/base.py:135\u001b[0m, in \u001b[0;36mHuggingFaceEmbedding._get_query_embedding\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_query_embedding\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    134\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get query embedding.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/embeddings/huggingface/base.py:126\u001b[0m, in \u001b[0;36mHuggingFaceEmbedding._embed\u001b[0;34m(self, sentences, prompt_name)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Embed sentences.\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_model\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241m.\u001b[39mencode(\n\u001b[1;32m    127\u001b[0m     sentences,\n\u001b[1;32m    128\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_batch_size,\n\u001b[1;32m    129\u001b[0m     prompt_name\u001b[38;5;241m=\u001b[39mprompt_name,\n\u001b[1;32m    130\u001b[0m     normalize_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize,\n\u001b[1;32m    131\u001b[0m )\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HuggingFaceEmbedding' object has no attribute '_model'"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch, BootstrapFinetune, BootstrapFewShot\n",
    "perc_train = 0.7\n",
    "num_train = int(len(train_data) * perc_train)\n",
    "def validate_answer(pred, example, trace=None):\n",
    "    return example.answer.lower() == pred.answer.lower()\n",
    "metric = dspy.evaluate.metrics.answer_exact_match\n",
    "# metric = validate_answer\n",
    "\n",
    "#Configure model to finetune\n",
    "config = dict(target=model_name, epochs=10, bf16=True, bsize=1, accumsteps=3, lr=8e-5) #path_prefix=None\n",
    "\n",
    "#Compile program on BootstrapFinetune\n",
    "finetune_optimizer = BootstrapFinetune(metric=metric)\n",
    "# finetune_optimizer = BootstrapFewShot(metric=metric)\n",
    "\n",
    "finetune_program = finetune_optimizer.compile(spreadsheeet_ananlyst, trainset=train_data[:num_train], **config)\n",
    "# finetune_program = finetune_optimizer.compile(spreadsheeet_ananlyst, trainset=train_data[:22])\n",
    "\n",
    "# finetune_program = spreadsheeet_ananlyst\n",
    "\n",
    "# #Load program and activate model's parameters in program before evaluation\n",
    "# ckpt_path = \"saved_checkpoint_path_from_finetuning\"\n",
    "# LM = dspy.HFModel(checkpoint=ckpt_path, model=model_name)\n",
    "\n",
    "# for p in finetune_program.predictors():\n",
    "#     p.lm = LM\n",
    "#     p.activated = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load program and activate model's parameters in program before evaluation\n",
    "ckpt_path = \"/workspace/repos/finetuning_ckpts/4Q6YOISLR4V7G.all/checkpoint-35\"\n",
    "LM = dspy.HFModel(checkpoint=ckpt_path, model=model_name)\n",
    "\n",
    "for p in finetune_program.predictors():\n",
    "    p.lm = LM\n",
    "    p.activated = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perc_train = 0.7\n",
    "# num_train = int(len(train_data) * perc_train)\n",
    "# metric = dspy.evaluate.metrics.answer_exact_match\n",
    "\n",
    "scores = []\n",
    "for x in train_data[num_train:num_train+34]:\n",
    "    pred = finetune_program(**x.inputs())\n",
    "    score = metric(x, pred)\n",
    "    scores.append(score)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saved_checkpoint_path_from_finetuning = '/workspace/repos/finetuning_ckpts/NFAI903XCHAMQ.all/checkpoint-53'\n",
    "llm.model=None\n",
    "llm.model=AutoModelForCausalLM.from_pretrained(saved_checkpoint_path_from_finetuning, quantization_config=None, \n",
    "                                               trust_remote_code=True, device_map=\"auto\", \n",
    "                                               attn_implementation=\"flash_attention_2\",  \n",
    "                                               torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train_utils import operators_dict, range_description_json\n",
    "from models_testing import SpreadSheetAnalyzer\n",
    "spreadsheeet_ananlyst = SpreadSheetAnalyzer(range_description_json, operators_dict, query_engine=query_engine, num_passages=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_data = \"\"\n",
    "for string in final_split_dfs:\n",
    "    input_data += string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_str = get_csv_string(dfs)\n",
    "collection = []\n",
    "for value_to_extract in gt_collect:\n",
    "    question = f\"Extract the value for the variable name '{value_to_extract}'?\"\n",
    "    print(question)\n",
    "    pred = spreadsheeet_ananlyst(question, verbose=True)\n",
    "    collection.append((pred, f\"{value_to_extract}: {gt_collect[value_to_extract]}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in collection:\n",
    "    print(i[0].answer,\"---\", i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([i[0].answer == i[1] for i in collection])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline RAG and extractor only 0.35294117647058826\n",
    "# baseline RAG, extractor, float and format checks 0.35294117647058826"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt.signature_opt_typed import optimize_signature\n",
    "from dspy.evaluate.metrics import answer_exact_match\n",
    "from dspy.functional import TypedChainOfThought\n",
    "\n",
    "compiled_program = optimize_signature(\n",
    "    student=TypedChainOfThought(\"question -> answer\"),\n",
    "    evaluator=Evaluate(devset=devset, metric=answer_exact_match, num_threads=10, display_progress=True),\n",
    "    n_iterations=50,\n",
    ").program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch, BootstrapFinetune\n",
    "perc_train = 0.7\n",
    "num_train = int(len(train_data) * perc_train)\n",
    "metric = dspy.evaluate.metrics.answer_exact_match\n",
    "\n",
    "#Compile program on current dspy.settings.lm\n",
    "fewshot_optimizer = BootstrapFewShotWithRandomSearch(metric=metric, max_bootstrapped_demos=2, num_threads=1)\n",
    "your_dspy_program_compiled = tp.compile(spreadsheeet_ananlyst, trainset=train_data[:num_train], valset=train_data[num_train:])\n",
    "\n",
    "#Configure model to finetune\n",
    "config = dict(target=llm.model, epochs=2, bf16=True, bsize=1, accumsteps=2, lr=5e-5)\n",
    "\n",
    "#Compile program on BootstrapFinetune\n",
    "finetune_optimizer = BootstrapFinetune(metric=metric)\n",
    "finetune_program = finetune_optimizer.compile(spreadsheeet_ananlyst, trainset=some_new_dataset_for_finetuning_model, **config)\n",
    "\n",
    "finetune_program = spreadsheeet_ananlyst\n",
    "\n",
    "#Load program and activate model's parameters in program before evaluation\n",
    "ckpt_path = \"saved_checkpoint_path_from_finetuning\"\n",
    "LM = dspy.HFModel(checkpoint=ckpt_path, model=llm.model)\n",
    "\n",
    "for p in finetune_program.predictors():\n",
    "    p.lm = LM\n",
    "    p.activated = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this code implements a wrapper around the llama_index library to emulate a dspy llm\n",
    "\n",
    "this allows the llama_index library to be used in the dspy framework since dspy has limited support for LLMs\n",
    "\n",
    "This code is a slightly modified copy of dspy/dsp/modules/azure_openai.py\n",
    "\n",
    "The way this works is simply by creating a dummy openai client that wraps around any llama_index LLM object and implements .complete and .chat\n",
    "\n",
    "tested with python 3.12\n",
    "\n",
    "dspy==0.1.4\n",
    "dspy-ai==2.4.9\n",
    "llama-index==0.10.35\n",
    "llama-index-llms-openai==0.1.18\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from typing import Any, Literal\n",
    "\n",
    "from easydict import EasyDict\n",
    "from llama_index.core.base.llms.types import ChatMessage\n",
    "from llama_index.core.llms import LLM\n",
    "\n",
    "\n",
    "def LlamaIndexOpenAIClientWrapper(llm: LLM):\n",
    "    def chat(messages: list[ChatMessage], **kwargs) -> Any:\n",
    "        return llm.chat([ChatMessage(**message) for message in messages], **kwargs)\n",
    "\n",
    "    def complete(prompt: str, **kwargs) -> Any:\n",
    "        return llm.complete(prompt, **kwargs)\n",
    "\n",
    "    client = EasyDict(\n",
    "        {\n",
    "            'chat': EasyDict({'completions': EasyDict({'create': chat})}),\n",
    "            'completion': EasyDict({'create': complete}),\n",
    "            'ChatCompletion': EasyDict({'create': chat}),\n",
    "            'Completion': EasyDict({'create': complete}),\n",
    "        }\n",
    "    )\n",
    "    return client\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(message)s',\n",
    "    handlers=[logging.FileHandler('azure_openai_usage.log')],\n",
    ")\n",
    "\n",
    "import functools\n",
    "import json\n",
    "from typing import Any, Literal\n",
    "\n",
    "import backoff\n",
    "import dsp\n",
    "import openai\n",
    "from dsp.modules.cache_utils import CacheMemory, NotebookCacheMemory, cache_turn_on\n",
    "from dsp.modules.lm import LM\n",
    "\n",
    "try:\n",
    "    OPENAI_LEGACY = int(openai.version.__version__[0]) == 0\n",
    "except Exception:\n",
    "    OPENAI_LEGACY = True\n",
    "\n",
    "try:\n",
    "    import openai.error\n",
    "    from openai.openai_object import OpenAIObject\n",
    "\n",
    "    ERRORS = (\n",
    "        openai.error.RateLimitError,\n",
    "        openai.error.ServiceUnavailableError,\n",
    "        openai.error.APIError,\n",
    "    )\n",
    "except Exception:\n",
    "    ERRORS = (openai.RateLimitError, openai.APIError)\n",
    "    OpenAIObject = dict\n",
    "\n",
    "\n",
    "def backoff_hdlr(details):\n",
    "    \"\"\"Handler from https://pypi.org/project/backoff/\"\"\"\n",
    "    print(\n",
    "        'Backing off {wait:0.1f} seconds after {tries} tries ' 'calling function {target} with kwargs ' '{kwargs}'.format(**details),\n",
    "    )\n",
    "\n",
    "\n",
    "class DspyLlamaIndexWrapper(LM):\n",
    "    \"\"\"Wrapper around Azure's API for OpenAI.\n",
    "\n",
    "    Args:\n",
    "        api_base (str): Azure URL endpoint for model calling, often called 'azure_endpoint'.\n",
    "        api_version (str): Version identifier for API.\n",
    "        model (str, optional): OpenAI or Azure supported LLM model to use. Defaults to \"text-davinci-002\".\n",
    "        api_key (Optional[str], optional): API provider Authentication token. use Defaults to None.\n",
    "        model_type (Literal[\"chat\", \"text\"], optional): The type of model that was specified. Mainly to decide the optimal prompting strategy. Defaults to \"chat\".\n",
    "        **kwargs: Additional arguments to pass to the API provider.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: LLM,\n",
    "        model_type: Literal['chat', 'text'] = 'chat',\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(llm._model)\n",
    "        self.provider = 'openai'\n",
    "\n",
    "        self.llm = llm\n",
    "        self.client = LlamaIndexOpenAIClientWrapper(llm)\n",
    "        model = llm._model\n",
    "        self.model_type = model_type\n",
    "\n",
    "        # if not OPENAI_LEGACY and \"model\" not in kwargs:\n",
    "        #     if \"deployment_id\" in kwargs:\n",
    "        #         kwargs[\"model\"] = kwargs[\"deployment_id\"]\n",
    "        #         del kwargs[\"deployment_id\"]\n",
    "\n",
    "        #     if \"api_version\" in kwargs:\n",
    "        #         del kwargs[\"api_version\"]\n",
    "\n",
    "        if 'model' not in kwargs:\n",
    "            kwargs['model'] = model\n",
    "\n",
    "        self.kwargs = {\n",
    "            'temperature': 0.0,\n",
    "            'max_tokens': 150,\n",
    "            'top_p': 1,\n",
    "            'frequency_penalty': 0,\n",
    "            'presence_penalty': 0,\n",
    "            'n': 1,\n",
    "            **kwargs,\n",
    "        }  # TODO: add kwargs above for </s>\n",
    "\n",
    "        self.history: list[dict[str, Any]] = []\n",
    "\n",
    "    def _openai_client(self):\n",
    "        # if OPENAI_LEGACY:\n",
    "        #     return openai\n",
    "\n",
    "        return self.client\n",
    "\n",
    "    def log_usage(self, response):\n",
    "        \"\"\"Log the total tokens from the Azure OpenAI API response.\"\"\"\n",
    "        usage_data = response.get('usage')\n",
    "        if usage_data:\n",
    "            total_tokens = usage_data.get('total_tokens')\n",
    "            logging.info(f'{total_tokens}')\n",
    "\n",
    "    def basic_request(self, prompt: str, **kwargs):\n",
    "        raw_kwargs = kwargs\n",
    "\n",
    "        kwargs = {**self.kwargs, **kwargs}\n",
    "        if self.model_type == 'chat':\n",
    "            # caching mechanism requires hashable kwargs\n",
    "            kwargs['messages'] = [{'role': 'user', 'content': prompt}]\n",
    "            kwargs = {'stringify_request': json.dumps(kwargs)}\n",
    "            # response = chat_request(self.client, **kwargs)\n",
    "            # if OPENAI_LEGACY:\n",
    "            #     return _cached_gpt3_turbo_request_v2_wrapped(**kwargs)\n",
    "            # else:\n",
    "            return v1_chat_request(self.client, **kwargs)\n",
    "\n",
    "        else:\n",
    "            kwargs['prompt'] = prompt\n",
    "            response = self.completions_request(**kwargs)\n",
    "\n",
    "        history = {\n",
    "            'prompt': prompt,\n",
    "            'response': response,\n",
    "            'kwargs': kwargs,\n",
    "            'raw_kwargs': raw_kwargs,\n",
    "        }\n",
    "        self.history.append(history)\n",
    "\n",
    "        return response\n",
    "\n",
    "    @backoff.on_exception(\n",
    "        backoff.expo,\n",
    "        ERRORS,\n",
    "        max_time=1000,\n",
    "        on_backoff=backoff_hdlr,\n",
    "    )\n",
    "    def request(self, prompt: str, **kwargs):\n",
    "        \"\"\"Handles retrieval of GPT-3 completions whilst handling rate limiting and caching.\"\"\"\n",
    "        if 'model_type' in kwargs:\n",
    "            del kwargs['model_type']\n",
    "\n",
    "        return self.basic_request(prompt, **kwargs)\n",
    "\n",
    "    def _get_choice_text(self, choice: dict[str, Any]) -> str:\n",
    "        if self.model_type == 'chat':\n",
    "            return choice['message']['content']\n",
    "        return choice['text']\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        only_completed: bool = True,\n",
    "        return_sorted: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> list[dict[str, Any]]:\n",
    "        \"\"\"Retrieves completions from OpenAI Model.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): prompt to send to GPT-3\n",
    "            only_completed (bool, optional): return only completed responses and ignores completion due to length. Defaults to True.\n",
    "            return_sorted (bool, optional): sort the completion choices using the returned probabilities. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            list[dict[str, Any]]: list of completion choices\n",
    "        \"\"\"\n",
    "\n",
    "        assert only_completed, 'for now'\n",
    "        assert return_sorted is False, 'for now'\n",
    "\n",
    "        response = self.request(prompt, **kwargs)\n",
    "\n",
    "        try:\n",
    "            if dsp.settings.log_openai_usage:\n",
    "                self.log_usage(response)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        choices = response['choices']\n",
    "\n",
    "        completed_choices = [c for c in choices if c['finish_reason'] != 'length']\n",
    "\n",
    "        if only_completed and len(completed_choices):\n",
    "            choices = completed_choices\n",
    "\n",
    "        completions = [self._get_choice_text(c) for c in choices]\n",
    "        if return_sorted and kwargs.get('n', 1) > 1:\n",
    "            scored_completions = []\n",
    "\n",
    "            for c in choices:\n",
    "                tokens, logprobs = (\n",
    "                    c['logprobs']['tokens'],\n",
    "                    c['logprobs']['token_logprobs'],\n",
    "                )\n",
    "\n",
    "                if '<|endoftext|>' in tokens:\n",
    "                    index = tokens.index('<|endoftext|>') + 1\n",
    "                    tokens, logprobs = tokens[:index], logprobs[:index]\n",
    "\n",
    "                avglog = sum(logprobs) / len(logprobs)\n",
    "                scored_completions.append((avglog, self._get_choice_text(c)))\n",
    "\n",
    "            scored_completions = sorted(scored_completions, reverse=True)\n",
    "            completions = [c for _, c in scored_completions]\n",
    "\n",
    "        return completions\n",
    "\n",
    "    def completions_request(self, **kwargs):\n",
    "        # if OPENAI_LEGACY:\n",
    "        #     return cached_gpt3_request_v2_wrapped(**kwargs)\n",
    "        return v1_completions_request(self.client, **kwargs)\n",
    "\n",
    "\n",
    "def v1_chat_request(client, **kwargs):\n",
    "    @functools.lru_cache(maxsize=None if cache_turn_on else 0)\n",
    "    @NotebookCacheMemory.cache\n",
    "    def v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs):\n",
    "        @CacheMemory.cache\n",
    "        def v1_cached_gpt3_turbo_request_v2(**kwargs):\n",
    "            if 'stringify_request' in kwargs:\n",
    "                kwargs = json.loads(kwargs['stringify_request'])\n",
    "            return client.chat.completions.create(**kwargs)\n",
    "\n",
    "        return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
    "\n",
    "    response = v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs)\n",
    "\n",
    "    try:\n",
    "        response = response.model_dump()\n",
    "    except Exception:\n",
    "        response = response.raw\n",
    "        response['choices'] = [json.loads(x.json()) for x in response['choices']]\n",
    "        response['usage'] = json.loads(response['usage'].json())\n",
    "    return response\n",
    "\n",
    "\n",
    "def v1_completions_request(client, **kwargs):\n",
    "    @functools.lru_cache(maxsize=None if cache_turn_on else 0)\n",
    "    @NotebookCacheMemory.cache\n",
    "    def v1_cached_gpt3_request_v2_wrapped(**kwargs):\n",
    "        @CacheMemory.cache\n",
    "        def v1_cached_gpt3_request_v2(**kwargs):\n",
    "            return client.completions.create(**kwargs)\n",
    "\n",
    "        return v1_cached_gpt3_request_v2(**kwargs)\n",
    "\n",
    "    return v1_cached_gpt3_request_v2_wrapped(**kwargs).model_dump()\n",
    "\n",
    "\n",
    "## ======== test =========\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     print('Testing DspyLlamaIndexWrapper')\n",
    "#     import os\n",
    "\n",
    "#     import dspy\n",
    "#     from dspy.datasets.gsm8k import GSM8K, gsm8k_metric\n",
    "#     from llama_index.llms.openai import OpenAI\n",
    "\n",
    "#     llm = OpenAI(api_key=os.environ['OPENAI_API_KEY'], model='gpt-3.5-turbo')\n",
    "#     dspy_llm = DspyLlamaIndexWrapper(llm)\n",
    "\n",
    "#     # Load math questions from the GSM8K dataset.\n",
    "#     gsm8k = GSM8K()\n",
    "#     gsm8k_trainset, gsm8k_devset = gsm8k.train[:10], gsm8k.dev[:10]\n",
    "\n",
    "#     class CoT(dspy.Module):\n",
    "#         def __init__(self):\n",
    "#             super().__init__()\n",
    "#             self.prog = dspy.ChainOfThought('question -> answer')\n",
    "\n",
    "#         def forward(self, question):\n",
    "#             response = self.prog(question=question)\n",
    "#             return response\n",
    "\n",
    "#     ##\n",
    "\n",
    "#     dspy.settings.configure(lm=dspy_llm)\n",
    "\n",
    "#     from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "#     # Set up the optimizer: we want to \"bootstrap\" (i.e., self-generate) 4-shot examples of our CoT program.\n",
    "#     config = dict(max_bootstrapped_demos=4, max_labeled_demos=4)\n",
    "\n",
    "#     # Optimize! Use the `gsm8k_metric` here. In general, the metric is going to tell the optimizer how well it's doing.\n",
    "#     teleprompter = BootstrapFewShot(metric=gsm8k_metric, **config)\n",
    "#     optimized_cot = teleprompter.compile(CoT(), trainset=gsm8k_trainset)\n",
    "#     print(f'{optimized_cot=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this code implements a wrapper around the llama_index library to emulate a dspy llm\n",
    "\n",
    "this allows the llama_index library to be used in the dspy framework since dspy has limited support for LLMs\n",
    "\n",
    "This code is a slightly modified copy of dspy/dsp/modules/azure_openai.py\n",
    "\n",
    "The way this works is simply by creating a dummy openai client that wraps around any llama_index LLM object and implements .complete and .chat\n",
    "\n",
    "tested with python 3.12\n",
    "\n",
    "dspy==0.1.4\n",
    "dspy-ai==2.4.9\n",
    "llama-index==0.10.35\n",
    "llama-index-llms-openai==0.1.18\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from typing import Any, Literal\n",
    "\n",
    "from easydict import EasyDict\n",
    "from llama_index.core.base.llms.types import ChatMessage\n",
    "from llama_index.core.llms import LLM\n",
    "\n",
    "\n",
    "def LlamaIndexOpenAIClientWrapper(llm: LLM):\n",
    "    def chat(messages: list[ChatMessage], **kwargs) -> Any:\n",
    "        return llm.chat([ChatMessage(**message) for message in messages], **kwargs)\n",
    "\n",
    "    def complete(prompt: str, **kwargs) -> Any:\n",
    "        return llm.complete(prompt, **kwargs)\n",
    "\n",
    "    client = EasyDict(\n",
    "        {\n",
    "            'chat': EasyDict({'completions': EasyDict({'create': chat})}),\n",
    "            'completion': EasyDict({'create': complete}),\n",
    "            'ChatCompletion': EasyDict({'create': chat}),\n",
    "            'Completion': EasyDict({'create': complete}),\n",
    "        }\n",
    "    )\n",
    "    return client\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(message)s',\n",
    "    handlers=[logging.FileHandler('azure_openai_usage.log')],\n",
    ")\n",
    "\n",
    "import functools\n",
    "import json\n",
    "from typing import Any, Literal\n",
    "\n",
    "import backoff\n",
    "import dsp\n",
    "import openai\n",
    "from dsp.modules.cache_utils import CacheMemory, NotebookCacheMemory, cache_turn_on\n",
    "from dsp.modules.lm import LM\n",
    "\n",
    "try:\n",
    "    OPENAI_LEGACY = int(openai.version.__version__[0]) == 0\n",
    "except Exception:\n",
    "    OPENAI_LEGACY = True\n",
    "\n",
    "try:\n",
    "    import openai.error\n",
    "    from openai.openai_object import OpenAIObject\n",
    "\n",
    "    ERRORS = (\n",
    "        openai.error.RateLimitError,\n",
    "        openai.error.ServiceUnavailableError,\n",
    "        openai.error.APIError,\n",
    "    )\n",
    "except Exception:\n",
    "    ERRORS = (openai.RateLimitError, openai.APIError)\n",
    "    OpenAIObject = dict\n",
    "\n",
    "\n",
    "def backoff_hdlr(details):\n",
    "    \"\"\"Handler from https://pypi.org/project/backoff/\"\"\"\n",
    "    print(\n",
    "        'Backing off {wait:0.1f} seconds after {tries} tries ' 'calling function {target} with kwargs ' '{kwargs}'.format(**details),\n",
    "    )\n",
    "\n",
    "\n",
    "class DspyLlamaIndexWrapper(LM):\n",
    "    \"\"\"Wrapper around Azure's API for OpenAI.\n",
    "\n",
    "    Args:\n",
    "        api_base (str): Azure URL endpoint for model calling, often called 'azure_endpoint'.\n",
    "        api_version (str): Version identifier for API.\n",
    "        model (str, optional): OpenAI or Azure supported LLM model to use. Defaults to \"text-davinci-002\".\n",
    "        api_key (Optional[str], optional): API provider Authentication token. use Defaults to None.\n",
    "        model_type (Literal[\"chat\", \"text\"], optional): The type of model that was specified. Mainly to decide the optimal prompting strategy. Defaults to \"chat\".\n",
    "        **kwargs: Additional arguments to pass to the API provider.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: LLM,\n",
    "        model_type: Literal['chat', 'text'] = 'chat',\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(llm.model)\n",
    "        self.provider = 'openai'\n",
    "\n",
    "        self.llm = llm\n",
    "        self.client = LlamaIndexOpenAIClientWrapper(llm)\n",
    "        model = llm.model\n",
    "        self.model_type = model_type\n",
    "\n",
    "        # if not OPENAI_LEGACY and \"model\" not in kwargs:\n",
    "        #     if \"deployment_id\" in kwargs:\n",
    "        #         kwargs[\"model\"] = kwargs[\"deployment_id\"]\n",
    "        #         del kwargs[\"deployment_id\"]\n",
    "\n",
    "        #     if \"api_version\" in kwargs:\n",
    "        #         del kwargs[\"api_version\"]\n",
    "\n",
    "        if 'model' not in kwargs:\n",
    "            kwargs['model'] = model\n",
    "\n",
    "        self.kwargs = {\n",
    "            'temperature': 0.0,\n",
    "            'max_tokens': 150,\n",
    "            'top_p': 1,\n",
    "            'frequency_penalty': 0,\n",
    "            'presence_penalty': 0,\n",
    "            'n': 1,\n",
    "            **kwargs,\n",
    "        }  # TODO: add kwargs above for </s>\n",
    "\n",
    "        self.history: list[dict[str, Any]] = []\n",
    "\n",
    "    def _openai_client(self):\n",
    "        # if OPENAI_LEGACY:\n",
    "        #     return openai\n",
    "\n",
    "        return self.client\n",
    "\n",
    "    def log_usage(self, response):\n",
    "        \"\"\"Log the total tokens from the Azure OpenAI API response.\"\"\"\n",
    "        usage_data = response.get('usage')\n",
    "        if usage_data:\n",
    "            total_tokens = usage_data.get('total_tokens')\n",
    "            logging.info(f'{total_tokens}')\n",
    "\n",
    "    def basic_request(self, prompt: str, **kwargs):\n",
    "        raw_kwargs = kwargs\n",
    "\n",
    "        kwargs = {**self.kwargs, **kwargs}\n",
    "        if self.model_type == 'chat':\n",
    "            # caching mechanism requires hashable kwargs\n",
    "            kwargs['messages'] = [{'role': 'user', 'content': prompt}]\n",
    "            kwargs = {'stringify_request': json.dumps(kwargs)}\n",
    "            # response = chat_request(self.client, **kwargs)\n",
    "            # if OPENAI_LEGACY:\n",
    "            #     return _cached_gpt3_turbo_request_v2_wrapped(**kwargs)\n",
    "            # else:\n",
    "            return v1_chat_request(self.client, **kwargs)\n",
    "\n",
    "        else:\n",
    "            kwargs['prompt'] = prompt\n",
    "            response = self.completions_request(**kwargs)\n",
    "\n",
    "        history = {\n",
    "            'prompt': prompt,\n",
    "            'response': response,\n",
    "            'kwargs': kwargs,\n",
    "            'raw_kwargs': raw_kwargs,\n",
    "        }\n",
    "        self.history.append(history)\n",
    "\n",
    "        return response\n",
    "\n",
    "    @backoff.on_exception(\n",
    "        backoff.expo,\n",
    "        ERRORS,\n",
    "        max_time=1000,\n",
    "        on_backoff=backoff_hdlr,\n",
    "    )\n",
    "    def request(self, prompt: str, **kwargs):\n",
    "        \"\"\"Handles retrieval of GPT-3 completions whilst handling rate limiting and caching.\"\"\"\n",
    "        if 'model_type' in kwargs:\n",
    "            del kwargs['model_type']\n",
    "\n",
    "        return self.basic_request(prompt, **kwargs)\n",
    "\n",
    "    def _get_choice_text(self, choice: dict[str, Any]) -> str:\n",
    "        if self.model_type == 'chat':\n",
    "            return choice['message']['content']\n",
    "        return choice['text']\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        only_completed: bool = True,\n",
    "        return_sorted: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> list[dict[str, Any]]:\n",
    "        \"\"\"Retrieves completions from OpenAI Model.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): prompt to send to GPT-3\n",
    "            only_completed (bool, optional): return only completed responses and ignores completion due to length. Defaults to True.\n",
    "            return_sorted (bool, optional): sort the completion choices using the returned probabilities. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            list[dict[str, Any]]: list of completion choices\n",
    "        \"\"\"\n",
    "\n",
    "        assert only_completed, 'for now'\n",
    "        assert return_sorted is False, 'for now'\n",
    "\n",
    "        response = self.request(prompt, **kwargs)\n",
    "\n",
    "        try:\n",
    "            if dsp.settings.log_openai_usage:\n",
    "                self.log_usage(response)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        choices = response['choices']\n",
    "\n",
    "        completed_choices = [c for c in choices if c['finish_reason'] != 'length']\n",
    "\n",
    "        if only_completed and len(completed_choices):\n",
    "            choices = completed_choices\n",
    "\n",
    "        completions = [self._get_choice_text(c) for c in choices]\n",
    "        if return_sorted and kwargs.get('n', 1) > 1:\n",
    "            scored_completions = []\n",
    "\n",
    "            for c in choices:\n",
    "                tokens, logprobs = (\n",
    "                    c['logprobs']['tokens'],\n",
    "                    c['logprobs']['token_logprobs'],\n",
    "                )\n",
    "\n",
    "                if '<|endoftext|>' in tokens:\n",
    "                    index = tokens.index('<|endoftext|>') + 1\n",
    "                    tokens, logprobs = tokens[:index], logprobs[:index]\n",
    "\n",
    "                avglog = sum(logprobs) / len(logprobs)\n",
    "                scored_completions.append((avglog, self._get_choice_text(c)))\n",
    "\n",
    "            scored_completions = sorted(scored_completions, reverse=True)\n",
    "            completions = [c for _, c in scored_completions]\n",
    "\n",
    "        return completions\n",
    "\n",
    "    def completions_request(self, **kwargs):\n",
    "        # if OPENAI_LEGACY:\n",
    "        #     return cached_gpt3_request_v2_wrapped(**kwargs)\n",
    "        return v1_completions_request(self.client, **kwargs)\n",
    "\n",
    "\n",
    "def v1_chat_request(client, **kwargs):\n",
    "    @functools.lru_cache(maxsize=None if cache_turn_on else 0)\n",
    "    @NotebookCacheMemory.cache\n",
    "    def v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs):\n",
    "        @CacheMemory.cache\n",
    "        def v1_cached_gpt3_turbo_request_v2(**kwargs):\n",
    "            if 'stringify_request' in kwargs:\n",
    "                kwargs = json.loads(kwargs['stringify_request'])\n",
    "            return client.chat.completions.create(**kwargs)\n",
    "\n",
    "        return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
    "\n",
    "    response = v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs)\n",
    "\n",
    "    try:\n",
    "        response = response.model_dump()\n",
    "    except Exception:\n",
    "        response = response.raw\n",
    "        response['choices'] = [json.loads(x.json()) for x in response['choices']]\n",
    "        response['usage'] = json.loads(response['usage'].json())\n",
    "    return response\n",
    "\n",
    "\n",
    "def v1_completions_request(client, **kwargs):\n",
    "    @functools.lru_cache(maxsize=None if cache_turn_on else 0)\n",
    "    @NotebookCacheMemory.cache\n",
    "    def v1_cached_gpt3_request_v2_wrapped(**kwargs):\n",
    "        @CacheMemory.cache\n",
    "        def v1_cached_gpt3_request_v2(**kwargs):\n",
    "            return client.completions.create(**kwargs)\n",
    "\n",
    "        return v1_cached_gpt3_request_v2(**kwargs)\n",
    "\n",
    "    return v1_cached_gpt3_request_v2_wrapped(**kwargs).model_dump()\n",
    "\n",
    "\n",
    "## ======== test =========\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     print('Testing DspyLlamaIndexWrapper')\n",
    "#     import os\n",
    "\n",
    "#     import dspy\n",
    "#     from dspy.datasets.gsm8k import GSM8K, gsm8k_metric\n",
    "#     from llama_index.llms.openai import OpenAI\n",
    "\n",
    "#     llm = OpenAI(api_key=os.environ['OPENAI_API_KEY'], model='gpt-3.5-turbo')\n",
    "#     dspy_llm = DspyLlamaIndexWrapper(llm)\n",
    "\n",
    "#     # Load math questions from the GSM8K dataset.\n",
    "#     gsm8k = GSM8K()\n",
    "#     gsm8k_trainset, gsm8k_devset = gsm8k.train[:10], gsm8k.dev[:10]\n",
    "\n",
    "#     class CoT(dspy.Module):\n",
    "#         def __init__(self):\n",
    "#             super().__init__()\n",
    "#             self.prog = dspy.ChainOfThought('question -> answer')\n",
    "\n",
    "#         def forward(self, question):\n",
    "#             response = self.prog(question=question)\n",
    "#             return response\n",
    "\n",
    "#     ##\n",
    "\n",
    "#     dspy.settings.configure(lm=dspy_llm)\n",
    "\n",
    "#     from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "#     # Set up the optimizer: we want to \"bootstrap\" (i.e., self-generate) 4-shot examples of our CoT program.\n",
    "#     config = dict(max_bootstrapped_demos=4, max_labeled_demos=4)\n",
    "\n",
    "#     # Optimize! Use the `gsm8k_metric` here. In general, the metric is going to tell the optimizer how well it's doing.\n",
    "#     teleprompter = BootstrapFewShot(metric=gsm8k_metric, **config)\n",
    "#     optimized_cot = teleprompter.compile(CoT(), trainset=gsm8k_trainset)\n",
    "#     print(f'{optimized_cot=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
