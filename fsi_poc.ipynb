{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short list of objectives\n",
    "\n",
    "* Import pdf of Proposed Rule XXXXXXX\n",
    "* Query rule for baseline responses \n",
    "* Fine-tune embedding model, recheck responses\n",
    "* Summarize rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up ollama\n",
    "!apt-get update && apt-get install tmux vim -y\n",
    "!pip3 install llama-index llama-parse llama_deploy llama-index-llms-huggingface llama-index-embeddings-huggingface llama-index-llms-ollama llama-index-embeddings-ollama llama-index-vector-stores-neo4jvector llama-index-graph-stores-neo4j llama-index-finetuning llama-index-utils-workflow llama-index-readers-file\n",
    "!pip3 install sentencepiece protobuf evaluate rouge_score absl-py tensorboardX bitsandbytes peft accelerate python-dotenv graspologic fpdf2\n",
    "!pip3 install flash-attn --no-build-isolation\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Neo4j\n",
    "!apt install dialog apt-utils -y \n",
    "!wget -O - https://debian.neo4j.com/neotechnology.gpg.key | gpg --dearmor -o /etc/apt/keyrings/neotechnology.gpg\n",
    "!echo 'deb [signed-by=/etc/apt/keyrings/neotechnology.gpg] https://debian.neo4j.com stable latest' | tee -a /etc/apt/sources.list.d/neo4j.list\n",
    "!apt list -a neo4j\n",
    "!add-apt-repository universe -y\n",
    "!apt install neo4j=1:5.23.0 -y\n",
    "!echo \"neo4j-enterprise neo4j/question select I ACCEPT\" | debconf-set-selections\n",
    "!echo \"neo4j-enterprise neo4j/license note\" | debconf-set-selections\n",
    "!apt install openjdk-17-jre -y\n",
    "!cd /var/lib/neo4j/plugins/ && wget https://github.com/neo4j/apoc/releases/download/5.23.0/apoc-5.23.0-core.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up llama.cpp 22m 32s\n",
    "!cd /workspace/repos/ && git clone https://github.com/ggerganov/llama.cpp\n",
    "!cd /workspace/repos/llama.cpp && git pull && make clean && LLAMA_CUDA=0 make\n",
    "!chmod 755 /workspace/repos/llama.cpp/requirements.txt && pip3 install -r /workspace/repos/llama.cpp/requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "save_dir = \"/workspace/data/compliance/proposal_embedworking\"\n",
    "\n",
    "# tokenizer_name = \"dunzhang/stella_en_1.5B_v5\"\n",
    "tokenizer_name = \"Qwen/Qwen2-1.5B\"\n",
    "model_name = \"/workspace/data/compliance/rule_proposal_embedding_model_gpt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             return_dict=True,\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device_map=\"auto\",\n",
    "                                             )\n",
    "print('Saving model')\n",
    "model.save_pretrained(save_dir)\n",
    "print('Saving tokenizer')\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /workspace/repos/llama.cpp/convert_hf_to_gguf.py /workspace/data/compliance/proposal_embed\n",
    "!cd /workspace/data/compliance/proposal_embed && echo 'FROM \"/workspace/data/compliance/proposal_embed/Proposal_Embed-1.8B-F16.gguf\"' >> Modelfile && ollama create proposal_embed -f Modelfile\n",
    "\n",
    "    #FROM \"/workspace/data/compliance/proposal_embed/Proposal_Embed-1.8B-F16.gguf\"\n",
    "# then run in terminal\n",
    "    # ollama create proposal_embed -f Modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: notify if ollama server is running with model loaded\n",
    "import subprocess, os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.openai import OpenAI as LOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "# model_name, ctx_len = \"gpt-4o-2024-08-06\", 128000\n",
    "model_name, ctx_len = \"llama3.1:8b-instruct-q8_0\", 128000\n",
    "# model_name, ctx_len = \"solar-pro:latest\", 128000\n",
    "# model_name, ctx_len = \"hermes3:8b\", 128000\n",
    "# model_name, ctx_len = \"gemma2:latest\", 8192\n",
    "\n",
    "\n",
    "if \"gpt-4o\" in model_name:\n",
    "    openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "    \n",
    "    print(f\"Using OpenAI {model_name}...\")\n",
    "    llm = LOpenAI(model=model_name, max_tokens=8000)\n",
    "else:\n",
    "    try: \n",
    "        print(\"Pulling Ollama model...\")\n",
    "        sub_out = subprocess.run(['ollama', 'pull', model_name], capture_output=True, text=True)\n",
    "    except Exception as e: \n",
    "        print(f\"Error pulling model: Is the Ollama server running?\\n{e}\")\n",
    "    \n",
    "    addtion_kwargs = {\"max_new_tokens\": 8000}\n",
    "    # system_prompt = \"You are an expert at answering questions about rules and regulations regarding Title 17—Commodity and Securities Exchanges: CHAPTER II—SECURITIES AND EXCHANGE COMMISSION. Please provide a summary of the following text, and cite any sections, rules, acts or laws (e.g. § 230.503, § 240.13a-15, Act (15 U.S.C. 781), Investment Company Act of 1940) from context that support the answer. Be detailed in your response.\"\n",
    "    llm = Ollama(model=model_name, url=\"http://127.0.0.1:11434\", context_window=ctx_len, model_type=\"chat\", is_function_calling_model=False, \n",
    "                 request_timeout=4000.0, **addtion_kwargs) #, system_prompt=system_prompt) additional_kwargs=addtion_kwargs,\n",
    "    print(llm.metadata)\n",
    "\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEC rules and regulations\n",
    "!cd /workspace/data && curl -X GET \"https://www.ecfr.gov/api/versioner/v1/full/2024-07-23/title-17.xml?chapter=II\" -H \"accept: application/xml\" > title-17.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from sec_utils import get_tree_data\n",
    "\n",
    "# Path to your XML file\n",
    "xml_file_path = '/workspace/data/title-17.xml'\n",
    "\n",
    "# Parse the XML file\n",
    "tree = ET.parse(xml_file_path)\n",
    "\n",
    "# Get the root element of the XML document\n",
    "root = tree.getroot()\n",
    "\n",
    "sec_data = get_tree_data(root)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sec_utils import get_metadata\n",
    "from llama_index.core import Document\n",
    "\n",
    "documents = [Document(text=t, \n",
    "                          text_template='{metadata_str}\\n\\n{content}',\n",
    "                          metadata=get_metadata(m, t, metadata={\"section\":None, \"description\":None, \"mentioned_sections\":None})) \\\n",
    "                            for m,t in sec_data.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from llama_index.readers.file import PDFReader\n",
    "# from llamaindex_data_utils import extract_text_from_pdf\n",
    "from llama_index.core import Document\n",
    "from utils import fix_hyphenated_words\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "access_token = os.getenv('HF_TOKEN')\n",
    "llama_api_key = os.getenv('LLAMA_API_KEY')\n",
    "\n",
    "parser = PDFReader(return_full_document=True)\n",
    "documents_proposal = parser.load_data(\"/workspace/data/compliance/FSI_Rule_proposal.pdf\")\n",
    "\n",
    "# fixes poorly formatted hyphenated words\n",
    "doc_text = documents_proposal[0].text\n",
    "documents_proposal[0].text = fix_hyphenated_words(doc_text)\n",
    "\n",
    "step=10000\n",
    "start=0\n",
    "track=[]\n",
    "for i in range(1, 18):  # Adjust the range as needed\n",
    "    # Regular expression pattern to match the specific number preceded by \\n \\n or \\n\\n and followed by a space\n",
    "    if i == 1:\n",
    "        pattern = rf'(\\n \\n|\\n\\n| \\n)({i})( )'\n",
    "    else:\n",
    "        pattern = rf'(\\n \\n|\\n\\n)({i})( )'\n",
    "    text_slice = doc_text[start:start+step]\n",
    "    # Check if the pattern exists in the doc_text\n",
    "    if re.search(pattern, text_slice):\n",
    "        # Replace the matching patterns with an empty string\n",
    "        matches = list(re.finditer(pattern, text_slice))\n",
    "        # for mat in range(len(matches)):\n",
    "        #     if start > matches[mat].span()[0]:\n",
    "        #         continue\n",
    "        #     else:\n",
    "        #         break\n",
    "        # if matches[mat].span()[0] < start:\n",
    "        #     continue\n",
    "        # else:\n",
    "        print(f\"Found {i} {matches[0].span()}\")\n",
    "        track.append(i)\n",
    "        bingo=matches[0].span()\n",
    "        text_slice = text_slice[:bingo[0]]+\"\\n\\n\"+text_slice[bingo[1]:]\n",
    "        doc_text = doc_text[:start] + text_slice + doc_text[start+step:]\n",
    "        start = bingo[1]\n",
    "    else:\n",
    "        start += step\n",
    "\n",
    "# documents_proposal[0].text = doc_text\n",
    "# pattern = r'(?<=\\n)([IVX]+\\..*?)(?=\\n[IVX]+\\.\\s|\\Z)'\n",
    "# sections=[]\n",
    "# sections.append(\"Proposal Information\"+documents_proposal[0].text.split(\"I. Background A. Sta\")[0])\n",
    "# sections += re.findall(pattern, documents_proposal[0].text, re.DOTALL)\n",
    "# section4_5split = sections[4].split(\"V. Paperwork Reduction Act\")\n",
    "# sections[4]=section4_5split[0]\n",
    "# sections = sections[:5]+[\"V. Paperwork Reduction Act\"+section4_5split[-1]]+sections[5:]\n",
    "# for shead in [x.split('\\n')[0].strip() for x in sections]:\n",
    "#     print(shead)\n",
    "\n",
    "# # pdf_urls = [\"/workspace/data/compliance/FSI_Factsheet.pdf\", \"/workspace/data/compliance/FSI_Press_Release.pdf\", \"/workspace/data/compliance/FSI_Rule_proposal.pdf\"]\n",
    "# # descriptions = [\"Factsheet about newly proposed SEC rule.\", \"Press release regarding newly proposed SEC rule.\", \"The full text of the newly proposed SEC rule.\"]\n",
    "# # documents_proposal = extract_text_from_pdf(pdf_urls, llama_api_key, llamaparse_kwargs={\"split_by_page\":False}, save_json_path=None)\n",
    "# documents_proposal = [Document(text=t, \n",
    "#                           text_template='{metadata_str}\\n\\n{content}') for t in sections]\n",
    "\n",
    "# # add metadata to the documents_proposal\n",
    "# for i in range(len(documents_proposal)):\n",
    "#     documents_proposal[i].metadata[\"section\"] = documents_proposal[i].text.split(\"\\n\")[0].strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].text[19000:23000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PDFReader(return_full_document=True)\n",
    "docs = parser.load_data(\"/workspace/data/compliance/FSI_Rule_proposal.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "test=\"\"\"Something here.\n",
    "\n",
    "5 ksjdkfjlksdjf\n",
    "\n",
    "4 financing of terrorism, and other illicit financ e activity, and to safeguard the national security of \n",
    "the United States .1  The Secretary of the Treasury (“ the Secretary”) delegated the authority to \n",
    "implement, administer, and enforce the BSA and its implementing regulations to the Director of \n",
    "FinCEN .2 \n",
    "person’s identity, including name, address, and other identifying information; and (C) consulting lists of known or suspected terrorists or terrorist organizations provided to the financial institution by any government agency to determine whether a person seeking to open an account appears on any such list.”\n",
    "3 These programs are referred to as Customer Identification Program s \n",
    " \n",
    "1   See 31 U.S.C. 5311. Certain parts of the Currency and Foreign Transactions Reporting Act, as amended , \n",
    "and other statutes relating to the subject matter of that Act, have come to be referred to as the BSA. The \n",
    "BSA is codified at 12 U.S.C. 1829b, 12 U.S.C. 1951- 1960, and 31 U.S.C. 310, 5311- 5314, 5316- 5336, \n",
    "including notes thereto, with implementing regulations at 31 CFR chapter X.  \n",
    "2 See Treasury Order 180 –01, paragraph 3(a) (Jan. 14, 2020), available at https://home.treasury.gov/about/\n",
    "general -information/orders -and-directives/treasury -order -180-01. \n",
    "3 31 U.S.C. 5318(l)(2).  \n",
    " \n",
    "5 (“CIP s”) and are long-standing, foundational components of a financial institution’s  anti-money \n",
    "laundering program.   \n",
    "As enacted , section 326 applies to all “financial institutions .”  This term is  defined \n",
    "broadly in the BSA to encompass a variety of entities, including commercial banks; agencies, \n",
    "and branches of foreign banks in the United States ; thrift  institutions , credit unions, and private \n",
    "banker s; trust companies; securities brokers and dealers registered with the Commission ; \n",
    "investment companies; futures commission merchants ; insurance companies ; travel agencies ; \n",
    "pawnbrokers; dealers in precious metals , stones, and jewels; check -cashers ; certain  casinos; and \n",
    "telegraph companies, among others.4  The BSA also grants authority to the Secretary  to define, \n",
    "by regulation, additional types of businesses as financial institutions where the Secretary  \n",
    "determines that such businesses engage in any activity “similar to, related to, or a substitute for” those in which any of the businesses listed in the statutory definition are authorized to engage.\n",
    "\n",
    "As part of the implementation, administration , and enforcement of the BSA , this authority has \n",
    "\"\"\"\n",
    "test=docs[0].text\n",
    "\n",
    "\n",
    "step=20000\n",
    "start=0\n",
    "for i in range(1, 7):  # Adjust the range as needed\n",
    "    # Regular expression pattern to match the specific number preceded by \\n \\n or \\n\\n and followed by a space\n",
    "    if i == 1:\n",
    "        pattern = rf'(\\n \\n|\\n\\n| \\n)({i})( )'\n",
    "    else:\n",
    "        pattern = rf'(\\n \\n|\\n\\n)({i})( )'\n",
    "    text_slice = test[start:start+step]\n",
    "    # Check if the pattern exists in the test\n",
    "    if re.search(pattern, text_slice):\n",
    "        print(f\"Found {i}\")\n",
    "        # Replace the matching patterns with an empty string\n",
    "        matches = list(re.finditer(pattern, text_slice))\n",
    "        # if len(matches) > 0:\n",
    "        print(matches)\n",
    "        bingo=matches[0].span()\n",
    "        text_slice = text_slice[:bingo[0]]+\"\\n\\n\"+text_slice[bingo[1]:]\n",
    "        test = test[:start] + text_slice + test[start+step:]\n",
    "        start = bingo[1]\n",
    "    else:\n",
    "        start += step\n",
    "\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = rf'(\\n \\n|\\n\\n)({1})( )'\n",
    "matches = list(re.finditer(pattern, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Distillation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "parser = SentenceSplitter(chunk_size=300, chunk_overlap=50)\n",
    "nodes = parser.get_nodes_from_documents(documents_proposal, show_progress=True)\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = 'Context information is below.\\n\\n---------------------\\n{context_str}\\n---------------------\\n\\nGiven the context information and no prior knowledge, generate {num_questions_per_chunk} questions based on the below query.\\n\\nYou are a FINRA certified Compliance Specialist that writes exams for firm compliance professionals. Your task is to write precise questions for an upcoming Compliance Officer certification examination. The questions should be diverse across the context with no multiple choice. Restrict the questions to the context information provided.\"\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.finetuning import generate_qa_embedding_pairs\n",
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import random\n",
    "\n",
    "output_path = \"/workspace/data/train_dataset_proposal.json\"\n",
    "if not os.path.exists(output_path):\n",
    "    rand_index = random.sample(range(len(nodes)), len(nodes))\n",
    "    train_perc=1.0\n",
    "    train_size = int(len(rand_index)*train_perc)\n",
    "\n",
    "    train_dataset = generate_qa_embedding_pairs(\n",
    "        qa_generate_prompt_tmpl = system_prompt,\n",
    "        num_questions_per_chunk=5,\n",
    "        save_every=20,\n",
    "        output_path=output_path,\n",
    "        llm=llm, \n",
    "        nodes=[nodes[x] for x in rand_index[:train_size]],\n",
    "        verbose=False\n",
    "    )\n",
    "    train_dataset.save_json(output_path)\n",
    "\n",
    "    # val_dataset = generate_qa_embedding_pairs(\n",
    "    #     qa_generate_prompt_tmpl = system_prompt,\n",
    "    #     num_questions_per_chunk=5,\n",
    "    #     save_every=500,\n",
    "    #     output_path=\"/workspace/data/val_dataset.json\",\n",
    "    #     llm=llm, \n",
    "    #     nodes=[nodes[x] for x in rand_index[train_size:]],\n",
    "    #     verbose=False\n",
    "    # )\n",
    "    # # assert len(val_dataset.relevant_docs) == len(val_dataset.queries)\n",
    "    # assert (np.unique(list(val_dataset.relevant_docs.values()), return_counts=True)[1]==5).all()\n",
    "    # val_dataset.save_json(\"/workspace/data/val_dataset.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#####################\n",
    "# in llama_index/embeddings/huggingface/base need to add \"show_progress_bar=False\" to 204, 213\n",
    "#####################\n",
    "\n",
    "import os, json\n",
    "\n",
    "from llama_index.finetuning import EmbeddingAdapterFinetuneEngine, SentenceTransformersFinetuneEngine\n",
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "\n",
    "train_path = \"/workspace/data/train_dataset_proposal.json\"\n",
    "val_path = None #\"/workspace/data/val_dataset.json\"\n",
    "if os.path.exists(train_path):\n",
    "    train_dataset = EmbeddingQAFinetuneDataset.from_json(train_path)\n",
    "if val_path is not None and os.path.exists(val_path):\n",
    "    val_dataset = EmbeddingQAFinetuneDataset.from_json(val_path)\n",
    "else:\n",
    "    val_dataset = None\n",
    "\n",
    "embed_model_name = \"dunzhang/stella_en_1.5B_v5\" #7b params\n",
    "print(\"loading embed model...\")\n",
    "\n",
    "finetune_engine = SentenceTransformersFinetuneEngine(\n",
    "    train_dataset,\n",
    "    embed_model_name,\n",
    "    batch_size=6,\n",
    "    model_output_path=\"/workspace/data/rule_proposal_embedding_model\",\n",
    "    val_dataset=val_dataset,\n",
    "    epochs=4,\n",
    "    show_progress_bar=True,\n",
    "    # can optionally pass along any parameters that go into `train_model`\n",
    "    # optimizer_class=torch.optim.SGD,\n",
    "    # optimizer_params={\"lr\": 0.0001, \"weight_decay\": 0.01}\n",
    ")\n",
    "\n",
    "\n",
    "# finetune_engine = EmbeddingAdapterFinetuneEngine(\n",
    "#     train_dataset,\n",
    "#     base_embed_model,\n",
    "#     batch_size=10,\n",
    "#     model_output_path=\"/workspace/data/adapter_model\",\n",
    "#     # val_dataset=val_dataset,\n",
    "#     epochs=4,\n",
    "#     verbose=False,\n",
    "#     # can optionally pass along any parameters that go into `train_model`\n",
    "#     # optimizer_class=torch.optim.SGD,\n",
    "#     # optimizer_params={\"lr\": 0.001}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_engine.finetune(**{\"lr\": 0.001, \"weight_decay\": 0.0001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "\n",
    "# Clear memory\n",
    "del finetune_engine, train_dataset\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "embed_model_name = \"dunzhang/stella_en_1.5B_v5\" #7b params\n",
    "finetuned_embed_model_name = \"/workspace/data/compliance/rule_proposal_embedding_model_gpt\"\n",
    "print(\"loading embed model...\")\n",
    "proposal_embed_model = HuggingFaceEmbedding(model_name=finetuned_embed_model_name)\n",
    "rules_embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "\n",
    "# Settings.embed_model = embed_model\n",
    "# Settings.chunk_size = 300\n",
    "# Settings.chunk_overlap = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "entities = Literal[#\"PROPOSED_RULE\", \n",
    "                   \"ACTS\",\n",
    "                   \"SECTIONS\",\n",
    "                   \"RULES\"\n",
    "                   #\"AMENDMENTS\"\n",
    "]\n",
    "\n",
    "relations = Literal[\n",
    "    \"MENTIONS\",\n",
    "    #\"CHANGES\",\n",
    "    # \"AMENDS\", \n",
    "    \"REFERS_TO\"\n",
    "]\n",
    "\n",
    "validation_schema = {\n",
    "    # \"Proposed_Rules\": [\"CHANGES\", \"AMENDS\"],\n",
    "    \"Acts\": [\"MENTIONS\",\"REFERS_TO\"],\n",
    "    \"Sections\": [\"MENTIONS\",\"REFERS_TO\"],\n",
    "    \"Rules\": [\"MENTIONS\",\"REFERS_TO\"],\n",
    "    # \"Amendments\": [\"REFERS_TO\", \"AMENDS\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_utils import set_neo4j_password, add_lines_to_conf\n",
    "set_neo4j_password('bewaretheneo')\n",
    "# add_lines_to_conf()\n",
    "\n",
    "###### START NEO4J SERVER ######\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphRAG Database\n",
    "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor, SimpleLLMPathExtractor\n",
    "from rag_utils import create_neo4j_graph_store, create_neo4j_graphrag, neo4j_query, dump_neo4j_database\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "access_token = os.getenv('HF_TOKEN')\n",
    "llama_api_key = os.getenv('LLAMA_API_KEY')\n",
    "\n",
    "graph_idx_persist_dir = \"/workspace/data/compliance/graph_idx_testfull\"\n",
    "graph_store_persist_dir= None #\"/workspace/data/graph_store\"\n",
    "\n",
    "Settings.chunk_size = 500\n",
    "Settings.chunk_overlap = 20\n",
    "\n",
    "# kg_extractor = SchemaLLMPathExtractor(\n",
    "#     llm=llm,\n",
    "#     possible_entities=entities,\n",
    "#     possible_relations=relations,\n",
    "#     kg_validation_schema=validation_schema,\n",
    "#     strict=False,  # if false, will allow triples outside of the schema``\n",
    "#     num_workers=10,\n",
    "#     max_triplets_per_chunk=10,\n",
    "# )\n",
    "\n",
    "\n",
    "# extract_prompt = \"You are an expert compliance officer with vast knowledge of SEC Title 17 Chapter II. Your job is to read each section and link mentions of other sections, rules, or acts (e.g. § 230.503, § 240.13a-15, Act (15 U.S.C. 781), Investment Company Act of 1940) mentioned. If there are no mentions of other sections, rules, or acts, return an empty list.\"\n",
    "# extract_prompt = \"You are an expert compliance officer with vast knowledge of SEC Title 17 Chapter II. Your job is to read each section and link other sections, rules, and acts mentioned. What sections, rules, and acts (e.g. § 230.503, § 240.13a-15, Act (15 U.S.C. 781), Investment Company Act of 1940) are mentioned in the content? If there are no mentions, return an empty list.\"\n",
    "llm.is_function_calling_model = False\n",
    "extract_prompt = \"You are an expert compliance officer with vast knowledge of SEC Title 17 Chapter II. Your job is to find semantic, referential, and literal relationships between the sections. If there are no relationships, return an empty list.\"\n",
    "kg_extractor = SimpleLLMPathExtractor(\n",
    "        extract_prompt=extract_prompt,\n",
    "        llm=llm,\n",
    "        max_paths_per_chunk=10,\n",
    "        num_workers=6,\n",
    "    )\n",
    "\n",
    "# random.shuffle(documents)\n",
    "\n",
    "print(\"Creating graph store...\")\n",
    "graph_store = create_neo4j_graph_store(neo_url=\"bolt://localhost:7687\", \n",
    "                                       password=os.getenv(\"NEO4J_PWD\"), \n",
    "                                       config={\"connection_timeout\": 1000, \"connection_acquisition_timeout\": 1000, \"max_connection_pool_size\": 1000})\n",
    "\n",
    "if not os.path.exists(graph_idx_persist_dir):\n",
    "    print(\"Deleting all nodes and relationships...\")\n",
    "    neo4j_query(graph_store, query=\"\"\"MATCH n=() DETACH DELETE n\"\"\")\n",
    "\n",
    "print(\"Creating graphrag index...\")\n",
    "graph_index = create_neo4j_graphrag(documents, llm, rules_embed_model, kg_extractor, graph_store, graph_idx_persist_dir=graph_idx_persist_dir, graph_store_persist_dir=graph_store_persist_dir, similarity_top_k=3)\n",
    "\n",
    "dump_neo4j_database('neo4j', '/workspace/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = graph_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_query(graph_store, query=\"\"\"MATCH n=() DETACH DELETE n\"\"\")\n",
    "graph_store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"llama2\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Database RAG\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "from rag_utils import create_llama_vector_index_rag\n",
    "\n",
    "Settings.chunk_size = 300\n",
    "Settings.chunk_overlap = 20\n",
    "vector_index = create_llama_vector_index_rag(llm, proposal_embed_model, documents=documents_proposal, persist_dir=\"/workspace/data/compliance/vector_idx_proposed\")\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    # node_postprocessors=[\n",
    "    #     LLMRerank(\n",
    "    #         choice_batch_size=5,\n",
    "    #         top_n=2,\n",
    "    #     )\n",
    "    # ],\n",
    "    # see https://github.com/run-llama/llama_index/blob/f7c5ee5efbb6172e819f26d1705fcdf6114b11a3/llama-index-core/llama_index/core/response_synthesizers/type.py#L4\n",
    "    response_mode=\"tree_summarize\", # \"accumulate\", \"compact_accumulate\", \"compact\", \"simple_summarize\", \"tree_summarize\"\n",
    ")\n",
    "\n",
    "Settings.chunk_size = 500\n",
    "Settings.chunk_overlap = 20\n",
    "rules_vector_index = create_llama_vector_index_rag(llm, rules_embed_model, documents=documents, persist_dir=\"/workspace/data/compliance/vector_idx_sec\")\n",
    "graph_index = rules_vector_index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    # node_postprocessors=[\n",
    "    #     LLMRerank(\n",
    "    #         choice_batch_size=5,\n",
    "    #         top_n=2,\n",
    "    #     )\n",
    "    # ],\n",
    "    # see https://github.com/run-llama/llama_index/blob/f7c5ee5efbb6172e819f26d1705fcdf6114b11a3/llama-index-core/llama_index/core/response_synthesizers/type.py#L4\n",
    "    response_mode=\"tree_summarize\", # \"accumulate\", \"compact_accumulate\", \"compact\", \"simple_summarize\", \"tree_summarize\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Summarize section IX.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize each section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "\n",
    "def parse_list_from_output_string(output_string):\n",
    "    \"\"\"\n",
    "    Parses and extracts a Python list from the given output string.\n",
    "\n",
    "    Args:\n",
    "    output_string (str): The output string containing the list representation.\n",
    "\n",
    "    Returns:\n",
    "    list: The extracted Python list.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use regex to find the list portion within the string\n",
    "        list_match = re.search(r'(\\[[^\\]]*\\])', output_string, re.DOTALL)\n",
    "        if list_match:\n",
    "            list_str = list_match.group(1)\n",
    "            # Safely evaluate the list string\n",
    "            data = ast.literal_eval(list_str)\n",
    "            if isinstance(data, list):\n",
    "                return data\n",
    "            else:\n",
    "                raise ValueError(\"The extracted portion is not a list.\")\n",
    "        else:\n",
    "            raise ValueError(\"No list found in the output string.\")\n",
    "    except (ValueError, SyntaxError):\n",
    "        raise ValueError(\"The output string is not a valid list representation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "summarizer = TreeSummarize(llm=llm, verbose=True)\n",
    "# original prompt\n",
    "# prompt_summary = f\"\"\"\n",
    "# Summarize this section of a new SEC rule proposal and/or amendment. \n",
    "# Make sure the summary includes every mention of any regulatory section, rule, or act (e.g. 12 U.S.C. 1843(k)(4)(C), 240.13a-15, Investment Company Act of 1940) in the text. \n",
    "# Do not use any prior knowledge except what is in the text to generate the summary.\n",
    "# Be detailed and verbose in your response.\n",
    "# Do not output any premable.\n",
    "# \"\"\"\n",
    "\n",
    "# gpt enhanced prompt\n",
    "prompt_summary = \"\"\"\n",
    "Summarize the content of the following section from a new SEC rule proposal or amendment. Ensure that the summary:\n",
    "\n",
    "1. Includes every reference to any specific regulatory section, rule, or act mentioned in the text (e.g., 12 U.S.C. 1843(k)(4)(C), 240.13a-15, Investment Company Act of 1940).\n",
    "2. Stays strictly within the information presented in the text, without incorporating any outside or prior knowledge.\n",
    "3. Is detailed, thorough, and specific in its coverage of all key points.\n",
    "4. Avoids adding any introductory or concluding remarks outside the scope of the summary itself.\n",
    "\"\"\"\n",
    "\n",
    "section_summaries = []\n",
    "extracted_regs = []\n",
    "\n",
    "for i,sec in enumerate(sections):\n",
    "    rule_prompt = f\"\"\"\n",
    "    Extract all mentions of any regulatory sections, rules, and acts in the following text.\n",
    "    Compile all extracted items into a single Python List of Strings object.\n",
    "    For example, [\"Item 1\", \"Item 2\", \"Item 3\"].\n",
    "\n",
    "    Here is the text:\n",
    "    {sec}\n",
    "    \"\"\"\n",
    "    response = llm.complete(rule_prompt)\n",
    "\n",
    "    # List checker\n",
    "    rewrite_counter=0\n",
    "    while True:\n",
    "        try: \n",
    "            extracted_reg = parse_list_from_output_string(response.text)\n",
    "            break\n",
    "        except:\n",
    "            print(\"Correcting list format...\")\n",
    "            response = llm.complete(f\"\"\"The following text does not contain a valid Python List of Strings? \n",
    "                                        Rewrite the text so that the List is in a valid Python format.\n",
    "                                        For example, [\"Item 1\", \"Item 2\", \"Item 3\"].\\n\\n{response.text}\n",
    "                                    \"\"\")\n",
    "            rewrite_counter+=1\n",
    "            print(f\"   Rewrote list {rewrite_counter} times.\")\n",
    "            if rewrite_counter>5:\n",
    "                raise ValueError(\"Could not correct list format.\")\n",
    "\n",
    "    extracted_regs.append(extracted_reg)\n",
    "    prompt_summary = f\"\"\"\n",
    "    Summarize the content of the following section from a new SEC rule proposal or amendment. Ensure that the summary:\n",
    "\n",
    "    1. Includes every reference to any specific regulatory section, rule, or act mentioned in the text (e.g., 12 U.S.C. 1843(k)(4)(C), 240.13a-15, Unfunded Mandates Reform Act (section 202(a)), Investment Company Act of 1940).\n",
    "    2. Stays strictly within the information presented in the text, without incorporating any outside or prior knowledge.\n",
    "    3. Is detailed, thorough, and specific in its coverage of all key points.\n",
    "    4. Avoids adding any introductory or concluding remarks outside the scope of the summary itself.\n",
    "\n",
    "    Here are some of the regulatory sections, rules, and acts:\n",
    "    {extracted_reg} \n",
    "    \"\"\"\n",
    "    print(f\"Summarizing section {i+1}/{len(sections)}...\")\n",
    "    parser = SentenceSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    nodes = parser.get_nodes_from_documents([documents_proposal[i]], show_progress=True)\n",
    "    response = await summarizer.aget_response(prompt_summary, [doc.text for doc in nodes])\n",
    "    section_summaries.append(response)\n",
    "    # if i==3:\n",
    "    #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex in extracted_regs:\n",
    "    print(ex)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goods=0\n",
    "bads=0\n",
    "for i,(sec,summary) in enumerate(zip(sections, section_summaries)):\n",
    "    numbers = re.findall(r'(?<!\\w)(\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?|\\d+\\.\\d+|\\d+)(?!\\w)', summary)\n",
    "    numbers = [x for x in numbers if len(x)>1]\n",
    "    print(numbers)\n",
    "    for number in numbers:\n",
    "        if number not in sec:\n",
    "            bads+=1\n",
    "            print(f\"{number} not in section {i}!\")\n",
    "        else:\n",
    "            goods+=1\n",
    "\n",
    "print(f\"Goods: {goods}, Bads: {bads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "\n",
    "class PDF(FPDF):\n",
    "    def header(self):\n",
    "        self.set_font('Arial', 'B', 12)\n",
    "        self.cell(0, 10, 'Summary of Sections', 0, 1, 'C')\n",
    "\n",
    "    def chapter_title(self, title):\n",
    "        self.set_font('Arial', 'B', 14)\n",
    "        self.cell(0, 10, title, 0, 1, 'L')\n",
    "        self.ln(5)\n",
    "\n",
    "    def chapter_body(self, body):\n",
    "        self.set_font('Arial', '', 12)\n",
    "        self.multi_cell(0, 10, body)\n",
    "        self.ln()\n",
    "\n",
    "def create_pdf(summaries, section_headers, output_filename):\n",
    "    pdf = PDF()\n",
    "\n",
    "    pdf.add_page()\n",
    "\n",
    "    for header, summary in zip(section_headers, summaries):\n",
    "        print(header)\n",
    "        pdf.chapter_title(header.replace(\"’\", \"'\"))\n",
    "        pdf.chapter_body(summary.replace(\"–\", \"-\"))\n",
    "\n",
    "    pdf.output(output_filename)\n",
    "\n",
    "# Save the summaries to a PDF\n",
    "output_filename = \"/workspace/data/summary_of_sections.pdf\"\n",
    "section_headers = [x.split('\\n')[0].strip() for x in sections]\n",
    "create_pdf(section_summaries, section_headers, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_summary_of_summaries = f\"\"\"\n",
    "Provide a high-level summary of the following section summaries from a new SEC rule proposal. Ensure that the summary:\n",
    "\n",
    "1. Focuses on the most relevant points for a financial executive, emphasizing practical implications, compliance requirements, and potential business impacts.\n",
    "2. Is thorough yet concise, presenting detailed insights that are actionable and strategic.\n",
    "3. Avoids any preamble or extraneous details not directly related to the financial executive’s decision-making needs.\n",
    "\"\"\"\n",
    "summary_of_section_summaries = await summarizer.aget_response(prompt_summary_of_summaries, [doc for doc in section_summaries])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_of_section_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(section_summaries[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sections[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Securities Exchange Act of 1934\n",
    "# Investment Advisers Act of 1940\n",
    "# Gramm-Leach-Bliley Act\n",
    "# 31 U.S.C. § 5311-5336\n",
    "# 12 U.S.C. § 1843(k)(4)(C)\n",
    "# 15 U.S.C. 80b (Investment Advisers Act of 1940)\n",
    "# 15 U.S.C. 6809(2) (Gramm-Leach-Bliley Act)\n",
    "# Securities Exchange Act of 1934\n",
    "# 31 CFR part 1032\n",
    "# 31 CFR 1023.220 (Customer Identification Program rule for broker-dealers)\n",
    "# 31 CFR 1020.220 (CIP regulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import subprocess, os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.openai import OpenAI as LOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "from utils import parse_list_from_output_string, extract_list_from_string\n",
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    ")\n",
    "\n",
    "from llama_index.core.agent.react import ReActAgent\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import ToolSelection, ToolOutput\n",
    "\n",
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.openai import OpenAI as LOpenAI\n",
    "from rag_utils import create_llama_vector_index_rag\n",
    "\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from sec_utils import get_tree_data, get_metadata\n",
    "from llamaindex_data_utils import extract_text_from_pdf\n",
    "\n",
    "import os, re\n",
    "from llama_index.readers.file import PDFReader\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "class SummaryStartEvent(Event):\n",
    "    pass\n",
    "\n",
    "# class InitializationEvent(Event):\n",
    "#     pass\n",
    "\n",
    "class InitializationCleanupEvent(Event):\n",
    "    result: str\n",
    "\n",
    "class RegulationsExtractionEvent(Event):\n",
    "    pass\n",
    "\n",
    "class FormatCorrectionEvent(Event):\n",
    "    result: list\n",
    "\n",
    "class SummarizationEvent(Event):\n",
    "    result: list\n",
    "\n",
    "class SummarizationNumericalValidationEvent(Event):\n",
    "    result: list\n",
    "    summaries: list\n",
    "\n",
    "\n",
    "class RuleSummarizationFlow(Workflow):\n",
    "\n",
    "    @step\n",
    "    async def initialize(self, ctx: Context, ev: StartEvent) -> RegulationsExtractionEvent:\n",
    "        \n",
    "        # Open source work horse model\n",
    "        model_name, ctx_len = \"llama3.1:8b-instruct-q8_0\", 128000\n",
    "        addtion_kwargs = {\"max_new_tokens\": 8000}\n",
    "        ctx.data[\"llm\"] = Ollama(model=model_name, url=\"http://127.0.0.1:11434\", context_window=ctx_len, model_type=\"chat\", is_function_calling_model=True, \n",
    "                                    request_timeout=4000.0, **addtion_kwargs)\n",
    "        \n",
    "        # Expert closed model\n",
    "        ctx.data[\"expert_llm\"] = ctx.data[\"llm\"] # OpenAI(model=\"gpt-4o\",temperature=0.1)\n",
    "        \n",
    "        # embed_model = OllamaEmbedding(model_name, base_url=\"http://localhost:11434\")\n",
    "        \n",
    "        parser = PDFReader(return_full_document=True)\n",
    "        documents_proposal = parser.load_data(\"/workspace/data/compliance/FSI_Rule_proposal.pdf\")\n",
    "\n",
    "        # remove page numbers. they mess up later parsing.\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Create a regex pattern for the current page number\n",
    "            pattern = rf'^{page_number}+ '\n",
    "            # Substitute the pattern with an empty string\n",
    "            new_text = re.sub(pattern, '', documents_proposal[0].text, flags=re.MULTILINE)\n",
    "            # If no substitution was made, break the loop\n",
    "            if new_text == documents_proposal[0].text:\n",
    "                break\n",
    "            # Update the text and increment the page number\n",
    "            documents_proposal[0].text = new_text\n",
    "            page_number += 1\n",
    "\n",
    "        # TODO: Generalize (Very manual splitting of sections)\n",
    "        pattern = r'(?<=\\n)([IVX]+\\..*?)(?=\\n[IVX]+\\.\\s|\\Z)'\n",
    "        sections=[]\n",
    "        sections.append(\"Proposal Information\"+documents_proposal[0].text.split(\"I. Background A. Sta\")[0])\n",
    "        sections += re.findall(pattern, documents_proposal[0].text, re.DOTALL)\n",
    "        section4_5split = sections[4].split(\"V. Paperwork Reduction Act\")\n",
    "        sections[4]=section4_5split[0]\n",
    "        sections = sections[:5]+[\"V. Paperwork Reduction Act\"+section4_5split[-1]]+sections[5:]\n",
    "        for shead in [x.split('\\n')[0].strip() for x in sections]:\n",
    "            print(shead)\n",
    "\n",
    "\n",
    "        documents_proposal = [Document(text=t, \n",
    "                                text_template='{metadata_str}\\n\\n{content}') for t in sections]\n",
    "\n",
    "        # add metadata to the documents_proposal\n",
    "        for i in range(len(documents_proposal)):\n",
    "            documents_proposal[i].metadata[\"section\"] = documents_proposal[i].text.split(\"\\n\")[0].strip()\n",
    "            \n",
    "        # documents_proposal.extend(docs)\n",
    "        \n",
    "        # Global state context\n",
    "        ctx.data[\"num_sections\"] = len(sections)\n",
    "        ctx.data[\"bad_summaries\"] = [x for x in range(len(sections))]\n",
    "        ctx.data[\"chat\"] = SimpleChatEngine.from_defaults(llm=ctx.data[\"expert_llm\"])\n",
    "        ctx.data[\"summarizer\"] = TreeSummarize(llm=ctx.data[\"expert_llm\"], verbose=False)\n",
    "        ctx.data[\"new_rule_documents\"] = documents_proposal\n",
    "        \n",
    "        ctx.data[\"initialized\"] = True\n",
    "        return RegulationsExtractionEvent()\n",
    "    \n",
    "    # @step\n",
    "    # async def check_initialization(self, ctx: Context, ev: InitializationEvent) -> InitializationCleanupEvent:\n",
    "    #     assert ctx.data[\"initialized\"], \"Workflow not initialized.\"\n",
    "    #     for doc in ctx.data[\"new_rule_documents\"]:\n",
    "    #         response = await ctx.data[\"chat\"].chat(f\"\"\"\n",
    "    #         Given text, determine if there are any formatting issues. \n",
    "    #         If it's good, return the original text.\n",
    "    #         If it's bad, rewrite the text so that it the text is clean.\n",
    "    #         Bad formatting contains accidental spaces (e.g. \"Co mputer\", \"tit- for-tat\") and .\n",
    "\n",
    "    #         Here is the text: {doc.text}\n",
    "    #         \"\"\")\n",
    "        \n",
    "    #         print(\"Query Judge response:\", response)\n",
    "    #         if response == \"bad\":\n",
    "    #             # try again\n",
    "    #             return BadQueryEvent(query=ev.query)\n",
    "    #         else:\n",
    "    #             pass\n",
    "\n",
    "    #     return SummaryStartEvent()\n",
    "\n",
    "    @step\n",
    "    async def extract_regulations(self, ctx: Context, ev: RegulationsExtractionEvent) -> FormatCorrectionEvent:\n",
    "        assert ctx.data[\"initialized\"], \"Workflow not initialized.\"\n",
    "        \n",
    "        regs_extraction=[]\n",
    "        for i,sec in enumerate(ctx.data[\"new_rule_documents\"]):\n",
    "\n",
    "            rule_prompt = f\"\"\"\n",
    "            Extract all mentions of any regulatory sections, rules, and acts in the following text.\n",
    "            Compile all extracted items into a single Python List of Strings object.\n",
    "            For example, [\"Item 1\", \"Item 2\", \"Item 3\"].\n",
    "            Only return the Python List of Strings.\n",
    "\n",
    "            Here is the text:\n",
    "            {sec}\n",
    "            \"\"\"\n",
    "            response = ctx.data[\"chat\"].chat(rule_prompt)\n",
    "            regs_extraction.append(response)\n",
    "        \n",
    "        return FormatCorrectionEvent(result=regs_extraction)\n",
    "    \n",
    "    @step\n",
    "    async def correct_format(self, ctx: Context, ev: FormatCorrectionEvent) -> SummarizationEvent:\n",
    "        \n",
    "        regs_list = []\n",
    "        for response in ev.result:\n",
    "            # List checker\n",
    "            rewrite_counter=0\n",
    "            while True:\n",
    "                try: \n",
    "                    \n",
    "                    # extracted_reg = parse_list_from_output_string(response)\n",
    "                    extracted_reg = extract_list_from_string(response)\n",
    "                    break\n",
    "                except:\n",
    "                    print(\"Correcting list format...\")\n",
    "                    response = ctx.data[\"chat\"].chat(f\"\"\"The following text does not contain a valid Python List of Strings? \n",
    "                                                Rewrite the text so that the List is in a valid Python format.\n",
    "                                                For example, [\"Item 1\", \"Item 2\", \"Item 3\"].\\nText:\\n\\n{response}\n",
    "                                            \"\"\")\n",
    "                    rewrite_counter+=1\n",
    "                    print(f\"   Rewrote list {rewrite_counter} times.\")\n",
    "                    if rewrite_counter>5:\n",
    "                        raise ValueError(\"Could not correct list format.\")\n",
    "            regs_list.append(extracted_reg)\n",
    "\n",
    "        return SummarizationEvent(result=regs_list)\n",
    "\n",
    "\n",
    "    @step\n",
    "    async def summarize_sections(self, ctx: Context, ev: SummarizationEvent) -> SummarizationNumericalValidationEvent | StopEvent: #SummarizationValidationEvent:\n",
    "        \n",
    "        if len(ctx.data[\"bad_summaries\"])==0:\n",
    "            return StopEvent(result=ctx.data[\"section_summaries\"])\n",
    "        elif \"section_summaries\" in ctx.data:\n",
    "            print(\"   Bad summaries found. Correcting...\")\n",
    "            section_summaries = ctx.data[\"section_summaries\"]\n",
    "            summary_count = ctx.data[\"bad_summaries\"]\n",
    "            prompt_suffix = f\"\"\"\\nThe first attempt at summarizing this section had numerical copy mistakes. Copy numbers exactly as they appear in the text.\"\"\"\n",
    "        else:\n",
    "            section_summaries = [None]*ctx.data[\"num_sections\"]\n",
    "            summary_count = range(ctx.data[\"num_sections\"])\n",
    "            prompt_suffix = \"\"\n",
    "        \n",
    "        for i in summary_count:\n",
    "            extracted_reg = ev.result[i]\n",
    "            prompt_summary = f\"\"\"\n",
    "            Summarize the content of the following section from a new SEC rule proposal or amendment. Ensure that the summary:\n",
    "\n",
    "            1. Includes every reference to any specific regulatory section, rule, or act mentioned in the text (e.g., 12 U.S.C. 1843(k)(4)(C), 240.13a-15, Unfunded Mandates Reform Act (section 202(a)), Investment Company Act of 1940).\n",
    "            2. Stays strictly within the information presented in the text, without incorporating any outside or prior knowledge.\n",
    "            3. Is detailed, thorough, and specific in its coverage of all key points, definitions and exclusions.\n",
    "            4. Avoids adding any introductory or concluding remarks outside the scope of the summary itself.\n",
    "\n",
    "            Here are some of the regulatory sections, rules, and acts:\n",
    "            {extracted_reg} \n",
    "            \"\"\"\n",
    "            print(f\"   Summarizing section {i+1}/{ctx.data['num_sections']}...\")\n",
    "            parser = SentenceSplitter(chunk_size=500, chunk_overlap=20)\n",
    "            nodes = parser.get_nodes_from_documents(ctx.data[\"new_rule_documents\"], show_progress=True)\n",
    "            response = ctx.data[\"summarizer\"].get_response(prompt_summary+prompt_suffix, [doc.text for doc in nodes])\n",
    "            section_summaries[i] = response\n",
    "        \n",
    "        ctx.data[\"section_summaries\"] = section_summaries\n",
    "        return SummarizationNumericalValidationEvent(result=ev.result, summaries=section_summaries)\n",
    "    \n",
    "    @step\n",
    "    async def validate_summaries(self, ctx: Context, ev: SummarizationNumericalValidationEvent) -> SummarizationEvent:\n",
    "        \n",
    "        bad_summaries = []\n",
    "        sections = [x.text for x in ctx.data[\"new_rule_documents\"]]\n",
    "        for i in ctx.data[\"bad_summaries\"]:\n",
    "            goods=0\n",
    "            bads=0\n",
    "            numbers = re.findall(r'(?<!\\w)(\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?|\\d+\\.\\d+|\\d+)(?!\\w)', ev.summaries[i])\n",
    "            numbers = [x for x in numbers if len(x)>1]\n",
    "            for number in numbers:\n",
    "                if number not in sections[i]:\n",
    "                    bads+=1\n",
    "                    # print(f\"{number} not in section {i}!\")\n",
    "                else:\n",
    "                    goods+=1\n",
    "        \n",
    "            # TODO: this is a manual param. put in ctx.data\n",
    "            if bads>7:\n",
    "                bad_summaries.append(i)\n",
    "            print(f\"   Section {i} Goods: {goods}, Bads: {bads}\")\n",
    "\n",
    "        ctx.data[\"bad_summaries\"] = bad_summaries            \n",
    "        return SummarizationEvent(result=ev.result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_all_possible_flows(RuleSummarizationFlow,filename=\"rule_flows.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = RuleSummarizationFlow(timeout=12000, verbose=True)\n",
    "result = await c.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "\n",
    "class PDF(FPDF):\n",
    "    def header(self):\n",
    "        self.set_font('Arial', 'B', 12)\n",
    "        self.cell(0, 10, 'Summary of Sections', 0, 1, 'C')\n",
    "\n",
    "    def chapter_title(self, title):\n",
    "        self.set_font('Arial', 'B', 14)\n",
    "        self.cell(0, 10, title, 0, 1, 'L')\n",
    "        self.ln(5)\n",
    "\n",
    "    def chapter_body(self, body):\n",
    "        self.set_font('Arial', '', 12)\n",
    "        self.multi_cell(0, 10, body)\n",
    "        self.ln()\n",
    "\n",
    "def create_pdf(summaries, section_headers, output_filename):\n",
    "    pdf = PDF()\n",
    "\n",
    "    pdf.add_page()\n",
    "\n",
    "    for header, summary in zip(section_headers, summaries):\n",
    "        print(header)\n",
    "        pdf.chapter_title(header.replace(\"’\", \"'\"))\n",
    "        # try:\n",
    "        pdf.chapter_body(summary.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"-\").replace(\"–\", \"-\").replace(\"—\", \"-\"))\n",
    "        # except:\n",
    "            # print(summary)\n",
    "        # pdf.chapter_body(summary.)\n",
    "        # pdf.chapter_body(summary)\n",
    "        \n",
    "\n",
    "    pdf.output(output_filename)\n",
    "\n",
    "# Save the summaries to a PDF\n",
    "output_filename = \"/workspace/data/summary_of_sections.pdf\"\n",
    "# section_headers = [x.split('\\n')[0].strip() for x in sections]\n",
    "section_headers = [\"Proposal Information\",\n",
    "\"I. Background A. Statutory Provisions\",\n",
    "\"II. Section -by-Section Analysis\",\n",
    "\"III. Request for Comments\",\n",
    "\"IV. Analysis of the Costs and Benefits Associated with the Proposed Rule\",\n",
    "\"V. Paperwork Reduction Act\",\n",
    "\"VI. Regulatory Flexibility Act\",\n",
    "\"VII. Considerations of th e Impact on the Economy\",\n",
    "\"IX. FinCEN’s Unfunded Mandates Reform Act Determination\"\n",
    "]\n",
    "create_pdf(result, section_headers, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic First Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.react import ReActAgent\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import ToolSelection, ToolOutput\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    Settings,\n",
    "    Document,\n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.core.workflow import (\n",
    "    step,\n",
    "    Context,\n",
    "    Workflow,\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent\n",
    ")\n",
    "from llama_index.core.postprocessor.rankGPT_rerank import RankGPTRerank\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.openai import OpenAI as LOpenAI\n",
    "from rag_utils import create_llama_vector_index_rag\n",
    "\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from sec_utils import get_tree_data, get_metadata\n",
    "from llamaindex_data_utils import extract_text_from_pdf\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from llama_index.readers.file import PDFReader\n",
    "from llamaindex_data_utils import extract_text_from_pdf\n",
    "from llama_index.core import Document\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "access_token = os.getenv('HF_TOKEN')\n",
    "llama_api_key = os.getenv('LLAMA_API_KEY')\n",
    "\n",
    "parser = PDFReader(return_full_document=True)\n",
    "documents_proposal = parser.load_data(\"/workspace/data/compliance/FSI_Rule_proposal.pdf\")\n",
    "# Regular expression to find numbers followed by a space at the beginning of a new line\n",
    "pattern = r'^\\d+ '\n",
    "# Replace page number pattern with an empty string\n",
    "documents_proposal[0].text = re.sub(pattern, '', documents_proposal[0].text, flags=re.MULTILINE)\n",
    "\n",
    "pattern = r'(?<=\\n)([IVX]+\\..*?)(?=\\n[IVX]+\\.\\s|\\Z)'\n",
    "sections=[]\n",
    "sections.append(\"Proposal Information\"+documents_proposal[0].text.split(\"I. Background A. Sta\")[0])\n",
    "sections += re.findall(pattern, documents_proposal[0].text, re.DOTALL)\n",
    "section4_5split = sections[4].split(\"V. Paperwork Reduction Act\")\n",
    "sections[4]=section4_5split[0]\n",
    "sections.append(\"V. Paperwork Reduction Act\"+section4_5split[-1])\n",
    "\n",
    "# pdf_urls = [\"/workspace/data/compliance/FSI_Factsheet.pdf\", \"/workspace/data/compliance/FSI_Press_Release.pdf\", \"/workspace/data/compliance/FSI_Rule_proposal.pdf\"]\n",
    "# descriptions = [\"Factsheet about newly proposed SEC rule.\", \"Press release regarding newly proposed SEC rule.\", \"The full text of the newly proposed SEC rule.\"]\n",
    "# pdf_urls = [\"/workspace/data/compliance/FSI_Rule_proposal.pdf\"]\n",
    "# descriptions = [\"The full text of the newly proposed SEC rule.\"]\n",
    "# documents_proposal = extract_text_from_pdf(pdf_urls, llama_api_key, llamaparse_kwargs={\"split_by_page\":False}, save_json_path=None)\n",
    "documents_proposal = [Document(text=t, \n",
    "                          text_template='{metadata_str}\\n\\n{content}') for t in sections]\n",
    "\n",
    "# add metadata to the documents_proposal\n",
    "for i in range(len(documents_proposal)):\n",
    "    documents_proposal[i].metadata[\"section\"] = documents_proposal[i].text.split(\"\\n\")[0].strip()\n",
    "    # documents_proposal[i].metadata[\"source\"] = pdf_urls[i]\n",
    "    # documents_proposal[i].metadata[\"description\"] = descriptions[i]\n",
    "    \n",
    "# documents_proposal.extend(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define the workflow and all events\n",
    "# class InputEvent(Event):\n",
    "#     input: list[ChatMessage]\n",
    "\n",
    "# class ToolCallEvent(Event):\n",
    "#     tool_calls: list[ToolSelection]\n",
    "\n",
    "# class FunctionOutputEvent(Event):\n",
    "#     output: ToolOutput\n",
    "\n",
    "class InitializeEvent(Event):\n",
    "    pass\n",
    "\n",
    "class QueryEvalEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class BadQueryEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class NaiveRAGEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class HighTopKEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class RerankEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class ComparisonEvent(Event):\n",
    "    query: str\n",
    "    documents: str\n",
    "\n",
    "class ResponseEvent(Event):\n",
    "    query: str\n",
    "    response: str\n",
    "\n",
    "class SummarizeEvent(Event):\n",
    "    query: str\n",
    "    documents: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class ComplianceWorkflow(Workflow):\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def initialize(self, ctx: Context, ev: InitializeEvent) -> QueryEvalEvent:\n",
    "        \n",
    "        # Open source work horse model\n",
    "        model_name, ctx_len = \"llama3.1:latest\", 128000\n",
    "        addtion_kwargs = {\"max_new_tokens\": 4000}\n",
    "        # system_prompt = \"You are an expert at answering questions about rules and regulations regarding Title 17—Commodity and Securities Exchanges: CHAPTER II—SECURITIES AND EXCHANGE COMMISSION. Please provide a summary of the following text, and cite any sections, rules, acts or laws (e.g. § 230.503, § 240.13a-15, Act (15 U.S.C. 781), Investment Company Act of 1940) from context that support the answer. Be detailed in your response.\"\n",
    "        ctx.data[\"llm\"] = Ollama(model=model_name, url=\"http://127.0.0.1:11434\", context_window=ctx_len, model_type=\"chat\", is_function_calling_model=True, \n",
    "                                    request_timeout=4000.0, additional_kwargs=addtion_kwargs) #, system_prompt=system_prompt)\n",
    "        \n",
    "        # Expert closed model\n",
    "        ctx.data[\"expert_llm\"] = ctx.data[\"llm\"] # OpenAI(model=\"gpt-4o\",temperature=0.1)\n",
    "        \n",
    "        # Settings.llm = ctx.data[\"llm\"]\n",
    "        # Embedding models\n",
    "        embed_model_name = \"dunzhang/stella_en_1.5B_v5\" \n",
    "        # finetuned_embed_model_name = \"/workspace/data/compliance/rule_proposal_embedding_model_gpt\"\n",
    "        # proposal_embed_model = HuggingFaceEmbedding(model_name=finetuned_embed_model_name)\n",
    "        sec_embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "        \n",
    "        # ctx.data[\"new_rule_index\"] = create_llama_vector_index_rag(llm=ctx.data[\"llm\"], \n",
    "        #                                                             embed_model=proposal_embed_model, \n",
    "        #                                                             persist_dir=\"/workspace/data/compliance/vector_idx_proposed\",\n",
    "        #                                                             vector_store_kwargs={\"chunk_size\": 300, \"chunk_overlap\": 20})\n",
    "        \n",
    "        ctx.data[\"sec_index\"] = create_llama_vector_index_rag(llm=ctx.data[\"llm\"], \n",
    "                                                                embed_model=sec_embed_model, \n",
    "                                                                persist_dir=\"/workspace/data/compliance/vector_idx_sec\",\n",
    "                                                                vector_store_kwargs={\"chunk_size\": 500, \"chunk_overlap\": 20})\n",
    "        \n",
    "        # we use a chat engine so it remembers previous interactions\n",
    "        ctx.data[\"judge\"] = SimpleChatEngine.from_defaults(llm=ctx.data[\"expert_llm\"])\n",
    "        ctx.data[\"summarizer\"] = TreeSummarize(llm=ctx.data[\"expert_llm\"], verbose=False)\n",
    "        ctx.data[\"new_rule_documents\"] = documents_proposal\n",
    "\n",
    "        ctx.data[\"initialized\"] = True\n",
    "        return QueryEvalEvent()\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def judge_query(self, ctx: Context, ev: StartEvent | QueryEvalEvent ) -> BadQueryEvent | RerankEvent | SummarizeEvent: #NaiveRAGEvent | HighTopKEvent |\n",
    "        # TODO: make initialize its own event\n",
    "        # initialize\n",
    "        if 'initialized' not in ctx.data:\n",
    "            return InitializeEvent()\n",
    "\n",
    "        response = await ctx.data[\"judge\"].chat(f\"\"\"\n",
    "            Given a user query, determine if this is likely to yield precise results from a financial RAG system as-is. \n",
    "            If it's good, return 'good', if it's bad, return 'bad'.\n",
    "            Good queries use a lot of relevant keywords and are detailed. Bad queries are vague or ambiguous.\n",
    "\n",
    "            Here is the query: {ev.query}\n",
    "            \"\"\")\n",
    "        \n",
    "        print(\"Query Judge response:\", response)\n",
    "        if response == \"bad\":\n",
    "            # try again\n",
    "            return BadQueryEvent(query=ev.query)\n",
    "        else:\n",
    "            # send query to all 3 strategies\n",
    "            # self.send_event(NaiveRAGEvent(query=ev.query))\n",
    "            # self.send_event(HighTopKEvent(query=ev.query))\n",
    "            await self.send_event(RerankEvent(query=ev.query))\n",
    "            await self.send_event(SummarizeEvent(query=ev.query, documents=ctx.data[\"new_rule_documents\"]))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def improve_query(self, ctx: Context, ev: BadQueryEvent) -> QueryEvalEvent:\n",
    "        response = await ctx.data[\"llm\"].complete(f\"\"\"\n",
    "            This is a query to a RAG system: {ev.query}\n",
    "\n",
    "            The query is bad because it is too vague. Please provide a more detailed query that includes specific keywords and removes any ambiguity.\n",
    "        \"\"\")\n",
    "        return QueryEvalEvent(query=str(response))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def naive_rag(self, ctx: Context, ev: NaiveRAGEvent) -> ResponseEvent:\n",
    "        index = ctx.data[\"new_rule_index\"]\n",
    "        engine = index.as_query_engine(llm=ctx.data[\"expert_llm\"], similarity_top_k=3)\n",
    "        response = await engine.query(ev.query)\n",
    "        print(\"Naive response:\", response)\n",
    "        return ResponseEvent(query=ev.query, source=\"Naive\", response=str(response))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def high_top_k(self, ctx: Context, ev: HighTopKEvent) -> ResponseEvent:\n",
    "        index = ctx.data[\"new_rule_index\"]\n",
    "        engine = index.as_query_engine(llm=ctx.data[\"expert_llm\"], similarity_top_k=5)\n",
    "        response = await engine.query(ev.query)\n",
    "        print(\"High top k response:\", response)\n",
    "        return ResponseEvent(query=ev.query, source=\"High top k\", response=str(response))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def extract_sec_sections(self, ctx: Context, ev: RerankEvent) -> ResponseEvent:\n",
    "        pass\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def rerank(self, ctx: Context, ev: RerankEvent) -> ResponseEvent:\n",
    "        index = ctx.data[\"new_rule_index\"]\n",
    "        reranker = RankGPTRerank(\n",
    "            top_n=2,\n",
    "            llm=ctx.data[\"expert_llm\"]\n",
    "        )\n",
    "        retriever = index.as_retriever(llm=ctx.data[\"expert_llm\"], similarity_top_k=5)\n",
    "        engine = RetrieverQueryEngine.from_args(\n",
    "            retriever=retriever,\n",
    "            node_postprocessors=[reranker],\n",
    "        )\n",
    "        response = await engine.query(ev.query)\n",
    "        print(\"Reranker response:\", response)\n",
    "        return ResponseEvent(query=ev.query, source=\"Reranker\", response=str(response))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def summarize(self, ctx: Context, ev: SummarizeEvent) -> ResponseEvent:\n",
    "        prompt_summary = f\"\"\"\n",
    "        A long document of a newly proposed SEC rule has been provided. \n",
    "        Please provide a summary of the document with details on key points and sections\n",
    "        that would be relevant to a compliance officer.\n",
    "        \"\"\"\n",
    "\n",
    "        response = await ctx.data[\"summarizer\"].aget_response(prompt_summary, [doc.text for doc in ev.documents])\n",
    "        print(\"Summarizer response:\", response)\n",
    "        return ResponseEvent(query=ev.query, source=\"Summary\", response=str(response))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def judge(self, ctx: Context, ev: ResponseEvent) -> StopEvent:\n",
    "        ready = ctx.collect_events(ev, [ResponseEvent]*2)\n",
    "        if ready is None:\n",
    "            return None\n",
    "\n",
    "        response = ctx.data[\"judge\"].chat(f\"\"\"\n",
    "            A user has provided a query and 3 different strategies have been used\n",
    "            to try to answer the query. Your job is to decide which strategy best\n",
    "            answered the query. The query was: {ev.query}\n",
    "\n",
    "            Response 1 ({ready[0].source}): {ready[0].response}\n",
    "            Response 2 ({ready[1].source}): {ready[1].response}\n",
    "            Response 3 ({ready[2].source}): {ready[2].response}\n",
    "\n",
    "            Please provide the number of the best response (1, 2, or 3).\n",
    "            Just provide the number, with no other text or preamble.\n",
    "        \"\"\")\n",
    "\n",
    "        summary_idx = [i for i, r in enumerate(ready) if r.source == \"Summary\"][0]\n",
    "        best_response = int(str(response))\n",
    "        print(f\"Best response was number {best_response}, which was from {ready[best_response-1].source}\")\n",
    "        return StopEvent(result=[str(ready[best_response-1].response), ready[summary_idx].response])\n",
    "        # return StopEvent(result=str(ready[best_response-1].response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_all_possible_flows(ComplianceWorkflow,filename=\"concierge_flows.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ComplianceWorkflow(timeout=120, verbose=True)\n",
    "result = await c.run(\n",
    "    query=\"What SEC sections does the newly proposed rule affect?\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "parser = SentenceSplitter(chunk_size=500, chunk_overlap=50)\n",
    "nodes = parser.get_nodes_from_documents(documents_proposal, show_progress=True)\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "summarizer = TreeSummarize(llm=llm, verbose=True)\n",
    "# prompt_summary = \"You are a professional executive of AlphaTrAI. Your job is to summarize this text in great detail from a video transcription. The summary will be distributed to investors and stakeholders, so give a lot of details and examples from the transcription.\"\n",
    "prompt_summary = \"\"\"\n",
    "You are an expert compliance officer who specializes at examining multiple documents for the purpose of extracting novelty, importance, and summarizing findings. Your task is to read the following documents and provide a summary of the key points, sections, and rules that are relevant to a compliance officer. Be detailed in your response.\n",
    "\"\"\"\n",
    "\n",
    "response = await summarizer.aget_response(prompt_summary, [doc.text for doc in nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.function_calling \n",
    "response_information_tool = []\n",
    "llm.chat_with_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define the workflow and all events\n",
    "# class InputEvent(Event):\n",
    "#     input: list[ChatMessage]\n",
    "\n",
    "# class ToolCallEvent(Event):\n",
    "#     tool_calls: list[ToolSelection]\n",
    "\n",
    "# class FunctionOutputEvent(Event):\n",
    "#     output: ToolOutput\n",
    "\n",
    "class InitializeEvent(Event):\n",
    "    pass\n",
    "\n",
    "class JudgeEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class BadQueryEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class NaiveRAGEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class HighTopKEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class RerankEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class ComparisonEvent(Event):\n",
    "    query: str\n",
    "    documents: str\n",
    "\n",
    "class ResponseEvent(Event):\n",
    "    query: str\n",
    "    response: str\n",
    "\n",
    "class SummarizeEvent(Event):\n",
    "    query: str\n",
    "    documents: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplianceWorkflow(Workflow):\n",
    "\n",
    "    def load_or_create_index(self, persist_dir, documents=None):\n",
    "        # Check if the index already exists\n",
    "        if os.path.exists(persist_dir):\n",
    "            print(\"Loading existing index...\")\n",
    "            # Load the index from disk\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "            index = load_index_from_storage(storage_context)\n",
    "        else:\n",
    "            print(\"Creating new index...\")\n",
    "            # Load documents from the specified directory\n",
    "            # TODO: add a check for the documents\n",
    "\n",
    "            # Create a new index from the documents\n",
    "            index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "            # Persist the index to disk\n",
    "            index.storage_context.persist(persist_dir=persist_dir)\n",
    "\n",
    "        return index\n",
    "    \n",
    "    @step(pass_context=True)\n",
    "    async def initialize(self, ctx: Context, ev: InitializeEvent) -> JudgeEvent:\n",
    "        \n",
    "        # Open source work horse model\n",
    "        model_name, ctx_len = \"hermes3:8b\", 128000\n",
    "        addtion_kwargs = {\"max_new_tokens\": 2000}\n",
    "        # system_prompt = \"You are an expert at answering questions about rules and regulations regarding Title 17—Commodity and Securities Exchanges: CHAPTER II—SECURITIES AND EXCHANGE COMMISSION. Please provide a summary of the following text, and cite any sections, rules, acts or laws (e.g. § 230.503, § 240.13a-15, Act (15 U.S.C. 781), Investment Company Act of 1940) from context that support the answer. Be detailed in your response.\"\n",
    "        ctx.data[\"llm\"] = Ollama(model=model_name, url=\"http://127.0.0.1:11434\", context_window=ctx_len, model_type=\"chat\", is_function_calling_model=True, \n",
    "                                    request_timeout=4000.0, additional_kwargs=addtion_kwargs) #, system_prompt=system_prompt)\n",
    "        \n",
    "        # Expert closed model\n",
    "        ctx.data[\"expert_llm\"] = ctx.data[\"llm\"] # OpenAI(model=\"gpt-4o\",temperature=0.1)\n",
    "        \n",
    "        # Settings.llm = ctx.data[\"llm\"]\n",
    "        # Embedding models\n",
    "        embed_model_name = \"dunzhang/stella_en_1.5B_v5\" \n",
    "        finetuned_embed_model_name = \"/workspace/data/compliance/rule_proposal_embedding_model_gpt\"\n",
    "        proposal_embed_model = HuggingFaceEmbedding(model_name=finetuned_embed_model_name)\n",
    "        sec_embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "        \n",
    "        ctx.data[\"new_rule_index\"] = create_llama_vector_index_rag(llm=ctx.data[\"llm\"], \n",
    "                                                                    embed_model=proposal_embed_model, \n",
    "                                                                    persist_dir=\"/workspace/data/compliance/vector_idx_proposed\",\n",
    "                                                                    vector_store_kwargs={\"chunk_size\": 300, \"chunk_overlap\": 20})\n",
    "        \n",
    "        ctx.data[\"sec_index\"] = create_llama_vector_index_rag(llm=ctx.data[\"llm\"], \n",
    "                                                                embed_model=sec_embed_model, \n",
    "                                                                persist_dir=\"/workspace/data/compliance/vector_idx_sec\",\n",
    "                                                                vector_store_kwargs={\"chunk_size\": 500, \"chunk_overlap\": 20})\n",
    "        \n",
    "        # we use a chat engine so it remembers previous interactions\n",
    "        ctx.data[\"judge\"] = SimpleChatEngine.from_defaults(llm=ctx.data[\"expert_llm\"])\n",
    "        ctx.data[\"summarizer\"] = TreeSummarize(llm=ctx.data[\"expert_llm\"], verbose=False)\n",
    "        ctx.data[\"new_rule_documents\"] = documents_proposal\n",
    "\n",
    "        ctx.data[\"initialized\"] = True\n",
    "        return JudgeEvent()\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def judge_query(self, ctx: Context, ev: StartEvent | JudgeEvent ) -> BadQueryEvent | NaiveRAGEvent | HighTopKEvent | RerankEvent | SummarizeEvent:\n",
    "        # TODO: make initialize its own event\n",
    "        # initialize\n",
    "        if 'initialized' not in ctx.data:\n",
    "            return InitializeEvent()\n",
    "\n",
    "        response = ctx.data[\"judge\"].chat(f\"\"\"\n",
    "            Given a user query, determine if this is likely to yield good results from a RAG system as-is. If it's good, return 'good', if it's bad, return 'bad'.\n",
    "            Good queries use a lot of relevant keywords and are detailed. Bad queries are vague or ambiguous.\n",
    "\n",
    "            Here is the query: {ev.query}\n",
    "            \"\"\")\n",
    "        if response == \"bad\":\n",
    "            # try again\n",
    "            return BadQueryEvent(query=ev.query)\n",
    "        else:\n",
    "            # send query to all 3 strategies\n",
    "            self.send_event(NaiveRAGEvent(query=ev.query))\n",
    "            self.send_event(HighTopKEvent(query=ev.query))\n",
    "            self.send_event(RerankEvent(query=ev.query))\n",
    "            self.send_event(SummarizeEvent(query=ev.query, documents=ctx.data[\"new_rule_documents\"]))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def improve_query(self, ctx: Context, ev: BadQueryEvent) -> JudgeEvent:\n",
    "        response = ctx.data[\"llm\"].complete(f\"\"\"\n",
    "            This is a query to a RAG system: {ev.query}\n",
    "\n",
    "            The query is bad because it is too vague. Please provide a more detailed query that includes specific keywords and removes any ambiguity.\n",
    "        \"\"\")\n",
    "        return JudgeEvent(query=str(response))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def naive_rag(self, ctx: Context, ev: NaiveRAGEvent) -> ResponseEvent:\n",
    "        index = ctx.data[\"new_rule_index\"]\n",
    "        engine = index.as_query_engine(llm=ctx.data[\"expert_llm\"], similarity_top_k=3)\n",
    "        response = engine.query(ev.query)\n",
    "        print(\"Naive response:\", response)\n",
    "        return ResponseEvent(query=ev.query, source=\"Naive\", response=str(response))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def high_top_k(self, ctx: Context, ev: HighTopKEvent) -> ResponseEvent:\n",
    "        index = ctx.data[\"new_rule_index\"]\n",
    "        engine = index.as_query_engine(llm=ctx.data[\"expert_llm\"], similarity_top_k=5)\n",
    "        response = engine.query(ev.query)\n",
    "        print(\"High top k response:\", response)\n",
    "        return ResponseEvent(query=ev.query, source=\"High top k\", response=str(response))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def rerank(self, ctx: Context, ev: RerankEvent) -> ResponseEvent:\n",
    "        index = ctx.data[\"new_rule_index\"]\n",
    "        reranker = RankGPTRerank(\n",
    "            top_n=2,\n",
    "            llm=ctx.data[\"expert_llm\"]\n",
    "        )\n",
    "        retriever = index.as_retriever(llm=ctx.data[\"expert_llm\"], similarity_top_k=5)\n",
    "        engine = RetrieverQueryEngine.from_args(\n",
    "            retriever=retriever,\n",
    "            node_postprocessors=[reranker],\n",
    "        )\n",
    "        response = engine.query(ev.query)\n",
    "        print(\"Reranker response:\", response)\n",
    "        return ResponseEvent(query=ev.query, source=\"Reranker\", response=str(response))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def summarize(self, ctx: Context, ev: SummarizeEvent) -> ResponseEvent:\n",
    "        prompt_summary = f\"\"\"\n",
    "        A long document of a newly proposed SEC rule has been provided. \n",
    "        Please provide a summary of the document with details on key points and sections\n",
    "        that would be relevant to a compliance officer.\n",
    "        \"\"\"\n",
    "\n",
    "        response = await ctx.data[\"summarizer\"].aget_response(prompt_summary, [doc.text for doc in ev.documents])\n",
    "        print(\"Summarizer response:\", response)\n",
    "        return ResponseEvent(query=ev.query, source=\"Summary\", response=str(response))\n",
    "\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def judge(self, ctx: Context, ev: ResponseEvent) -> StopEvent:\n",
    "        ready = ctx.collect_events(ev, [ResponseEvent]*4)\n",
    "        if ready is None:\n",
    "            return None\n",
    "\n",
    "        response = ctx.data[\"judge\"].chat(f\"\"\"\n",
    "            A user has provided a query and 3 different strategies have been used\n",
    "            to try to answer the query. Your job is to decide which strategy best\n",
    "            answered the query. The query was: {ev.query}\n",
    "\n",
    "            Response 1 ({ready[0].source}): {ready[0].response}\n",
    "            Response 2 ({ready[1].source}): {ready[1].response}\n",
    "            Response 3 ({ready[2].source}): {ready[2].response}\n",
    "\n",
    "            Please provide the number of the best response (1, 2, or 3).\n",
    "            Just provide the number, with no other text or preamble.\n",
    "        \"\"\")\n",
    "\n",
    "        summary_idx = [i for i, r in enumerate(ready) if r.source == \"Summary\"][0]\n",
    "        best_response = int(str(response))\n",
    "        print(f\"Best response was number {best_response}, which was from {ready[best_response-1].source}\")\n",
    "        return StopEvent(result=[str(ready[best_response-1].response), ready[-1].response])\n",
    "        # return StopEvent(result=str(ready[best_response-1].response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph_engine_tools = QueryEngineTool(\n",
    "            query_engine=graph_index,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"sec_title_17_chapter_ii_tool\",\n",
    "                description=(\n",
    "                    \"Contains all the current sections, rules, and relationships of SEC Title 17 Chapter II.\"\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "query_engine_tools = QueryEngineTool(\n",
    "            query_engine=query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"new_rule_proposal_tool\",\n",
    "                description=(\n",
    "                    \"Contains all the information about the newly proposed SEC rule.\"\n",
    "                ),\n",
    "            ),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "\n",
    "from llama_index.core.llms.function_calling import FunctionCallingLLM\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.tools.types import BaseTool\n",
    "from llama_index.core.workflow import Workflow, StartEvent, StopEvent, step\n",
    "\n",
    "\n",
    "class FuncationCallingAgent(Workflow):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args: Any,\n",
    "        llm: FunctionCallingLLM | None = None,\n",
    "        tools: List[BaseTool] | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tools = tools or []\n",
    "\n",
    "        self.llm = llm\n",
    "        assert self.llm.metadata.is_function_calling_model\n",
    "\n",
    "        self.memory = ChatMemoryBuffer.from_defaults(llm=llm)\n",
    "        self.sources = []\n",
    "\n",
    "    @step\n",
    "    async def prepare_chat_history(self, ev: StartEvent) -> InputEvent:\n",
    "        # clear sources\n",
    "        self.sources = []\n",
    "\n",
    "        # get user input\n",
    "        user_input = ev.input\n",
    "        user_msg = ChatMessage(role=\"user\", content=user_input)\n",
    "        self.memory.put(user_msg)\n",
    "\n",
    "        # get chat history\n",
    "        chat_history = self.memory.get()\n",
    "        return InputEvent(input=chat_history)\n",
    "\n",
    "    @step\n",
    "    async def handle_llm_input(\n",
    "        self, ev: InputEvent\n",
    "    ) -> ToolCallEvent | StopEvent:\n",
    "        chat_history = ev.input\n",
    "        '''\n",
    "        Takes in the chat history, and uses tools to generate a response.\n",
    "        '''\n",
    "        response = await self.llm.achat_with_tools(\n",
    "            self.tools, chat_history=chat_history\n",
    "        )\n",
    "        self.memory.put(response.message)\n",
    "\n",
    "        tool_calls = self.llm.get_tool_calls_from_response(\n",
    "            response, error_on_no_tool_call=False\n",
    "        )\n",
    "\n",
    "        if not tool_calls:\n",
    "            return StopEvent(\n",
    "                result={\"response\": response, \"sources\": [*self.sources]}\n",
    "            )\n",
    "        else:\n",
    "            return ToolCallEvent(tool_calls=tool_calls)\n",
    "\n",
    "    @step\n",
    "    async def handle_tool_calls(self, ev: ToolCallEvent) -> InputEvent:\n",
    "        tool_calls = ev.tool_calls\n",
    "        tools_by_name = {tool.metadata.get_name(): tool for tool in self.tools}\n",
    "\n",
    "        tool_msgs = []\n",
    "\n",
    "        # call tools -- safely!\n",
    "        for tool_call in tool_calls:\n",
    "            tool = tools_by_name.get(tool_call.tool_name)\n",
    "            additional_kwargs = {\n",
    "                \"tool_call_id\": tool_call.tool_id,\n",
    "                \"name\": tool.metadata.get_name(),\n",
    "            }\n",
    "            if not tool:\n",
    "                tool_msgs.append(\n",
    "                    ChatMessage(\n",
    "                        role=\"tool\",\n",
    "                        content=f\"Tool {tool_call.tool_name} does not exist\",\n",
    "                        additional_kwargs=additional_kwargs,\n",
    "                    )\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                tool_output = tool(**tool_call.tool_kwargs)\n",
    "                self.sources.append(tool_output)\n",
    "                tool_msgs.append(\n",
    "                    ChatMessage(\n",
    "                        role=\"tool\",\n",
    "                        content=tool_output.content,\n",
    "                        additional_kwargs=additional_kwargs,\n",
    "                    )\n",
    "                )\n",
    "            except Exception as e:\n",
    "                tool_msgs.append(\n",
    "                    ChatMessage(\n",
    "                        role=\"tool\",\n",
    "                        content=f\"Encountered error in tool call: {e}\",\n",
    "                        additional_kwargs=additional_kwargs,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        for msg in tool_msgs:\n",
    "            self.memory.put(msg)\n",
    "\n",
    "        chat_history = self.memory.get()\n",
    "        return InputEvent(input=chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm.is_function_calling_model = True\n",
    "\n",
    "agent = FuncationCallingAgent(\n",
    "    llm=llm, tools=all_tools, timeout=120, verbose=True\n",
    ")\n",
    "\n",
    "ret = await agent.run(input=\"What is a summary of the proposed rule, and what SEC rules does it change?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ret[\"response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "from llama_index.utils.workflow import draw_most_recent_execution\n",
    "from IPython.display import display, HTML\n",
    "draw_all_possible_flows(FuncationCallingAgent, \"first_func_agent.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='/workspace/repos/agentic-ai/first_func_agent.html', width=700, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Read the HTML file content\n",
    "with open('/workspace/repos/agentic-ai/first_func_agent.html', 'r') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Render the HTML content in a Jupyter cell\n",
    "display(HTML(html_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import asyncio\n",
    "from pydantic_settings import BaseSettings\n",
    "import signal\n",
    "from llama_deploy.deploy.deploy import (\n",
    "                        _get_message_queue_client,\n",
    "                        _deploy_local_message_queue,\n",
    "                        _get_shutdown_handler\n",
    "                    )\n",
    "\n",
    "from llama_deploy import (\n",
    "    deploy_core,\n",
    "    ControlPlaneConfig,\n",
    "    SimpleMessageQueueConfig,\n",
    "    SimpleOrchestratorConfig,\n",
    "    ControlPlaneServer,\n",
    "    SimpleOrchestrator,\n",
    "    LlamaDeployClient\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_deploy import (\n",
    "    deploy_core,\n",
    "    ControlPlaneConfig,\n",
    "    SimpleMessageQueueConfig,\n",
    ")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    await deploy_core(\n",
    "        control_plane_config=ControlPlaneConfig(port=8002),\n",
    "        message_queue_config=SimpleMessageQueueConfig(port=8003),\n",
    "    )\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import asyncio\n",
    "\n",
    "#     asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_deploy import (\n",
    "    deploy_workflow,\n",
    "    WorkflowServiceConfig,\n",
    "    ControlPlaneConfig,\n",
    "    SimpleMessageQueueConfig,\n",
    ")\n",
    "from llama_index.core.workflow import Workflow, StartEvent, StopEvent, step\n",
    "\n",
    "\n",
    "# create a dummy workflow\n",
    "class MyWorkflow(Workflow):\n",
    "    @step()\n",
    "    async def run_step(self, ev: StartEvent) -> StopEvent:\n",
    "        # Your workflow logic here\n",
    "        arg1 = str(ev.get(\"arg1\", \"\"))\n",
    "        result = arg1 + \"_result\"\n",
    "        return StopEvent(result=result)\n",
    "\n",
    "\n",
    "async def main():\n",
    "    await deploy_workflow(\n",
    "        workflow=MyWorkflow(),\n",
    "        workflow_config=WorkflowServiceConfig(\n",
    "            host=\"127.0.0.1\", port=8004, service_name=\"my_workflow\"\n",
    "        ),\n",
    "        control_plane_config=ControlPlaneConfig(),\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context,\n",
    ")\n",
    "import random\n",
    "from llama_index.core.workflow import draw_all_possible_flows\n",
    "from llama_index.utils.workflow import draw_most_recent_execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import asyncio\n",
    "from pydantic_settings import BaseSettings\n",
    "import signal\n",
    "from llama_deploy.deploy.deploy import (\n",
    "                        _get_message_queue_client,\n",
    "                        _deploy_local_message_queue,\n",
    "                        _get_shutdown_handler\n",
    "                    )\n",
    "\n",
    "from llama_deploy import (\n",
    "    deploy_core,\n",
    "    ControlPlaneConfig,\n",
    "    SimpleMessageQueueConfig,\n",
    "    SimpleOrchestratorConfig,\n",
    "    ControlPlaneServer,\n",
    "    SimpleOrchestrator,\n",
    "    LlamaDeployClient\n",
    ")\n",
    "\n",
    "\n",
    "async def deploy_core(\n",
    "    control_plane_config: ControlPlaneConfig,\n",
    "    message_queue_config: BaseSettings,\n",
    "    orchestrator_config: Optional[SimpleOrchestratorConfig] = None,\n",
    ") -> None:\n",
    "    orchestrator_config = orchestrator_config or SimpleOrchestratorConfig()\n",
    "\n",
    "    message_queue_client = _get_message_queue_client(message_queue_config)\n",
    "\n",
    "    control_plane = ControlPlaneServer(\n",
    "        message_queue_client,\n",
    "        SimpleOrchestrator(**orchestrator_config.model_dump()),\n",
    "        **control_plane_config.model_dump(),\n",
    "    )\n",
    "\n",
    "    message_queue_task = None\n",
    "    if isinstance(message_queue_config, SimpleMessageQueueConfig):\n",
    "        message_queue_task = _deploy_local_message_queue(message_queue_config)\n",
    "\n",
    "    control_plane_task = asyncio.create_task(control_plane.launch_server())\n",
    "\n",
    "    # let services spin up\n",
    "    await asyncio.sleep(1)\n",
    "\n",
    "    # register the control plane as a consumer\n",
    "    control_plane_consumer_fn = await control_plane.register_to_message_queue()\n",
    "\n",
    "    consumer_task = asyncio.create_task(control_plane_consumer_fn())\n",
    "\n",
    "    # let things sync up\n",
    "    await asyncio.sleep(1)\n",
    "\n",
    "    # let things run\n",
    "    if message_queue_task:\n",
    "        all_tasks = [control_plane_task, consumer_task, message_queue_task]\n",
    "    else:\n",
    "        all_tasks = [control_plane_task, consumer_task]\n",
    "\n",
    "    shutdown_handler = _get_shutdown_handler(all_tasks)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    while loop.is_running():\n",
    "        await asyncio.sleep(0.1)\n",
    "        signal.signal(signal.SIGINT, shutdown_handler)\n",
    "\n",
    "        for task in all_tasks:\n",
    "            if task.done() and task.exception():  # type: ignore\n",
    "                raise task.exception()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from llama_index.core.workflow import Workflow\n",
    "from llama_deploy import (\n",
    "    WorkflowServiceConfig,\n",
    "    WorkflowService,\n",
    ")\n",
    "\n",
    "from llama_deploy.deploy.deploy import (\n",
    "                        _get_message_queue_config,\n",
    "                    )\n",
    "\n",
    "async def deploy_workflow(\n",
    "    workflow: Workflow,\n",
    "    workflow_config: WorkflowServiceConfig,\n",
    "    control_plane_config: ControlPlaneConfig,\n",
    ") -> None:\n",
    "    control_plane_url = control_plane_config.url\n",
    "\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.get(f\"{control_plane_url}/queue_config\")\n",
    "        queue_config_dict = response.json()\n",
    "\n",
    "    message_queue_config = _get_message_queue_config(queue_config_dict)\n",
    "    message_queue_client = _get_message_queue_client(message_queue_config)\n",
    "\n",
    "    service = WorkflowService(\n",
    "        workflow=workflow,\n",
    "        message_queue=message_queue_client,\n",
    "        **workflow_config.model_dump(),\n",
    "    )\n",
    "\n",
    "    service_task = asyncio.create_task(service.launch_server())\n",
    "\n",
    "    # let service spin up\n",
    "    await asyncio.sleep(1)\n",
    "\n",
    "    # register to message queue\n",
    "    consumer_fn = await service.register_to_message_queue()\n",
    "\n",
    "    # register to control plane\n",
    "    control_plane_url = (\n",
    "        f\"http://{control_plane_config.host}:{control_plane_config.port}\"\n",
    "    )\n",
    "    await service.register_to_control_plane(control_plane_url)\n",
    "\n",
    "    # create consumer task\n",
    "    consumer_task = asyncio.create_task(consumer_fn())\n",
    "\n",
    "    # let things sync up\n",
    "    await asyncio.sleep(1)\n",
    "\n",
    "    all_tasks = [consumer_task, service_task]\n",
    "\n",
    "    shutdown_handler = _get_shutdown_handler(all_tasks)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    while loop.is_running():\n",
    "        await asyncio.sleep(0.1)\n",
    "        signal.signal(signal.SIGINT, shutdown_handler)\n",
    "\n",
    "        for task in all_tasks:\n",
    "            if task.done() and task.exception():  # type: ignore\n",
    "                raise task.exception()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents import (\n",
    "    AgentService,\n",
    "    ToolService,\n",
    "    MetaServiceTool,\n",
    "    ControlPlaneServer,\n",
    "    SimpleMessageQueue,\n",
    "    AgentOrchestrator,\n",
    ")\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import ReActAgentWorker, ReActAgent\n",
    "\n",
    "\n",
    "\n",
    "# create our multi-agent framework components\n",
    "message_queue = SimpleMessageQueue()\n",
    "control_plane = ControlPlaneServer(\n",
    "    message_queue=message_queue,\n",
    "    orchestrator=AgentOrchestrator(llm=llm),\n",
    ")\n",
    "\n",
    "# define Tool Service\n",
    "tool_service = ToolService(\n",
    "    message_queue=message_queue,\n",
    "    tools=all_tools,\n",
    "    running=True,\n",
    "    step_interval=0.5,\n",
    ")\n",
    "\n",
    "# define meta-tools here\n",
    "meta_tools = [\n",
    "    await MetaServiceTool.from_tool_service(\n",
    "        t.metadata.name,\n",
    "        message_queue=message_queue,\n",
    "        tool_service=tool_service,\n",
    "    )\n",
    "    for t in all_tools\n",
    "]\n",
    "\n",
    "\n",
    "# define Agent and agent service\n",
    "# worker1 = FunctionCallingAgentWorker.from_tools(\n",
    "worker1 = ReActAgentWorker.from_tools(\n",
    "    meta_tools,\n",
    "    llm=llm,\n",
    ")\n",
    "agent1 = worker1.as_agent()\n",
    "agent_server_1 = AgentService(\n",
    "    agent=agent1,\n",
    "    message_queue=message_queue,\n",
    "    description=\"Agent that answers questions based on the newly proposed SEC rule.\",\n",
    "    service_name=\"rule_proposal_agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent1.chat('When are written comments on this notice of joint proposed rulemaking need to be submitted?')\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions\n",
    "# Name of rule \n",
    "# Is it a proposed rule or a final rule\n",
    "# Issue date and Federal Register date\n",
    "# What agency(ies) is the Rule coming from\n",
    "# If a proposed rule, when are public comments due by and where should they be sent (this info is in the Rule document under Dates and Addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "summarizer = TreeSummarize(llm=llm, verbose=True)\n",
    "# prompt_summary = \"You are a professional executive of AlphaTrAI. Your job is to summarize this text in great detail from a video transcription. The summary will be distributed to investors and stakeholders, so give a lot of details and examples from the transcription.\"\n",
    "prompt_summary = f\"\"\"You are a professional executive at AlphaTrAI. Your job is to summarize the text from a video transcription. The summary will be a memo distributed to investors and stakeholders. Be sure it the memo has the following items:\n",
    "1. Extract all the names of new hires and their position, and/or new advisors mentioned in the transcription.\n",
    "2. Create a section to mention the personnel new to AlphaTrAI.\n",
    "3. Include other highlights and progress made by AlphaTrAI.\n",
    "4. Ensure the memo and ensure it is factual, optimistic, and any values mention come directly from the text. \n",
    "\n",
    "The transcription is as follows:\\n{full_doc}\"\"\"\n",
    "\n",
    "response = await summarizer.aget_response(prompt_summary, [doc.text for doc in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM direct summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_summary = f\"\"\"You are a professional executive at AlphaTrAI. Your job is to summarize the text from a video transcription. The summary will be a memo distributed to investors and stakeholders. Be sure it the memo has the following items:\n",
    "1. Extract all the names of new hires and their position, and/or new advisors mentioned in the transcription.\n",
    "2. Include other highlights and progress made by AlphaTrAI.\n",
    "3. Ensure the memo is professional, fluid, factual, and optimistic. \n",
    "\n",
    "The transcription is as follows:\\n{full_doc}\"\"\"\n",
    "\n",
    "response = llm.complete(prompt_summary, max_tokens=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install llama-index-embeddings-huggingface llama-index-vector-stores-neo4jvector llama-index-graph-stores-neo4j\n",
    "!apt install dialog apt-utils -y (done above)\n",
    "!wget -O - https://debian.neo4j.com/neotechnology.gpg.key | gpg --dearmor -o /etc/apt/keyrings/neotechnology.gpg\n",
    "!echo 'deb [signed-by=/etc/apt/keyrings/neotechnology.gpg] https://debian.neo4j.com stable latest' | tee -a /etc/apt/sources.list.d/neo4j.list\n",
    "!apt list -a neo4j\n",
    "!add-apt-repository universe -y\n",
    "!apt install neo4j=1:5.22.0 -y\n",
    "!echo \"neo4j-enterprise neo4j/question select I ACCEPT\" | debconf-set-selections\n",
    "!echo \"neo4j-enterprise neo4j/license note\" | debconf-set-selections\n",
    "!apt install openjdk-17-jre -y\n",
    "!cd /var/lib/neo4j/plugins/ && wget https://github.com/neo4j/apoc/releases/download/5.22.0/apoc-5.22.0-core.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_neo4j_password('bewaretheneo')\n",
    "add_lines_to_conf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from llama_index.core.agent import ReActAgent\n",
    "# from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from rag_utils import create_neo4j_graph_store, create_neo4j_graphrag, neo4j_query, set_neo4j_password, add_lines_to_conf\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm.is_function_calling_model = True\n",
    "\n",
    "embed_model_name = \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\"\n",
    "print(\"loading embed model...\")\n",
    "embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 300\n",
    "Settings.chunk_overlap = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor\n",
    "\n",
    "entities = Literal[\"PEOPLE\", \n",
    "                   \"PLACE\"\n",
    "]\n",
    "\n",
    "relations = Literal[\n",
    "    \"ROLE\",\n",
    "    \"COMPANY\"\n",
    "]\n",
    "\n",
    "validation_schema = {\n",
    "    \"People\": [\"ROLE\"],\n",
    "    \"Place\": [\"COMPANY\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.chunk_size = 300\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "kg_extractor = SchemaLLMPathExtractor(\n",
    "    llm=llm,\n",
    "    possible_entities=entities,\n",
    "    possible_relations=relations,\n",
    "    kg_validation_schema=validation_schema,\n",
    "    strict=True,  # if false, will allow triples outside of the schema\n",
    "    num_workers=4,\n",
    "    max_triplets_per_chunk=10,\n",
    ")\n",
    "\n",
    "graph_store = create_neo4j_graph_store(neo_url=\"bolt://localhost:7687\", \n",
    "                                       password=os.getenv(\"NEO4J_PWD\"), \n",
    "                                       config={\"connection_timeout\": 240, \"connection_acquisition_timeout\": 240, \"max_connection_pool_size\": 1000})\n",
    "neo4j_query(graph_store, query=\"\"\"MATCH (n) DETACH DELETE n\"\"\")\n",
    "\n",
    "\n",
    "graph_index = create_neo4j_graphrag(documents, llm, embed_model, kg_extractor, graph_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "query_engine_tools = QueryEngineTool(\n",
    "            query_engine=graph_index,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"graph_tool\",\n",
    "                description=(\n",
    "                    \"Useful for finding people names and roles, and the company they work for.\"\n",
    "                ),\n",
    "            ),\n",
    "        ),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install llama-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents import (\n",
    "    AgentService,\n",
    "    ToolService,\n",
    "    MetaServiceTool,\n",
    "    ControlPlaneServer,\n",
    "    SimpleMessageQueue,\n",
    "    AgentOrchestrator,\n",
    ")\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker, ReActAgentWorker, ReActAgent, LATSAgentWorker\n",
    "\n",
    "\n",
    "\n",
    "# create our multi-agent framework components\n",
    "message_queue = SimpleMessageQueue()\n",
    "control_plane = ControlPlaneServer(\n",
    "    message_queue=message_queue,\n",
    "    orchestrator=AgentOrchestrator(llm=llm),\n",
    ")\n",
    "\n",
    "# define Tool Service\n",
    "tool_service = ToolService(\n",
    "    message_queue=message_queue,\n",
    "    tools=[query_engine_tools],#, adding_tool],\n",
    "    running=True,\n",
    "    step_interval=0.5,\n",
    ")\n",
    "\n",
    "# define meta-tools here\n",
    "meta_tools = [\n",
    "    await MetaServiceTool.from_tool_service(\n",
    "        t.metadata.name,\n",
    "        message_queue=message_queue,\n",
    "        tool_service=tool_service,\n",
    "    )\n",
    "    for t in [query_engine_tools]#, adding_tool]\n",
    "]\n",
    "\n",
    "\n",
    "# define Agent and agent service\n",
    "# worker1 = FunctionCallingAgentWorker.from_tools(\n",
    "worker1 = ReActAgentWorker.from_tools(\n",
    "    meta_tools,\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "worker2 = LATSAgentWorker.from_tools(\n",
    "    meta_tools,\n",
    "    llm=llm,\n",
    "    num_expansions=2,\n",
    "    max_rollouts=3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent1 = worker1.as_agent()\n",
    "agent_server_1 = AgentService(\n",
    "    agent=agent1,\n",
    "    message_queue=message_queue,\n",
    "    description=\"Summarize a transcription as a memo for investors and stakeholders.\",\n",
    "    service_name=\"summarize_transcription\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
