{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short list of objectives\n",
    "\n",
    "- Instruction pretrain/fine-tuning LLM https://github.com/yizhongw/self-instruct/blob/main/data/seed_tasks.jsonl\n",
    "    - Llama3 paper https://arxiv.org/pdf/2407.21783\n",
    "- Use Ollama embeddings \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1581 B]\n",
      "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \n",
      "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1027 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]                \n",
      "Get:5 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3113 kB]\n",
      "Get:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2318 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.7 kB]\n",
      "Get:9 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1159 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]       \n",
      "Get:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [27.8 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2596 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1447 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3191 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [51.8 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.7 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [81.4 kB]\n",
      "Fetched 35.5 MB in 3s (10.9 MB/s)                           \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libevent-core-2.1-7 libgpm2 libsodium23 libutempter0 vim-common vim-runtime\n",
      "  xxd\n",
      "Suggested packages:\n",
      "  gpm ctags vim-doc vim-scripts\n",
      "The following NEW packages will be installed:\n",
      "  libevent-core-2.1-7 libgpm2 libsodium23 libutempter0 tmux vim vim-common\n",
      "  vim-runtime xxd\n",
      "0 upgraded, 9 newly installed, 0 to remove and 96 not upgraded.\n",
      "Need to get 9404 kB of archives.\n",
      "After this operation, 40.1 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xxd amd64 2:8.2.3995-1ubuntu2.19 [52.9 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 vim-common all 2:8.2.3995-1ubuntu2.19 [81.5 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libevent-core-2.1-7 amd64 2.1.12-stable-1build3 [93.9 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgpm2 amd64 1.20.7-10build1 [15.3 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsodium23 amd64 1.0.18-1build2 [164 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libutempter0 amd64 1.2.1-2build2 [8848 B]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 tmux amd64 3.2a-4ubuntu0.2 [428 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 vim-runtime all 2:8.2.3995-1ubuntu2.19 [6826 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 vim amd64 2:8.2.3995-1ubuntu2.19 [1733 kB]\n",
      "Fetched 9404 kB in 1s (10.5 MB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package xxd.\n",
      "(Reading database ... 21038 files and directories currently installed.)\n",
      "Preparing to unpack .../0-xxd_2%3a8.2.3995-1ubuntu2.19_amd64.deb ...\n",
      "Unpacking xxd (2:8.2.3995-1ubuntu2.19) ...\n",
      "Selecting previously unselected package vim-common.\n",
      "Preparing to unpack .../1-vim-common_2%3a8.2.3995-1ubuntu2.19_all.deb ...\n",
      "Unpacking vim-common (2:8.2.3995-1ubuntu2.19) ...\n",
      "Selecting previously unselected package libevent-core-2.1-7:amd64.\n",
      "Preparing to unpack .../2-libevent-core-2.1-7_2.1.12-stable-1build3_amd64.deb ...\n",
      "Unpacking libevent-core-2.1-7:amd64 (2.1.12-stable-1build3) ...\n",
      "Selecting previously unselected package libgpm2:amd64.\n",
      "Preparing to unpack .../3-libgpm2_1.20.7-10build1_amd64.deb ...\n",
      "Unpacking libgpm2:amd64 (1.20.7-10build1) ...\n",
      "Selecting previously unselected package libsodium23:amd64.\n",
      "Preparing to unpack .../4-libsodium23_1.0.18-1build2_amd64.deb ...\n",
      "Unpacking libsodium23:amd64 (1.0.18-1build2) ...\n",
      "Selecting previously unselected package libutempter0:amd64.\n",
      "Preparing to unpack .../5-libutempter0_1.2.1-2build2_amd64.deb ...\n",
      "Unpacking libutempter0:amd64 (1.2.1-2build2) ...\n",
      "Selecting previously unselected package tmux.\n",
      "Preparing to unpack .../6-tmux_3.2a-4ubuntu0.2_amd64.deb ...\n",
      "Unpacking tmux (3.2a-4ubuntu0.2) ...\n",
      "Selecting previously unselected package vim-runtime.\n",
      "Preparing to unpack .../7-vim-runtime_2%3a8.2.3995-1ubuntu2.19_all.deb ...\n",
      "Adding 'diversion of /usr/share/vim/vim82/doc/help.txt to /usr/share/vim/vim82/doc/help.txt.vim-tiny by vim-runtime'\n",
      "Adding 'diversion of /usr/share/vim/vim82/doc/tags to /usr/share/vim/vim82/doc/tags.vim-tiny by vim-runtime'\n",
      "Unpacking vim-runtime (2:8.2.3995-1ubuntu2.19) ...\n",
      "Selecting previously unselected package vim.\n",
      "Preparing to unpack .../8-vim_2%3a8.2.3995-1ubuntu2.19_amd64.deb ...\n",
      "Unpacking vim (2:8.2.3995-1ubuntu2.19) ...\n",
      "Setting up libsodium23:amd64 (1.0.18-1build2) ...\n",
      "Setting up libgpm2:amd64 (1.20.7-10build1) ...\n",
      "Setting up xxd (2:8.2.3995-1ubuntu2.19) ...\n",
      "Setting up vim-common (2:8.2.3995-1ubuntu2.19) ...\n",
      "Setting up libevent-core-2.1-7:amd64 (2.1.12-stable-1build3) ...\n",
      "Setting up libutempter0:amd64 (1.2.1-2build2) ...\n",
      "Setting up vim-runtime (2:8.2.3995-1ubuntu2.19) ...\n",
      "Setting up tmux (3.2a-4ubuntu0.2) ...\n",
      "Setting up vim (2:8.2.3995-1ubuntu2.19) ...\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vim (vim) in auto mode\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vimdiff (vimdiff) in auto mode\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/rvim (rvim) in auto mode\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/rview (rview) in auto mode\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vi (vi) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/da/man1/vi.1.gz because associated file /usr/share/man/da/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/de/man1/vi.1.gz because associated file /usr/share/man/de/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/vi.1.gz because associated file /usr/share/man/fr/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/it/man1/vi.1.gz because associated file /usr/share/man/it/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ja/man1/vi.1.gz because associated file /usr/share/man/ja/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/pl/man1/vi.1.gz because associated file /usr/share/man/pl/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ru/man1/vi.1.gz because associated file /usr/share/man/ru/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/vi.1.gz because associated file /usr/share/man/man1/vim.1.gz (of link group vi) doesn't exist\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/view (view) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/da/man1/view.1.gz because associated file /usr/share/man/da/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/de/man1/view.1.gz because associated file /usr/share/man/de/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/view.1.gz because associated file /usr/share/man/fr/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/it/man1/view.1.gz because associated file /usr/share/man/it/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ja/man1/view.1.gz because associated file /usr/share/man/ja/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/pl/man1/view.1.gz because associated file /usr/share/man/pl/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ru/man1/view.1.gz because associated file /usr/share/man/ru/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/view.1.gz because associated file /usr/share/man/man1/vim.1.gz (of link group view) doesn't exist\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/ex (ex) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/da/man1/ex.1.gz because associated file /usr/share/man/da/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/de/man1/ex.1.gz because associated file /usr/share/man/de/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/ex.1.gz because associated file /usr/share/man/fr/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/it/man1/ex.1.gz because associated file /usr/share/man/it/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ja/man1/ex.1.gz because associated file /usr/share/man/ja/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/pl/man1/ex.1.gz because associated file /usr/share/man/pl/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ru/man1/ex.1.gz because associated file /usr/share/man/ru/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/ex.1.gz because associated file /usr/share/man/man1/vim.1.gz (of link group ex) doesn't exist\n",
      "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/editor (editor) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/da/man1/editor.1.gz because associated file /usr/share/man/da/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/de/man1/editor.1.gz because associated file /usr/share/man/de/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/editor.1.gz because associated file /usr/share/man/fr/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/it/man1/editor.1.gz because associated file /usr/share/man/it/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ja/man1/editor.1.gz because associated file /usr/share/man/ja/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/pl/man1/editor.1.gz because associated file /usr/share/man/pl/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/ru/man1/editor.1.gz because associated file /usr/share/man/ru/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/editor.1.gz because associated file /usr/share/man/man1/vim.1.gz (of link group editor) doesn't exist\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.6) ...\n",
      "Collecting llama-index\n",
      "  Downloading llama_index-0.11.16-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-parse\n",
      "  Downloading llama_parse-0.5.7-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting llama_deploy\n",
      "  Downloading llama_deploy-0.1.3-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting llama-index-llms-huggingface\n",
      "  Downloading llama_index_llms_huggingface-0.3.4-py3-none-any.whl.metadata (840 bytes)\n",
      "Collecting llama-index-embeddings-huggingface\n",
      "  Downloading llama_index_embeddings_huggingface-0.3.1-py3-none-any.whl.metadata (718 bytes)\n",
      "Collecting llama-index-llms-ollama\n",
      "  Downloading llama_index_llms_ollama-0.3.3-py3-none-any.whl.metadata (668 bytes)\n",
      "Collecting llama-index-embeddings-ollama\n",
      "  Downloading llama_index_embeddings_ollama-0.3.1-py3-none-any.whl.metadata (693 bytes)\n",
      "Collecting llama-index-vector-stores-neo4jvector\n",
      "  Downloading llama_index_vector_stores_neo4jvector-0.2.2-py3-none-any.whl.metadata (717 bytes)\n",
      "Collecting llama-index-graph-stores-neo4j\n",
      "  Downloading llama_index_graph_stores_neo4j-0.3.2-py3-none-any.whl.metadata (696 bytes)\n",
      "Collecting llama-index-finetuning\n",
      "  Downloading llama_index_finetuning-0.2.1-py3-none-any.whl.metadata (992 bytes)\n",
      "Collecting llama-index-utils-workflow\n",
      "  Downloading llama_index_utils_workflow-0.2.1-py3-none-any.whl.metadata (667 bytes)\n",
      "Collecting llama-index-readers-file\n",
      "  Downloading llama_index_readers_file-0.2.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-llms-llama-cpp\n",
      "  Downloading llama_index_llms_llama_cpp-0.2.2-py3-none-any.whl.metadata (695 bytes)\n",
      "Collecting llama-index-retrievers-you\n",
      "  Downloading llama_index_retrievers_you-0.2.0-py3-none-any.whl.metadata (655 bytes)\n",
      "Collecting llama-index-agent-openai<0.4.0,>=0.3.4 (from llama-index)\n",
      "  Downloading llama_index_agent_openai-0.3.4-py3-none-any.whl.metadata (728 bytes)\n",
      "Collecting llama-index-cli<0.4.0,>=0.3.1 (from llama-index)\n",
      "  Downloading llama_index_cli-0.3.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.12.0,>=0.11.16 (from llama-index)\n",
      "  Downloading llama_index_core-0.11.16-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting llama-index-embeddings-openai<0.3.0,>=0.2.4 (from llama-index)\n",
      "  Downloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl.metadata (686 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.4.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
      "  Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.3.0,>=0.2.10 (from llama-index)\n",
      "  Downloading llama_index_llms_openai-0.2.11-py3-none-any.whl.metadata (649 bytes)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 (from llama-index)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.2.2-py3-none-any.whl.metadata (678 bytes)\n",
      "Collecting llama-index-program-openai<0.3.0,>=0.2.0 (from llama-index)\n",
      "  Downloading llama_index_program_openai-0.2.0-py3-none-any.whl.metadata (766 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.3.0,>=0.2.0 (from llama-index)\n",
      "  Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-llama-parse>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting nltk>3.8.1 (from llama-index)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fastapi>=0.109.1 (from llama_deploy)\n",
      "  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting pydantic-settings<3.0,>=2.0 (from llama_deploy)\n",
      "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting pytest-asyncio<0.24.0,>=0.23.7 (from llama_deploy)\n",
      "  Downloading pytest_asyncio-0.23.8-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting pytest-mock<4.0.0,>=3.14.0 (from llama_deploy)\n",
      "  Downloading pytest_mock-3.14.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting uvicorn>=0.12.0 (from llama_deploy)\n",
      "  Downloading uvicorn-0.31.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting huggingface-hub<0.24.0,>=0.23.0 (from llama-index-llms-huggingface)\n",
      "  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting text-generation<0.8.0,>=0.7.0 (from llama-index-llms-huggingface)\n",
      "  Downloading text_generation-0.7.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting torch<3.0.0,>=2.1.2 (from llama-index-llms-huggingface)\n",
      "  Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting transformers<5.0.0,>=4.37.0 (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface)\n",
      "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentence-transformers>=2.6.1 (from llama-index-embeddings-huggingface)\n",
      "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting ollama>=0.3.0 (from llama-index-llms-ollama)\n",
      "  Downloading ollama-0.3.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting neo4j<6.0.0,>=5.16.0 (from llama-index-vector-stores-neo4jvector)\n",
      "  Downloading neo4j-5.25.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting llama-index-embeddings-adapter<0.3.0,>=0.2.0 (from llama-index-finetuning)\n",
      "  Downloading llama_index_embeddings_adapter-0.2.2-py3-none-any.whl.metadata (688 bytes)\n",
      "Collecting llama-index-llms-azure-openai<0.3.0,>=0.2.0 (from llama-index-finetuning)\n",
      "  Downloading llama_index_llms_azure_openai-0.2.1-py3-none-any.whl.metadata (779 bytes)\n",
      "Collecting llama-index-llms-mistralai<0.3.0,>=0.2.0 (from llama-index-finetuning)\n",
      "  Downloading llama_index_llms_mistralai-0.2.5-py3-none-any.whl.metadata (677 bytes)\n",
      "Collecting llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0 (from llama-index-finetuning)\n",
      "  Downloading llama_index_postprocessor_cohere_rerank-0.2.1-py3-none-any.whl.metadata (723 bytes)\n",
      "Collecting pyvis<0.4.0,>=0.3.2 (from llama-index-utils-workflow)\n",
      "  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from llama-index-readers-file)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting pandas (from llama-index-readers-file)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file)\n",
      "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-cpp-python<0.3.0,>=0.2.32 (from llama-index-llms-llama-cpp)\n",
      "  Downloading llama_cpp_python-0.2.90.tar.gz (63.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file)\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting starlette<0.39.0,>=0.37.2 (from fastapi>=0.109.1->llama_deploy)\n",
      "  Downloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 (from fastapi>=0.109.1->llama_deploy)\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions>=4.8.0 (from fastapi>=0.109.1->llama_deploy)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting filelock (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting packaging>=20.9 (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface)\n",
      "  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface)\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting requests (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.42.1 (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Downloading aiohttp-3.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting minijinja>=1.0 (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Downloading minijinja-2.2.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting numpy>=1.20.0 (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp)\n",
      "  Downloading numpy-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting jinja2>=2.11.3 (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp)\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.4.0,>=0.3.4->llama-index)\n",
      "  Downloading openai-1.51.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.16->llama-index)\n",
      "  Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting dataclasses-json (from llama-index-core<0.12.0,>=0.11.16->llama-index)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.12.0,>=0.11.16->llama-index)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.12.0,>=0.11.16->llama-index)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting httpx (from llama-index-core<0.12.0,>=0.11.16->llama-index)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting nest-asyncio<2.0.0,>=1.5.8 (from llama-index-core<0.12.0,>=0.11.16->llama-index)\n",
      "  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting networkx>=3.0 (from llama-index-core<0.12.0,>=0.11.16->llama-index)\n",
      "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting numpy>=1.20.0 (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pillow>=9.0.0 (from llama-index-core<0.12.0,>=0.11.16->llama-index)\n",
      "  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core<0.12.0,>=0.11.16->llama-index)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.12.0,>=0.11.16->llama-index)\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.12.0,>=0.11.16->llama-index)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core<0.12.0,>=0.11.16->llama-index)\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index)\n",
      "  Downloading llama_cloud-0.1.2-py3-none-any.whl.metadata (763 bytes)\n",
      "Collecting azure-identity<2.0.0,>=1.15.0 (from llama-index-llms-azure-openai<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading azure_identity-1.18.0-py3-none-any.whl.metadata (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mistralai>=1.0.0 (from llama-index-llms-mistralai<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading mistralai-1.1.0-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting cohere<6.0.0,>=5.1.1 (from llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading cohere-5.11.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting pytz (from neo4j<6.0.0,>=5.16.0->llama-index-vector-stores-neo4jvector)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting click (from nltk>3.8.1->llama-index)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk>3.8.1->llama-index)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk>3.8.1->llama-index)\n",
      "  Downloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-dotenv>=0.21.0 (from pydantic-settings<3.0,>=2.0->llama_deploy)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting pytest<9,>=7.0.0 (from pytest-asyncio<0.24.0,>=0.23.7->llama_deploy)\n",
      "  Downloading pytest-8.3.3-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting ipython>=5.3.0 (from pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow)\n",
      "  Downloading ipython-8.28.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting jsonpickle>=1.4.1 (from pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow)\n",
      "  Downloading jsonpickle-3.3.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting scikit-learn (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting scipy (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sympy (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface)\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface)\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface)\n",
      "  Downloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting accelerate>=0.26.0 (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface)\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting h11>=0.8 (from uvicorn>=0.12.0->llama_deploy)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas->llama-index-readers-file)\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->llama-index-readers-file)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting psutil (from accelerate>=0.26.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface)\n",
      "  Downloading psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Downloading yarl-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0 (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting azure-core>=1.31.0 (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading azure_core-1.31.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting cryptography>=2.5 (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading cryptography-43.0.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting msal>=1.30.0 (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading msal-1.31.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting msal-extensions>=1.2.0 (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading msal_extensions-1.2.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting boto3<2.0.0,>=1.34.0 (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading boto3-1.35.34-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting httpx-sse==0.4.0 (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting parameterized<0.10.0,>=0.9.0 (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
      "Collecting pydantic-core<3.0.0,>=2.18.2 (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading pydantic_core-2.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting sagemaker<3.0.0,>=2.232.1 (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading sagemaker-2.232.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading types_requests-2.32.0.20240914-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting anyio (from httpx->llama-index-core<0.12.0,>=0.11.16->llama-index)\n",
      "  Downloading anyio-4.6.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting certifi (from httpx->llama-index-core<0.12.0,>=0.11.16->llama-index)\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx->llama-index-core<0.12.0,>=0.11.16->llama-index)\n",
      "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx->llama-index-core<0.12.0,>=0.11.16->llama-index)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sniffio (from httpx->llama-index-core<0.12.0,>=0.11.16->llama-index)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting decorator (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow)\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting jedi>=0.16 (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow)\n",
      "  Downloading jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting matplotlib-inline (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow)\n",
      "  Downloading matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting prompt-toolkit<3.1.0,>=3.0.41 (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow)\n",
      "  Downloading prompt_toolkit-3.0.48-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting pygments>=2.4.0 (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow)\n",
      "  Downloading pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting stack-data (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow)\n",
      "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting traitlets>=5.13.0 (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow)\n",
      "  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting exceptiongroup (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow)\n",
      "  Using cached exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting pexpect>4.3 (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow)\n",
      "  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp)\n",
      "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting eval-type-backport<0.3.0,>=0.2.0 (from mistralai>=1.0.0->llama-index-llms-mistralai<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading eval_type_backport-0.2.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting jsonpath-python<2.0.0,>=1.0.6 (from mistralai>=1.0.0->llama-index-llms-mistralai<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas->llama-index-readers-file)\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->llama-index-readers-file)\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index)\n",
      "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.109.1->llama_deploy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core<3.0.0,>=2.18.2 (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting iniconfig (from pytest<9,>=7.0.0->pytest-asyncio<0.24.0,>=0.23.7->llama_deploy)\n",
      "  Downloading iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting pluggy<2,>=1.5 (from pytest<9,>=7.0.0->pytest-asyncio<0.24.0,>=0.23.7->llama_deploy)\n",
      "  Downloading pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting tomli>=1 (from pytest<9,>=7.0.0->pytest-asyncio<0.24.0,>=0.23.7->llama_deploy)\n",
      "  Using cached tomli-2.0.2-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface)\n",
      "  Downloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface)\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.16->llama-index)\n",
      "  Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.16->llama-index)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.12.0,>=0.11.16->llama-index)\n",
      "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting botocore<1.36.0,>=1.35.34 (from boto3<2.0.0,>=1.34.0->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading botocore-1.35.34-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting cffi>=1.12 (from cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting parso<0.9.0,>=0.8.3 (from jedi>=0.16->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow)\n",
      "  Downloading parso-0.8.4-py2.py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting PyJWT<3,>=1.0.0 (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading PyJWT-2.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting portalocker<3,>=1.4 (from msal-extensions>=1.2.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting ptyprocess>=0.5 (from pexpect>4.3->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow)\n",
      "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting wcwidth (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow)\n",
      "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting cloudpickle==2.2.1 (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting docker (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting google-pasta (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting importlib-metadata<7.0,>=1.4.0 (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting jsonschema (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting pathos (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading pathos-0.3.3-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting platformdirs (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading platformdirs-4.3.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting protobuf<5.0,>=3.12 (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting sagemaker-core<2.0.0,>=1.0.0 (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading sagemaker_core-1.0.10-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting sagemaker-mlflow (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading sagemaker_mlflow-0.1.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting schema (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading schema-0.7.7-py2.py3-none-any.whl.metadata (34 kB)\n",
      "Collecting smdebug-rulesconfig==1.0.1 (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting tblib<4,>=1.7.0 (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading tblib-3.0.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting executing>=1.2.0 (from stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow)\n",
      "  Downloading executing-2.1.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting asttokens>=2.1.0 (from stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow)\n",
      "  Downloading asttokens-2.4.1-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pure-eval (from stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow)\n",
      "  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting zipp>=0.5 (from importlib-metadata<7.0,>=1.4.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading zipp-3.20.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting rich<14.0.0,>=13.0.0 (from sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading rich-13.9.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting mock<5.0,>4.0 (from sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading mock-4.0.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading rpds_py-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting ppft>=1.7.6.9 (from pathos->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading ppft-1.7.6.9-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting dill>=0.3.9 (from pathos->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pox>=0.3.5 (from pathos->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading pox-0.3.5-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting multiprocess>=0.70.17 (from pathos->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting mlflow>=2.8 (from sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading mlflow-2.16.2-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting mlflow-skinny==2.16.2 (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading mlflow_skinny-2.16.2-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting Flask<4 (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting alembic!=1.10.0,<2 (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting graphene<4 (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading graphene-3.3-py2.py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting markdown<4,>=3.3 (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting matplotlib<4 (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting pyarrow<18,>=4.0.0 (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting gunicorn<24 (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting cachetools<6,>=5.0.0 (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading databricks_sdk-0.33.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting gitpython<4,>=3.1.9 (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading sqlparse-0.5.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting Mako (from alembic!=1.10.0,<2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting Werkzeug>=3.0.0 (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading werkzeug-3.0.4-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting itsdangerous>=2.1.2 (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting blinker>=1.6.2 (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading graphql_core-3.2.4-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting aniso8601<10,>=8 (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading contourpy-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading fonttools-4.54.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.7/163.7 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading kiwisolver-1.4.7-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting google-auth~=2.0 (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading google_auth-2.35.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading llama_index-0.11.16-py3-none-any.whl (6.8 kB)\n",
      "Downloading llama_parse-0.5.7-py3-none-any.whl (10 kB)\n",
      "Downloading llama_deploy-0.1.3-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_llms_huggingface-0.3.4-py3-none-any.whl (11 kB)\n",
      "Downloading llama_index_embeddings_huggingface-0.3.1-py3-none-any.whl (8.6 kB)\n",
      "Downloading llama_index_llms_ollama-0.3.3-py3-none-any.whl (4.7 kB)\n",
      "Downloading llama_index_embeddings_ollama-0.3.1-py3-none-any.whl (2.6 kB)\n",
      "Downloading llama_index_vector_stores_neo4jvector-0.2.2-py3-none-any.whl (7.3 kB)\n",
      "Downloading llama_index_graph_stores_neo4j-0.3.2-py3-none-any.whl (12 kB)\n",
      "Downloading llama_index_finetuning-0.2.1-py3-none-any.whl (30 kB)\n",
      "Downloading llama_index_utils_workflow-0.2.1-py3-none-any.whl (2.6 kB)\n",
      "Downloading llama_index_readers_file-0.2.2-py3-none-any.whl (38 kB)\n",
      "Downloading llama_index_llms_llama_cpp-0.2.2-py3-none-any.whl (6.0 kB)\n",
      "Downloading llama_index_retrievers_you-0.2.0-py3-none-any.whl (3.1 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.5-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.8/402.8 kB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_agent_openai-0.3.4-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_cli-0.3.1-py3-none-any.whl (27 kB)\n",
      "Downloading llama_index_core-0.11.16-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_embeddings_adapter-0.2.2-py3-none-any.whl (4.5 kB)\n",
      "Downloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl (6.1 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.4.0-py3-none-any.whl (10 kB)\n",
      "Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_llms_azure_openai-0.2.1-py3-none-any.whl (5.1 kB)\n",
      "Downloading llama_index_llms_mistralai-0.2.5-py3-none-any.whl (5.8 kB)\n",
      "Downloading llama_index_llms_openai-0.2.11-py3-none-any.whl (12 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.2.2-py3-none-any.whl (5.9 kB)\n",
      "Downloading llama_index_postprocessor_cohere_rerank-0.2.1-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_program_openai-0.2.0-py3-none-any.whl (5.3 kB)\n",
      "Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl (2.5 kB)\n",
      "Downloading neo4j-5.25.0-py3-none-any.whl (296 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.6/296.6 kB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m141.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ollama-0.3.3-py3-none-any.whl (10 kB)\n",
      "Downloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
      "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytest_asyncio-0.23.8-py3-none-any.whl (17 kB)\n",
      "Downloading pytest_mock-3.14.0-py3-none-any.whl (9.9 kB)\n",
      "Downloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading text_generation-0.7.0-py3-none-any.whl (12 kB)\n",
      "Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m118.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading uvicorn-0.31.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading azure_identity-1.18.0-py3-none-any.whl (187 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cohere-5.11.0-py3-none-any.whl (249 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.2/249.2 kB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ipython-8.28.0-py3-none-any.whl (819 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.5/819.5 kB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonpickle-3.3.0-py3-none-any.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_cloud-0.1.2-py3-none-any.whl (173 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.8/173.8 kB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading minijinja-2.2.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (861 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m861.9/861.9 kB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mistralai-1.1.0-py3-none-any.whl (229 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.7/229.7 kB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
      "Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m114.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.51.0-py3-none-any.whl (383 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.5/383.5 kB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached packaging-24.1-py3-none-any.whl (53 kB)\n",
      "Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytest-8.3.3-py3-none-any.whl (342 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.3/342.3 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.7/782.7 kB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.38.6-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading anyio-4.6.0-py3-none-any.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.6/89.6 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading azure_core-1.31.0-py3-none-any.whl (197 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.4/197.4 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading boto3-1.35.34-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.3/167.3 kB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.1/142.1 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cryptography-43.0.1-cp39-abi3-manylinux_2_28_x86_64.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading eval_type_backport-0.2.0-py3-none-any.whl (5.9 kB)\n",
      "Using cached exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
      "Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (599 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.5/599.5 kB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m118.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading msal-1.31.0-py3-none-any.whl (113 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.1/113.1 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading msal_extensions-1.2.0-py3-none-any.whl (19 kB)\n",
      "Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
      "Downloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
      "Downloading prompt_toolkit-3.0.48-py3-none-any.whl (386 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sagemaker-2.232.2-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached tomli-2.0.2-py3-none-any.whl (13 kB)\n",
      "Downloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading types_requests-2.32.0.20240914-py3-none-any.whl (15 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (447 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.9/447.9 kB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)\n",
      "Downloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (290 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.5/290.5 kB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
      "Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
      "Downloading botocore-1.35.34-py3-none-any.whl (12.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (446 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m446.2/446.2 kB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading executing-2.1.0-py2.py3-none-any.whl (25 kB)\n",
      "Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading parso-0.8.4-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.7/103.7 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Downloading PyJWT-2.9.0-py3-none-any.whl (22 kB)\n",
      "Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sagemaker_core-1.0.10-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.4/388.4 kB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.5/88.5 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading platformdirs-4.3.6-py3-none-any.whl (18 kB)\n",
      "Downloading tblib-3.0.0-py3-none-any.whl (12 kB)\n",
      "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pathos-0.3.3-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
      "Downloading sagemaker_mlflow-0.1.0-py3-none-any.whl (24 kB)\n",
      "Downloading schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
      "Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Downloading mlflow-2.16.2-py3-none-any.whl (26.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading mlflow_skinny-2.16.2-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
      "Downloading multiprocess-0.70.17-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pox-0.3.5-py3-none-any.whl (29 kB)\n",
      "Downloading ppft-1.7.6.9-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Downloading rich-13.9.2-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.1/242.1 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rpds_py-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (354 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.8/354.8 kB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.6/117.6 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graphene-3.3-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading contourpy-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading databricks_sdk-0.33.0-py3-none-any.whl (562 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.0/563.0 kB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fonttools-4.54.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graphql_core-3.2.4-py3-none-any.whl (203 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading kiwisolver-1.4.7-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sqlparse-0.5.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.0.4-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.35.0-py2.py3-none-any.whl (208 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.0/209.0 kB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.1/83.1 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.90-cp310-cp310-linux_x86_64.whl size=3396519 sha256=fad2e8073ce1e953c57f9ee4f339f82529c34b5d19f09670784339a0e74d0283\n",
      "  Stored in directory: /root/.cache/pip/wheels/3d/67/02/f950031435db4a5a02e6269f6adb6703bf1631c3616380f3c6\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: wcwidth, striprtf, schema, pytz, pure-eval, ptyprocess, mpmath, dirtyjson, aniso8601, zipp, wrapt, urllib3, tzdata, typing-extensions, traitlets, tqdm, tomli, threadpoolctl, tenacity, tblib, sympy, sqlparse, soupsieve, sniffio, smmap, smdebug-rulesconfig, six, safetensors, rpds-py, regex, pyyaml, python-dotenv, pyparsing, PyJWT, pygments, pycparser, pyasn1, psutil, protobuf, prompt-toolkit, ppft, pox, portalocker, pluggy, platformdirs, pillow, pexpect, parso, parameterized, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, nest-asyncio, neo4j, mypy-extensions, mock, minijinja, mdurl, MarkupSafe, markdown, kiwisolver, jsonpickle, jsonpath-python, joblib, jmespath, jiter, itsdangerous, iniconfig, idna, httpx-sse, h11, greenlet, graphql-core, fsspec, frozenlist, fonttools, filelock, fastavro, executing, exceptiongroup, eval-type-backport, distro, diskcache, dill, decorator, cycler, cloudpickle, click, charset-normalizer, certifi, cachetools, blinker, attrs, async-timeout, annotated-types, aiohappyeyeballs, Werkzeug, uvicorn, typing-inspect, types-requests, triton, SQLAlchemy, scipy, rsa, requests, referencing, python-dateutil, pytest, pypdf, pydantic-core, pyasn1-modules, pyarrow, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nltk, multiprocess, multidict, matplotlib-inline, marshmallow, markdown-it-py, Mako, jinja2, jedi, importlib-metadata, httpcore, gunicorn, graphql-relay, google-pasta, gitdb, deprecated, contourpy, cffi, beautifulsoup4, asttokens, anyio, aiosignal, yarl, tiktoken, starlette, stack-data, scikit-learn, rich, pytest-mock, pytest-asyncio, pydantic, pathos, pandas, opentelemetry-api, nvidia-cusolver-cu12, matplotlib, llama-cpp-python, jsonschema-specifications, huggingface-hub, httpx, graphene, google-auth, gitpython, Flask, docker, dataclasses-json, cryptography, botocore, azure-core, alembic, torch, tokenizers, s3transfer, pydantic-settings, opentelemetry-semantic-conventions, openai, ollama, mistralai, llama-cloud, jsonschema, ipython, fastapi, databricks-sdk, aiohttp, transformers, text-generation, pyvis, opentelemetry-sdk, msal, llama-index-legacy, llama-index-core, boto3, accelerate, sentence-transformers, sagemaker-core, msal-extensions, mlflow-skinny, llama-parse, llama-index-vector-stores-neo4jvector, llama-index-utils-workflow, llama-index-retrievers-you, llama-index-readers-file, llama-index-llms-openai, llama-index-llms-ollama, llama-index-llms-mistralai, llama-index-llms-llama-cpp, llama-index-indices-managed-llama-cloud, llama-index-graph-stores-neo4j, llama-index-embeddings-openai, llama-index-embeddings-ollama, llama-index-embeddings-adapter, llama_deploy, mlflow, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-llms-huggingface, llama-index-embeddings-huggingface, llama-index-cli, llama-index-agent-openai, azure-identity, sagemaker-mlflow, llama-index-program-openai, llama-index-llms-azure-openai, sagemaker, llama-index-question-gen-openai, llama-index, cohere, llama-index-postprocessor-cohere-rerank, llama-index-finetuning\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.2.0 requires torch==2.2.0, but you have torch 2.4.1 which is incompatible.\n",
      "torchvision 0.17.0 requires torch==2.2.0, but you have torch 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Flask-3.0.3 Mako-1.3.5 MarkupSafe-2.1.5 PyJWT-2.9.0 SQLAlchemy-2.0.35 Werkzeug-3.0.4 accelerate-0.34.2 aiohappyeyeballs-2.4.3 aiohttp-3.10.9 aiosignal-1.3.1 alembic-1.13.3 aniso8601-9.0.1 annotated-types-0.7.0 anyio-4.2.0 asttokens-2.4.1 async-timeout-4.0.3 attrs-23.2.0 azure-core-1.31.0 azure-identity-1.18.0 beautifulsoup4-4.12.3 blinker-1.8.2 boto3-1.35.34 botocore-1.35.34 cachetools-5.5.0 certifi-2024.2.2 cffi-1.16.0 charset-normalizer-3.3.2 click-8.1.7 cloudpickle-2.2.1 cohere-5.11.0 contourpy-1.3.0 cryptography-43.0.1 cycler-0.12.1 databricks-sdk-0.33.0 dataclasses-json-0.6.7 decorator-5.1.1 deprecated-1.2.14 dill-0.3.9 dirtyjson-1.0.8 diskcache-5.6.3 distro-1.9.0 docker-7.1.0 eval-type-backport-0.2.0 exceptiongroup-1.2.0 executing-2.0.1 fastapi-0.115.0 fastavro-1.9.7 filelock-3.13.1 fonttools-4.54.1 frozenlist-1.4.1 fsspec-2024.2.0 gitdb-4.0.11 gitpython-3.1.43 google-auth-2.35.0 google-pasta-0.2.0 graphene-3.3 graphql-core-3.2.4 graphql-relay-3.2.0 greenlet-3.1.1 gunicorn-23.0.0 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 httpx-sse-0.4.0 huggingface-hub-0.23.5 idna-3.10 importlib-metadata-6.11.0 iniconfig-2.0.0 ipython-8.21.0 itsdangerous-2.2.0 jedi-0.19.1 jinja2-3.1.3 jiter-0.5.0 jmespath-1.0.1 joblib-1.4.2 jsonpath-python-1.0.6 jsonpickle-3.3.0 jsonschema-4.21.1 jsonschema-specifications-2023.12.1 kiwisolver-1.4.7 llama-cloud-0.1.2 llama-cpp-python-0.2.90 llama-index-0.11.16 llama-index-agent-openai-0.3.4 llama-index-cli-0.3.1 llama-index-core-0.11.16 llama-index-embeddings-adapter-0.2.2 llama-index-embeddings-huggingface-0.3.1 llama-index-embeddings-ollama-0.3.1 llama-index-embeddings-openai-0.2.5 llama-index-finetuning-0.2.1 llama-index-graph-stores-neo4j-0.3.2 llama-index-indices-managed-llama-cloud-0.4.0 llama-index-legacy-0.9.48.post3 llama-index-llms-azure-openai-0.2.1 llama-index-llms-huggingface-0.3.4 llama-index-llms-llama-cpp-0.2.2 llama-index-llms-mistralai-0.2.5 llama-index-llms-ollama-0.3.3 llama-index-llms-openai-0.2.11 llama-index-multi-modal-llms-openai-0.2.2 llama-index-postprocessor-cohere-rerank-0.2.1 llama-index-program-openai-0.2.0 llama-index-question-gen-openai-0.2.0 llama-index-readers-file-0.2.2 llama-index-readers-llama-parse-0.3.0 llama-index-retrievers-you-0.2.0 llama-index-utils-workflow-0.2.1 llama-index-vector-stores-neo4jvector-0.2.2 llama-parse-0.5.7 llama_deploy-0.1.3 markdown-3.7 markdown-it-py-3.0.0 marshmallow-3.22.0 matplotlib-3.9.2 matplotlib-inline-0.1.6 mdurl-0.1.2 minijinja-2.2.0 mistralai-1.1.0 mlflow-2.16.2 mlflow-skinny-2.16.2 mock-4.0.3 mpmath-1.3.0 msal-1.31.0 msal-extensions-1.2.0 multidict-6.1.0 multiprocess-0.70.17 mypy-extensions-1.0.0 neo4j-5.25.0 nest-asyncio-1.6.0 networkx-3.2.1 nltk-3.9.1 numpy-1.26.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 ollama-0.3.3 openai-1.51.0 opentelemetry-api-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 packaging-23.2 pandas-2.2.3 parameterized-0.9.0 parso-0.8.3 pathos-0.3.3 pexpect-4.9.0 pillow-10.2.0 platformdirs-4.2.0 pluggy-1.5.0 portalocker-2.10.1 pox-0.3.5 ppft-1.7.6.9 prompt-toolkit-3.0.43 protobuf-4.25.5 psutil-5.9.8 ptyprocess-0.7.0 pure-eval-0.2.2 pyarrow-17.0.0 pyasn1-0.6.1 pyasn1-modules-0.4.1 pycparser-2.21 pydantic-2.9.2 pydantic-core-2.23.4 pydantic-settings-2.5.2 pygments-2.17.2 pyparsing-3.1.4 pypdf-4.3.1 pytest-8.3.3 pytest-asyncio-0.23.8 pytest-mock-3.14.0 python-dateutil-2.8.2 python-dotenv-1.0.1 pytz-2024.2 pyvis-0.3.2 pyyaml-6.0.1 referencing-0.33.0 regex-2024.9.11 requests-2.31.0 rich-13.9.2 rpds-py-0.17.1 rsa-4.9 s3transfer-0.10.2 safetensors-0.4.5 sagemaker-2.232.2 sagemaker-core-1.0.10 sagemaker-mlflow-0.1.0 schema-0.7.7 scikit-learn-1.5.2 scipy-1.14.1 sentence-transformers-3.1.1 six-1.16.0 smdebug-rulesconfig-1.0.1 smmap-5.0.1 sniffio-1.3.0 soupsieve-2.5 sqlparse-0.5.1 stack-data-0.6.3 starlette-0.38.6 striprtf-0.0.26 sympy-1.12 tblib-3.0.0 tenacity-8.5.0 text-generation-0.7.0 threadpoolctl-3.5.0 tiktoken-0.8.0 tokenizers-0.20.0 tomli-2.0.1 torch-2.2.0 tqdm-4.66.5 traitlets-5.14.1 transformers-4.45.1 triton-2.2.0 types-requests-2.32.0.20240914 typing-extensions-4.12.2 typing-inspect-0.9.0 tzdata-2024.2 urllib3-2.2.0 uvicorn-0.31.0 wcwidth-0.2.13 wrapt-1.16.0 yarl-1.13.1 zipp-3.20.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (4.25.5)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting absl-py\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting tensorboardX\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.13.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
      "Collecting graspologic\n",
      "  Downloading graspologic-3.4.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting fpdf2\n",
      "  Downloading fpdf2-2.8.1-py2.py3-none-any.whl.metadata (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m903.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets>=2.0.0 (from evaluate)\n",
      "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.3)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.9)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.5)\n",
      "Collecting xxhash (from evaluate)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.17)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.23.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.2.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.45.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n",
      "Collecting POT<0.10,>=0.9 (from graspologic)\n",
      "  Downloading POT-0.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\n",
      "Collecting anytree<3.0.0,>=2.12.1 (from graspologic)\n",
      "  Downloading anytree-2.12.1-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting beartype<0.19.0,>=0.18.5 (from graspologic)\n",
      "  Downloading beartype-0.18.5-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting gensim<5.0.0,>=4.3.2 (from graspologic)\n",
      "  Downloading gensim-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n",
      "Collecting graspologic-native<2.0.0,>=1.2.1 (from graspologic)\n",
      "  Downloading graspologic_native-1.2.1-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
      "Collecting hyppo<0.5.0,>=0.4.0 (from graspologic)\n",
      "  Downloading hyppo-0.4.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: joblib<2.0.0,>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from graspologic) (1.4.2)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.8.4 in /usr/local/lib/python3.10/dist-packages (from graspologic) (3.9.2)\n",
      "Requirement already satisfied: networkx<4,>=3 in /usr/local/lib/python3.10/dist-packages (from graspologic) (3.2.1)\n",
      "Collecting numpy>=1.17 (from evaluate)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: scikit-learn<2.0.0,>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from graspologic) (1.5.2)\n",
      "Collecting scipy==1.12.0 (from graspologic)\n",
      "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting seaborn<0.14.0,>=0.13.2 (from graspologic)\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting statsmodels<0.15.0,>=0.14.2 (from graspologic)\n",
      "  Downloading statsmodels-0.14.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from graspologic) (4.12.2)\n",
      "Collecting umap-learn<0.6.0,>=0.5.6 (from graspologic)\n",
      "  Downloading umap_learn-0.5.6-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from fpdf2) (0.7.1)\n",
      "Requirement already satisfied: Pillow!=9.2.*,>=6.2.2 in /usr/local/lib/python3.10/dist-packages (from fpdf2) (10.2.0)\n",
      "Requirement already satisfied: fonttools>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from fpdf2) (4.54.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Collecting dill (from evaluate)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting requests>=2.19.0 (from evaluate)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.9)\n",
      "Collecting smart-open>=1.8.1 (from gensim<5.0.0,>=4.3.2->graspologic)\n",
      "  Downloading smart_open-7.0.5-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting numba>=0.46 (from hyppo<0.5.0,>=0.4.0->graspologic)\n",
      "  Downloading numba-0.60.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting autograd>=1.3 (from hyppo<0.5.0,>=0.4.0->graspologic)\n",
      "  Downloading autograd-1.7.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.4->graspologic) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.4->graspologic) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.4->graspologic) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.4->graspologic) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.4->graspologic) (2.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.4.2->graspologic) (3.5.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
      "Collecting patsy>=0.5.6 (from statsmodels<0.15.0,>=0.14.2->graspologic)\n",
      "  Downloading patsy-0.5.6-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.3.101)\n",
      "Collecting pynndescent>=0.5 (from umap-learn<0.6.0,>=0.5.6->graspologic)\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from evaluate)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.20.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.13.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba>=0.46->hyppo<0.5.0,>=0.4.0->graspologic)\n",
      "  Downloading llvmlite-0.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.2->graspologic) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.13.0-py3-none-any.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graspologic-3.4.1-py3-none-any.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fpdf2-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.8/227.8 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anytree-2.12.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading beartype-0.18.5-py3-none-any.whl (917 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m917.8/917.8 kB\u001b[0m \u001b[31m118.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gensim-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading graspologic_native-1.2.1-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hyppo-0.4.0-py3-none-any.whl (146 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.6/146.6 kB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Downloading POT-0.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (835 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m835.4/835.4 kB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading statsmodels-0.14.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading autograd-1.7.0-py3-none-any.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numba-0.60.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.9/233.9 kB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/61.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=b45b6c3ac4c3a55b020f5bdf8a314446883663398f6b317128679c8ceb7ab458\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: sentencepiece, xxhash, smart-open, requests, numpy, llvmlite, graspologic-native, fpdf2, dill, beartype, anytree, absl-py, tensorboardX, scipy, rouge_score, patsy, numba, multiprocess, autograd, statsmodels, POT, gensim, seaborn, pynndescent, hyppo, bitsandbytes, umap-learn, peft, datasets, graspologic, evaluate\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.3\n",
      "    Uninstalling numpy-1.26.3:\n",
      "      Successfully uninstalled numpy-1.26.3\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.9\n",
      "    Uninstalling dill-0.3.9:\n",
      "      Successfully uninstalled dill-0.3.9\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.17\n",
      "    Uninstalling multiprocess-0.70.17:\n",
      "      Successfully uninstalled multiprocess-0.70.17\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pathos 0.3.3 requires dill>=0.3.9, but you have dill 0.3.8 which is incompatible.\n",
      "pathos 0.3.3 requires multiprocess>=0.70.17, but you have multiprocess 0.70.16 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed POT-0.9.4 absl-py-2.1.0 anytree-2.12.1 autograd-1.7.0 beartype-0.18.5 bitsandbytes-0.44.1 datasets-3.0.1 dill-0.3.8 evaluate-0.4.3 fpdf2-2.8.1 gensim-4.3.3 graspologic-3.4.1 graspologic-native-1.2.1 hyppo-0.4.0 llvmlite-0.43.0 multiprocess-0.70.16 numba-0.60.0 numpy-1.26.4 patsy-0.5.6 peft-0.13.0 pynndescent-0.5.13 requests-2.32.3 rouge_score-0.1.2 scipy-1.12.0 seaborn-0.13.2 sentencepiece-0.2.0 smart-open-7.0.5 statsmodels-0.14.4 tensorboardX-2.6.2.2 umap-learn-0.5.6 xxhash-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.6.3.tar.gz (2.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.2.0)\n",
      "Collecting einops (from flash-attn)\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.6.3-cp310-cp310-linux_x86_64.whl size=187309225 sha256=237ef9c6157db394e1ddde4ba609a21ebb98382377a27041edc09318801a6f24\n",
      "  Stored in directory: /root/.cache/pip/wheels/7e/e3/c3/89c7a2f3c4adc07cd1c675f8bb7b9ad4d18f64a72bccdfe826\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: einops, flash-attn\n",
      "Successfully installed einops-0.8.0 flash-attn-2.6.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "######################################################################## 100.0%#=#=#                                                                         \n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    }
   ],
   "source": [
    "# Set up ollama\n",
    "!apt-get update && apt-get install tmux vim -y\n",
    "!pip3 install --ignore-installed llama-index llama-parse llama_deploy llama-index-llms-huggingface llama-index-embeddings-huggingface llama-index-llms-ollama llama-index-embeddings-ollama llama-index-vector-stores-neo4jvector llama-index-graph-stores-neo4j llama-index-finetuning llama-index-utils-workflow llama-index-readers-file llama-index-retrievers-you\n",
    "!pip3 install sentencepiece protobuf evaluate rouge_score absl-py tensorboardX bitsandbytes peft accelerate python-dotenv graspologic fpdf2\n",
    "!pip3 install flash-attn --no-build-isolation\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  apt libapt-pkg6.0\n",
      "Suggested packages:\n",
      "  apt-doc aptitude | synaptic | wajig powermgmt-base\n",
      "The following NEW packages will be installed:\n",
      "  apt-utils dialog\n",
      "The following packages will be upgraded:\n",
      "  apt libapt-pkg6.0\n",
      "2 upgraded, 2 newly installed, 0 to remove and 94 not upgraded.\n",
      "Need to get 2789 kB of archives.\n",
      "After this operation, 2070 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libapt-pkg6.0 amd64 2.4.13 [912 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apt amd64 2.4.13 [1363 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apt-utils amd64 2.4.13 [211 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 dialog amd64 1.3-20211214-1 [303 kB]\n",
      "Fetched 2789 kB in 1s (1916 kB/s)0m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "(Reading database ... 23089 files and directories currently installed.)\n",
      "Preparing to unpack .../libapt-pkg6.0_2.4.13_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  6%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking libapt-pkg6.0:amd64 (2.4.13) over (2.4.11) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 12%]\u001b[49m\u001b[39m [######....................................................] \u001b8Setting up libapt-pkg6.0:amd64 (2.4.13) ...\n",
      "(Reading database ... 23089 files and directories currently installed.)..................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 24%]\u001b[49m\u001b[39m [#############.............................................] \u001b8(Reading database ... \n",
      "Preparing to unpack .../archives/apt_2.4.13_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 29%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Unpacking apt (2.4.13) over (2.4.11) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 35%]\u001b[49m\u001b[39m [####################......................................] \u001b8Setting up apt (2.4.13) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 41%]\u001b[49m\u001b[39m [#######################...................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 47%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Selecting previously unselected package apt-utils.\n",
      "(Reading database ... 23089 files and directories currently installed.)\n",
      "Preparing to unpack .../apt-utils_2.4.13_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 53%]\u001b[49m\u001b[39m [##############################............................] \u001b8Unpacking apt-utils (2.4.13) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 59%]\u001b[49m\u001b[39m [##################################........................] \u001b8Selecting previously unselected package dialog.\n",
      "Preparing to unpack .../dialog_1.3-20211214-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 65%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8Unpacking dialog (1.3-20211214-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 71%]\u001b[49m\u001b[39m [########################################..................] \u001b8Setting up apt-utils (2.4.13) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 76%]\u001b[49m\u001b[39m [############################################..............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 82%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up dialog (1.3-20211214-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 88%]\u001b[49m\u001b[39m [###################################################.......] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 94%]\u001b[49m\u001b[39m [######################################################....] \u001b8Processing triggers for libc-bin (2.35-0ubuntu3.6) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J--2024-10-05 01:51:05--  https://debian.neo4j.com/neotechnology.gpg.key\n",
      "Resolving debian.neo4j.com (debian.neo4j.com)... 18.65.229.27, 18.65.229.44, 18.65.229.93, ...\n",
      "Connecting to debian.neo4j.com (debian.neo4j.com)|18.65.229.27|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6342 (6.2K) [application/pgp-keys]\n",
      "Saving to: ‘STDOUT’\n",
      "\n",
      "-                   100%[===================>]   6.19K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-10-05 01:51:05 (5.57 GB/s) - written to stdout [6342/6342]\n",
      "\n",
      "deb [signed-by=/etc/apt/keyrings/neotechnology.gpg] https://debian.neo4j.com stable latest\n",
      "Listing... Done\n",
      "Adding component(s) 'universe' to all repositories.\n",
      "Get:1 https://debian.neo4j.com stable InRelease [44.2 kB]\n",
      "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \n",
      "Get:4 https://debian.neo4j.com stable/latest amd64 Packages [9592 B]           \n",
      "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease                         \n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \n",
      "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Fetched 311 kB in 1s (314 kB/s)\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  adwaita-icon-theme alsa-topology-conf alsa-ucm-conf at-spi2-core\n",
      "  ca-certificates-java cypher-shell daemon dbus-user-session\n",
      "  dconf-gsettings-backend dconf-service fontconfig fonts-dejavu-extra\n",
      "  gsettings-desktop-schemas gtk-update-icon-cache hicolor-icon-theme\n",
      "  humanity-icon-theme java-common libasound2 libasound2-data\n",
      "  libatk-bridge2.0-0 libatk-wrapper-java libatk-wrapper-java-jni libatk1.0-0\n",
      "  libatk1.0-data libatspi2.0-0 libavahi-client3 libavahi-common-data\n",
      "  libavahi-common3 libcairo-gobject2 libcairo2 libcolord2 libcups2 libdatrie1\n",
      "  libdconf1 libepoxy0 libfontenc1 libfribidi0 libgdk-pixbuf-2.0-0\n",
      "  libgdk-pixbuf2.0-bin libgdk-pixbuf2.0-common libgif7 libgraphite2-3\n",
      "  libgtk-3-0 libgtk-3-bin libgtk-3-common libharfbuzz0b libice6 liblcms2-2\n",
      "  libnspr4 libnss3 libpango-1.0-0 libpangocairo-1.0-0 libpangoft2-1.0-0\n",
      "  libpcsclite1 libpixman-1-0 librsvg2-2 librsvg2-common libsm6 libthai-data\n",
      "  libthai0 libwayland-client0 libwayland-cursor0 libwayland-egl1 libxaw7\n",
      "  libxcb-render0 libxcb-shape0 libxcomposite1 libxcursor1 libxdamage1 libxft2\n",
      "  libxi6 libxinerama1 libxkbcommon0 libxkbfile1 libxmu6 libxmuu1 libxrandr2\n",
      "  libxrender1 libxt6 libxtst6 libxv1 libxxf86dga1 openjdk-21-jre\n",
      "  openjdk-21-jre-headless psmisc session-migration shared-mime-info\n",
      "  ubuntu-mono x11-common x11-utils xkb-data\n",
      "Suggested packages:\n",
      "  default-jre libasound2-plugins alsa-utils colord cups-common gvfs\n",
      "  liblcms2-utils pcscd librsvg2-bin libnss-mdns fonts-ipafont-gothic\n",
      "  fonts-ipafont-mincho fonts-wqy-microhei | fonts-wqy-zenhei fonts-indic\n",
      "  mesa-utils\n",
      "The following NEW packages will be installed:\n",
      "  adwaita-icon-theme alsa-topology-conf alsa-ucm-conf at-spi2-core\n",
      "  ca-certificates-java cypher-shell daemon dbus-user-session\n",
      "  dconf-gsettings-backend dconf-service fontconfig fonts-dejavu-extra\n",
      "  gsettings-desktop-schemas gtk-update-icon-cache hicolor-icon-theme\n",
      "  humanity-icon-theme java-common libasound2 libasound2-data\n",
      "  libatk-bridge2.0-0 libatk-wrapper-java libatk-wrapper-java-jni libatk1.0-0\n",
      "  libatk1.0-data libatspi2.0-0 libavahi-client3 libavahi-common-data\n",
      "  libavahi-common3 libcairo-gobject2 libcairo2 libcolord2 libcups2 libdatrie1\n",
      "  libdconf1 libepoxy0 libfontenc1 libfribidi0 libgdk-pixbuf-2.0-0\n",
      "  libgdk-pixbuf2.0-bin libgdk-pixbuf2.0-common libgif7 libgraphite2-3\n",
      "  libgtk-3-0 libgtk-3-bin libgtk-3-common libharfbuzz0b libice6 liblcms2-2\n",
      "  libnspr4 libnss3 libpango-1.0-0 libpangocairo-1.0-0 libpangoft2-1.0-0\n",
      "  libpcsclite1 libpixman-1-0 librsvg2-2 librsvg2-common libsm6 libthai-data\n",
      "  libthai0 libwayland-client0 libwayland-cursor0 libwayland-egl1 libxaw7\n",
      "  libxcb-render0 libxcb-shape0 libxcomposite1 libxcursor1 libxdamage1 libxft2\n",
      "  libxi6 libxinerama1 libxkbcommon0 libxkbfile1 libxmu6 libxmuu1 libxrandr2\n",
      "  libxrender1 libxt6 libxtst6 libxv1 libxxf86dga1 neo4j openjdk-21-jre\n",
      "  openjdk-21-jre-headless psmisc session-migration shared-mime-info\n",
      "  ubuntu-mono x11-common x11-utils xkb-data\n",
      "0 upgraded, 92 newly installed, 0 to remove and 94 not upgraded.\n",
      "Need to get 212 MB of archives.\n",
      "After this operation, 460 MB of additional disk space will be used.\n",
      "Get:1 https://debian.neo4j.com stable/latest amd64 cypher-shell all 1:5.24.1 [23.7 MB]\n",
      "Get:2 https://debian.neo4j.com stable/latest amd64 neo4j all 1:5.23.0 [120 MB] \u001b[0m\u001b[33m\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 daemon amd64 0.8-1 [60.9 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 psmisc amd64 23.4-2build3 [119 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libnspr4 amd64 2:4.35-0ubuntu0.22.04.1 [119 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libnss3 amd64 2:3.98-0ubuntu0.22.04.2 [1347 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ca-certificates-java all 20190909ubuntu1.2 [12.1 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 java-common all 0.72build2 [6782 B]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblcms2-2 amd64 2.12~rc1-2build2 [159 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpcsclite1 amd64 1.9.5-3ubuntu1 [19.8 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-21-jre-headless amd64 21.0.4+7-1ubuntu2~22.04 [47.0 MB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 hicolor-icon-theme all 0.17-2 [9976 B]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgdk-pixbuf2.0-common all 2.42.8+dfsg-1ubuntu0.3 [5630 B]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 shared-mime-info amd64 2.1-2 [454 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgdk-pixbuf-2.0-0 amd64 2.42.8+dfsg-1ubuntu0.3 [148 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gtk-update-icon-cache amd64 3.24.33-1ubuntu2.2 [31.4 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 humanity-icon-theme all 0.6.16 [1282 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 ubuntu-mono all 20.10-0ubuntu2 [153 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 adwaita-icon-theme all 41.0-1ubuntu1 [3444 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-data all 2.36.0-3build1 [2824 B]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-0 amd64 2.36.0-3build1 [51.9 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxi6 amd64 2:1.8-1build1 [32.6 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatspi2.0-0 amd64 2.44.0-3 [80.9 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-bridge2.0-0 amd64 2.38.0-3 [66.6 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpixman-1-0 amd64 0.40.0-1ubuntu0.22.04.1 [264 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-render0 amd64 1.14-3ubuntu3 [16.4 kB]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxrender1 amd64 1:0.9.10-1build4 [19.7 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcairo2 amd64 1.16.0-5ubuntu2 [628 kB]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcairo-gobject2 amd64 1.16.0-5ubuntu2 [19.4 kB]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcolord2 amd64 1.4.6-1 [155 kB]\n",
      "Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libavahi-common-data amd64 0.8-5ubuntu5.2 [23.8 kB]\n",
      "Get:32 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libavahi-common3 amd64 0.8-5ubuntu5.2 [23.9 kB]\n",
      "Get:33 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libavahi-client3 amd64 0.8-5ubuntu5.2 [28.0 kB]\n",
      "Get:34 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libcups2 amd64 2.4.1op1-1ubuntu4.11 [263 kB]\n",
      "Get:35 http://archive.ubuntu.com/ubuntu jammy/main amd64 libepoxy0 amd64 1.5.10-1 [237 kB]\n",
      "Get:36 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libfribidi0 amd64 1.0.8-2ubuntu3.1 [26.1 kB]\n",
      "Get:37 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgraphite2-3 amd64 1.3.14-1build2 [71.3 kB]\n",
      "Get:38 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libharfbuzz0b amd64 2.7.4-1ubuntu3.1 [352 kB]\n",
      "Get:39 http://archive.ubuntu.com/ubuntu jammy/main amd64 fontconfig amd64 2.13.1-4.2ubuntu5 [177 kB]\n",
      "Get:40 http://archive.ubuntu.com/ubuntu jammy/main amd64 libthai-data all 0.1.29-1build1 [162 kB]\n",
      "Get:41 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdatrie1 amd64 0.2.13-2 [19.9 kB]\n",
      "Get:42 http://archive.ubuntu.com/ubuntu jammy/main amd64 libthai0 amd64 0.1.29-1build1 [19.2 kB]\n",
      "Get:43 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpango-1.0-0 amd64 1.50.6+ds-2ubuntu1 [230 kB]\n",
      "Get:44 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpangoft2-1.0-0 amd64 1.50.6+ds-2ubuntu1 [54.0 kB]\n",
      "Get:45 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpangocairo-1.0-0 amd64 1.50.6+ds-2ubuntu1 [39.8 kB]\n",
      "Get:46 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libwayland-client0 amd64 1.20.0-1ubuntu0.1 [25.9 kB]\n",
      "Get:47 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libwayland-cursor0 amd64 1.20.0-1ubuntu0.1 [10.7 kB]\n",
      "Get:48 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libwayland-egl1 amd64 1.20.0-1ubuntu0.1 [5582 B]\n",
      "Get:49 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcomposite1 amd64 1:0.4.5-1build2 [7192 B]\n",
      "Get:50 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcursor1 amd64 1:1.2.0-2build4 [20.9 kB]\n",
      "Get:51 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxdamage1 amd64 1:1.1.5-2build2 [7154 B]\n",
      "Get:52 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxinerama1 amd64 2:1.1.4-3 [7382 B]\n",
      "Get:53 http://archive.ubuntu.com/ubuntu jammy/main amd64 xkb-data all 2.33-1 [394 kB]\n",
      "Get:54 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon0 amd64 1.4.0-1 [125 kB]\n",
      "Get:55 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxrandr2 amd64 2:1.5.2-1build1 [20.4 kB]\n",
      "Get:56 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 dbus-user-session amd64 1.12.20-2ubuntu4.1 [9442 B]\n",
      "Get:57 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdconf1 amd64 0.40.0-3 [40.5 kB]\n",
      "Get:58 http://archive.ubuntu.com/ubuntu jammy/main amd64 dconf-service amd64 0.40.0-3 [28.5 kB]\n",
      "Get:59 http://archive.ubuntu.com/ubuntu jammy/main amd64 dconf-gsettings-backend amd64 0.40.0-3 [22.8 kB]\n",
      "Get:60 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk-3-common all 3.24.33-1ubuntu2.2 [239 kB]\n",
      "Get:61 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk-3-0 amd64 3.24.33-1ubuntu2.2 [3053 kB]\n",
      "Get:62 http://archive.ubuntu.com/ubuntu jammy/main amd64 libasound2-data all 1.2.6.1-1ubuntu1 [19.1 kB]\n",
      "Get:63 http://archive.ubuntu.com/ubuntu jammy/main amd64 libasound2 amd64 1.2.6.1-1ubuntu1 [390 kB]\n",
      "Get:64 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgif7 amd64 5.1.9-2ubuntu0.1 [33.9 kB]\n",
      "Get:65 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-common all 1:7.7+23ubuntu2 [23.4 kB]\n",
      "Get:66 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
      "Get:67 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-21-jre amd64 21.0.4+7-1ubuntu2~22.04 [232 kB]\n",
      "Get:68 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxmuu1 amd64 2:1.1.3-3 [10.2 kB]\n",
      "Get:69 http://archive.ubuntu.com/ubuntu jammy/main amd64 alsa-topology-conf all 1.2.5.1-2 [15.5 kB]\n",
      "Get:70 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 alsa-ucm-conf all 1.2.6.3-1ubuntu1.11 [43.6 kB]\n",
      "Get:71 http://archive.ubuntu.com/ubuntu jammy/main amd64 session-migration amd64 0.3.6 [9774 B]\n",
      "Get:72 http://archive.ubuntu.com/ubuntu jammy/main amd64 gsettings-desktop-schemas all 42.0-1ubuntu1 [31.1 kB]\n",
      "Get:73 http://archive.ubuntu.com/ubuntu jammy/main amd64 at-spi2-core amd64 2.44.0-3 [54.4 kB]\n",
      "Get:74 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2041 kB]\n",
      "Get:75 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
      "Get:76 http://archive.ubuntu.com/ubuntu jammy/main amd64 libice6 amd64 2:1.0.10-1build2 [42.6 kB]\n",
      "Get:77 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsm6 amd64 2:1.2.3-1build2 [16.7 kB]\n",
      "Get:78 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt6 amd64 1:1.2.1-1 [177 kB]\n",
      "Get:79 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxmu6 amd64 2:1.1.3-3 [49.6 kB]\n",
      "Get:80 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxaw7 amd64 2:1.0.14-1 [191 kB]\n",
      "Get:81 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-shape0 amd64 1.14-3ubuntu3 [6158 B]\n",
      "Get:82 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxft2 amd64 2.3.4-1 [41.8 kB]\n",
      "Get:83 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
      "Get:84 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxv1 amd64 2:1.0.11-1build2 [11.2 kB]\n",
      "Get:85 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
      "Get:86 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
      "Get:87 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
      "Get:88 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
      "Get:89 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgdk-pixbuf2.0-bin amd64 2.42.8+dfsg-1ubuntu0.3 [14.2 kB]\n",
      "Get:90 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk-3-bin amd64 3.24.33-1ubuntu2.2 [69.6 kB]\n",
      "Get:91 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-2 amd64 2.52.5+dfsg-3ubuntu0.2 [2974 kB]\n",
      "Get:92 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-common amd64 2.52.5+dfsg-3ubuntu0.2 [17.7 kB]\n",
      "Fetched 212 MB in 6s (36.5 MB/s)              \u001b[0m\u001b[33m\n",
      "Extracting templates from packages: 100%\n",
      "Preconfiguring packages ...\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package daemon.\n",
      "(Reading database ... 23287 files and directories currently installed.)\n",
      "Preparing to unpack .../00-daemon_0.8-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8Unpacking daemon (0.8-1) ...\n",
      "Selecting previously unselected package psmisc.\n",
      "Preparing to unpack .../01-psmisc_23.4-2build3_amd64.deb ...\n",
      "Unpacking psmisc (23.4-2build3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  1%]\u001b[49m\u001b[39m [..........................................................] \u001b8Selecting previously unselected package libnspr4:amd64.\n",
      "Preparing to unpack .../02-libnspr4_2%3a4.35-0ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libnspr4:amd64 (2:4.35-0ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libnss3:amd64.\n",
      "Preparing to unpack .../03-libnss3_2%3a3.98-0ubuntu0.22.04.2_amd64.deb ...\n",
      "Unpacking libnss3:amd64 (2:3.98-0ubuntu0.22.04.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  2%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Selecting previously unselected package ca-certificates-java.\n",
      "Preparing to unpack .../04-ca-certificates-java_20190909ubuntu1.2_all.deb ...\n",
      "Unpacking ca-certificates-java (20190909ubuntu1.2) ...\n",
      "Selecting previously unselected package java-common.\n",
      "Preparing to unpack .../05-java-common_0.72build2_all.deb ...\n",
      "Unpacking java-common (0.72build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  3%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Selecting previously unselected package liblcms2-2:amd64.\n",
      "Preparing to unpack .../06-liblcms2-2_2.12~rc1-2build2_amd64.deb ...\n",
      "Unpacking liblcms2-2:amd64 (2.12~rc1-2build2) ...\n",
      "Selecting previously unselected package libpcsclite1:amd64.\n",
      "Preparing to unpack .../07-libpcsclite1_1.9.5-3ubuntu1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  4%]\u001b[49m\u001b[39m [##........................................................] \u001b8Unpacking libpcsclite1:amd64 (1.9.5-3ubuntu1) ...\n",
      "Selecting previously unselected package openjdk-21-jre-headless:amd64.\n",
      "Preparing to unpack .../08-openjdk-21-jre-headless_21.0.4+7-1ubuntu2~22.04_amd64.deb ...\n",
      "Unpacking openjdk-21-jre-headless:amd64 (21.0.4+7-1ubuntu2~22.04) ...\n",
      "Selecting previously unselected package hicolor-icon-theme.\n",
      "Preparing to unpack .../09-hicolor-icon-theme_0.17-2_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  5%]\u001b[49m\u001b[39m [##........................................................] \u001b8Unpacking hicolor-icon-theme (0.17-2) ...\n",
      "Selecting previously unselected package libgdk-pixbuf2.0-common.\n",
      "Preparing to unpack .../10-libgdk-pixbuf2.0-common_2.42.8+dfsg-1ubuntu0.3_all.deb ...\n",
      "Unpacking libgdk-pixbuf2.0-common (2.42.8+dfsg-1ubuntu0.3) ...\n",
      "Selecting previously unselected package shared-mime-info.\n",
      "Preparing to unpack .../11-shared-mime-info_2.1-2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  6%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking shared-mime-info (2.1-2) ...\n",
      "Selecting previously unselected package libgdk-pixbuf-2.0-0:amd64.\n",
      "Preparing to unpack .../12-libgdk-pixbuf-2.0-0_2.42.8+dfsg-1ubuntu0.3_amd64.deb ...\n",
      "Unpacking libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  7%]\u001b[49m\u001b[39m [####......................................................] \u001b8Selecting previously unselected package gtk-update-icon-cache.\n",
      "Preparing to unpack .../13-gtk-update-icon-cache_3.24.33-1ubuntu2.2_amd64.deb ...\n",
      "Unpacking gtk-update-icon-cache (3.24.33-1ubuntu2.2) ...\n",
      "Selecting previously unselected package humanity-icon-theme.\n",
      "Preparing to unpack .../14-humanity-icon-theme_0.6.16_all.deb ...\n",
      "Unpacking humanity-icon-theme (0.6.16) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  8%]\u001b[49m\u001b[39m [####......................................................] \u001b8Selecting previously unselected package ubuntu-mono.\n",
      "Preparing to unpack .../15-ubuntu-mono_20.10-0ubuntu2_all.deb ...\n",
      "Unpacking ubuntu-mono (20.10-0ubuntu2) ...\n",
      "Selecting previously unselected package adwaita-icon-theme.\n",
      "Preparing to unpack .../16-adwaita-icon-theme_41.0-1ubuntu1_all.deb ...\n",
      "Unpacking adwaita-icon-theme (41.0-1ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  9%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8Selecting previously unselected package libatk1.0-data.\n",
      "Preparing to unpack .../17-libatk1.0-data_2.36.0-3build1_all.deb ...\n",
      "Unpacking libatk1.0-data (2.36.0-3build1) ...\n",
      "Selecting previously unselected package libatk1.0-0:amd64.\n",
      "Preparing to unpack .../18-libatk1.0-0_2.36.0-3build1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 10%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8Unpacking libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
      "Selecting previously unselected package libxi6:amd64.\n",
      "Preparing to unpack .../19-libxi6_2%3a1.8-1build1_amd64.deb ...\n",
      "Unpacking libxi6:amd64 (2:1.8-1build1) ...\n",
      "Selecting previously unselected package libatspi2.0-0:amd64.\n",
      "Preparing to unpack .../20-libatspi2.0-0_2.44.0-3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 11%]\u001b[49m\u001b[39m [######....................................................] \u001b8Unpacking libatspi2.0-0:amd64 (2.44.0-3) ...\n",
      "Selecting previously unselected package libatk-bridge2.0-0:amd64.\n",
      "Preparing to unpack .../21-libatk-bridge2.0-0_2.38.0-3_amd64.deb ...\n",
      "Unpacking libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
      "Selecting previously unselected package libpixman-1-0:amd64.\n",
      "Preparing to unpack .../22-libpixman-1-0_0.40.0-1ubuntu0.22.04.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 12%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Unpacking libpixman-1-0:amd64 (0.40.0-1ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libxcb-render0:amd64.\n",
      "Preparing to unpack .../23-libxcb-render0_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-render0:amd64 (1.14-3ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 13%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Selecting previously unselected package libxrender1:amd64.\n",
      "Preparing to unpack .../24-libxrender1_1%3a0.9.10-1build4_amd64.deb ...\n",
      "Unpacking libxrender1:amd64 (1:0.9.10-1build4) ...\n",
      "Selecting previously unselected package libcairo2:amd64.\n",
      "Preparing to unpack .../25-libcairo2_1.16.0-5ubuntu2_amd64.deb ...\n",
      "Unpacking libcairo2:amd64 (1.16.0-5ubuntu2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 14%]\u001b[49m\u001b[39m [########..................................................] \u001b8Selecting previously unselected package libcairo-gobject2:amd64.\n",
      "Preparing to unpack .../26-libcairo-gobject2_1.16.0-5ubuntu2_amd64.deb ...\n",
      "Unpacking libcairo-gobject2:amd64 (1.16.0-5ubuntu2) ...\n",
      "Selecting previously unselected package libcolord2:amd64.\n",
      "Preparing to unpack .../27-libcolord2_1.4.6-1_amd64.deb ...\n",
      "Unpacking libcolord2:amd64 (1.4.6-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 15%]\u001b[49m\u001b[39m [########..................................................] \u001b8Selecting previously unselected package libavahi-common-data:amd64.\n",
      "Preparing to unpack .../28-libavahi-common-data_0.8-5ubuntu5.2_amd64.deb ...\n",
      "Unpacking libavahi-common-data:amd64 (0.8-5ubuntu5.2) ...\n",
      "Selecting previously unselected package libavahi-common3:amd64.\n",
      "Preparing to unpack .../29-libavahi-common3_0.8-5ubuntu5.2_amd64.deb ...\n",
      "Unpacking libavahi-common3:amd64 (0.8-5ubuntu5.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 16%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Selecting previously unselected package libavahi-client3:amd64.\n",
      "Preparing to unpack .../30-libavahi-client3_0.8-5ubuntu5.2_amd64.deb ...\n",
      "Unpacking libavahi-client3:amd64 (0.8-5ubuntu5.2) ...\n",
      "Selecting previously unselected package libcups2:amd64.\n",
      "Preparing to unpack .../31-libcups2_2.4.1op1-1ubuntu4.11_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 17%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Unpacking libcups2:amd64 (2.4.1op1-1ubuntu4.11) ...\n",
      "Selecting previously unselected package libepoxy0:amd64.\n",
      "Preparing to unpack .../32-libepoxy0_1.5.10-1_amd64.deb ...\n",
      "Unpacking libepoxy0:amd64 (1.5.10-1) ...\n",
      "Selecting previously unselected package libfribidi0:amd64.\n",
      "Preparing to unpack .../33-libfribidi0_1.0.8-2ubuntu3.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 18%]\u001b[49m\u001b[39m [##########................................................] \u001b8Unpacking libfribidi0:amd64 (1.0.8-2ubuntu3.1) ...\n",
      "Selecting previously unselected package libgraphite2-3:amd64.\n",
      "Preparing to unpack .../34-libgraphite2-3_1.3.14-1build2_amd64.deb ...\n",
      "Unpacking libgraphite2-3:amd64 (1.3.14-1build2) ...\n",
      "Selecting previously unselected package libharfbuzz0b:amd64.\n",
      "Preparing to unpack .../35-libharfbuzz0b_2.7.4-1ubuntu3.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 19%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking libharfbuzz0b:amd64 (2.7.4-1ubuntu3.1) ...\n",
      "Selecting previously unselected package fontconfig.\n",
      "Preparing to unpack .../36-fontconfig_2.13.1-4.2ubuntu5_amd64.deb ...\n",
      "Unpacking fontconfig (2.13.1-4.2ubuntu5) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Selecting previously unselected package libthai-data.\n",
      "Preparing to unpack .../37-libthai-data_0.1.29-1build1_all.deb ...\n",
      "Unpacking libthai-data (0.1.29-1build1) ...\n",
      "Selecting previously unselected package libdatrie1:amd64.\n",
      "Preparing to unpack .../38-libdatrie1_0.2.13-2_amd64.deb ...\n",
      "Unpacking libdatrie1:amd64 (0.2.13-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 21%]\u001b[49m\u001b[39m [############..............................................] \u001b8Selecting previously unselected package libthai0:amd64.\n",
      "Preparing to unpack .../39-libthai0_0.1.29-1build1_amd64.deb ...\n",
      "Unpacking libthai0:amd64 (0.1.29-1build1) ...\n",
      "Selecting previously unselected package libpango-1.0-0:amd64.\n",
      "Preparing to unpack .../40-libpango-1.0-0_1.50.6+ds-2ubuntu1_amd64.deb ...\n",
      "Unpacking libpango-1.0-0:amd64 (1.50.6+ds-2ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 22%]\u001b[49m\u001b[39m [############..............................................] \u001b8Selecting previously unselected package libpangoft2-1.0-0:amd64.\n",
      "Preparing to unpack .../41-libpangoft2-1.0-0_1.50.6+ds-2ubuntu1_amd64.deb ...\n",
      "Unpacking libpangoft2-1.0-0:amd64 (1.50.6+ds-2ubuntu1) ...\n",
      "Selecting previously unselected package libpangocairo-1.0-0:amd64.\n",
      "Preparing to unpack .../42-libpangocairo-1.0-0_1.50.6+ds-2ubuntu1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 23%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Unpacking libpangocairo-1.0-0:amd64 (1.50.6+ds-2ubuntu1) ...\n",
      "Selecting previously unselected package libwayland-client0:amd64.\n",
      "Preparing to unpack .../43-libwayland-client0_1.20.0-1ubuntu0.1_amd64.deb ...\n",
      "Unpacking libwayland-client0:amd64 (1.20.0-1ubuntu0.1) ...\n",
      "Selecting previously unselected package libwayland-cursor0:amd64.\n",
      "Preparing to unpack .../44-libwayland-cursor0_1.20.0-1ubuntu0.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 24%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Unpacking libwayland-cursor0:amd64 (1.20.0-1ubuntu0.1) ...\n",
      "Selecting previously unselected package libwayland-egl1:amd64.\n",
      "Preparing to unpack .../45-libwayland-egl1_1.20.0-1ubuntu0.1_amd64.deb ...\n",
      "Unpacking libwayland-egl1:amd64 (1.20.0-1ubuntu0.1) ...\n",
      "Selecting previously unselected package libxcomposite1:amd64.\n",
      "Preparing to unpack .../46-libxcomposite1_1%3a0.4.5-1build2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 25%]\u001b[49m\u001b[39m [##############............................................] \u001b8Unpacking libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
      "Selecting previously unselected package libxcursor1:amd64.\n",
      "Preparing to unpack .../47-libxcursor1_1%3a1.2.0-2build4_amd64.deb ...\n",
      "Unpacking libxcursor1:amd64 (1:1.2.0-2build4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 26%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Selecting previously unselected package libxdamage1:amd64.\n",
      "Preparing to unpack .../48-libxdamage1_1%3a1.1.5-2build2_amd64.deb ...\n",
      "Unpacking libxdamage1:amd64 (1:1.1.5-2build2) ...\n",
      "Selecting previously unselected package libxinerama1:amd64.\n",
      "Preparing to unpack .../49-libxinerama1_2%3a1.1.4-3_amd64.deb ...\n",
      "Unpacking libxinerama1:amd64 (2:1.1.4-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 27%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Selecting previously unselected package xkb-data.\n",
      "Preparing to unpack .../50-xkb-data_2.33-1_all.deb ...\n",
      "Unpacking xkb-data (2.33-1) ...\n",
      "Selecting previously unselected package libxkbcommon0:amd64.\n",
      "Preparing to unpack .../51-libxkbcommon0_1.4.0-1_amd64.deb ...\n",
      "Unpacking libxkbcommon0:amd64 (1.4.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 28%]\u001b[49m\u001b[39m [################..........................................] \u001b8Selecting previously unselected package libxrandr2:amd64.\n",
      "Preparing to unpack .../52-libxrandr2_2%3a1.5.2-1build1_amd64.deb ...\n",
      "Unpacking libxrandr2:amd64 (2:1.5.2-1build1) ...\n",
      "Selecting previously unselected package dbus-user-session.\n",
      "Preparing to unpack .../53-dbus-user-session_1.12.20-2ubuntu4.1_amd64.deb ...\n",
      "Unpacking dbus-user-session (1.12.20-2ubuntu4.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 29%]\u001b[49m\u001b[39m [################..........................................] \u001b8Selecting previously unselected package libdconf1:amd64.\n",
      "Preparing to unpack .../54-libdconf1_0.40.0-3_amd64.deb ...\n",
      "Unpacking libdconf1:amd64 (0.40.0-3) ...\n",
      "Selecting previously unselected package dconf-service.\n",
      "Preparing to unpack .../55-dconf-service_0.40.0-3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 30%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Unpacking dconf-service (0.40.0-3) ...\n",
      "Selecting previously unselected package dconf-gsettings-backend:amd64.\n",
      "Preparing to unpack .../56-dconf-gsettings-backend_0.40.0-3_amd64.deb ...\n",
      "Unpacking dconf-gsettings-backend:amd64 (0.40.0-3) ...\n",
      "Selecting previously unselected package libgtk-3-common.\n",
      "Preparing to unpack .../57-libgtk-3-common_3.24.33-1ubuntu2.2_all.deb ...\n",
      "Unpacking libgtk-3-common (3.24.33-1ubuntu2.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 31%]\u001b[49m\u001b[39m [##################........................................] \u001b8Selecting previously unselected package libgtk-3-0:amd64.\n",
      "Preparing to unpack .../58-libgtk-3-0_3.24.33-1ubuntu2.2_amd64.deb ...\n",
      "Unpacking libgtk-3-0:amd64 (3.24.33-1ubuntu2.2) ...\n",
      "Selecting previously unselected package libasound2-data.\n",
      "Preparing to unpack .../59-libasound2-data_1.2.6.1-1ubuntu1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 32%]\u001b[49m\u001b[39m [##################........................................] \u001b8Unpacking libasound2-data (1.2.6.1-1ubuntu1) ...\n",
      "Selecting previously unselected package libasound2:amd64.\n",
      "Preparing to unpack .../60-libasound2_1.2.6.1-1ubuntu1_amd64.deb ...\n",
      "Unpacking libasound2:amd64 (1.2.6.1-1ubuntu1) ...\n",
      "Selecting previously unselected package libgif7:amd64.\n",
      "Preparing to unpack .../61-libgif7_5.1.9-2ubuntu0.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Unpacking libgif7:amd64 (5.1.9-2ubuntu0.1) ...\n",
      "Selecting previously unselected package x11-common.\n",
      "Preparing to unpack .../62-x11-common_1%3a7.7+23ubuntu2_all.deb ...\n",
      "Unpacking x11-common (1:7.7+23ubuntu2) ...\n",
      "Selecting previously unselected package libxtst6:amd64.\n",
      "Preparing to unpack .../63-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 34%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
      "Selecting previously unselected package openjdk-21-jre:amd64.\n",
      "Preparing to unpack .../64-openjdk-21-jre_21.0.4+7-1ubuntu2~22.04_amd64.deb ...\n",
      "Unpacking openjdk-21-jre:amd64 (21.0.4+7-1ubuntu2~22.04) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 35%]\u001b[49m\u001b[39m [####################......................................] \u001b8Selecting previously unselected package cypher-shell.\n",
      "Preparing to unpack .../65-cypher-shell_1%3a5.24.1_all.deb ...\n",
      "Unpacking cypher-shell (1:5.24.1) ...\n",
      "Selecting previously unselected package neo4j.\n",
      "Preparing to unpack .../66-neo4j_1%3a5.23.0_all.deb ...\n",
      "Unpacking neo4j (1:5.23.0) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 36%]\u001b[49m\u001b[39m [####################......................................] \u001b8Selecting previously unselected package libxmuu1:amd64.\n",
      "Preparing to unpack .../67-libxmuu1_2%3a1.1.3-3_amd64.deb ...\n",
      "Unpacking libxmuu1:amd64 (2:1.1.3-3) ...\n",
      "Selecting previously unselected package alsa-topology-conf.\n",
      "Preparing to unpack .../68-alsa-topology-conf_1.2.5.1-2_all.deb ...\n",
      "Unpacking alsa-topology-conf (1.2.5.1-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 37%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Selecting previously unselected package alsa-ucm-conf.\n",
      "Preparing to unpack .../69-alsa-ucm-conf_1.2.6.3-1ubuntu1.11_all.deb ...\n",
      "Unpacking alsa-ucm-conf (1.2.6.3-1ubuntu1.11) ...\n",
      "Selecting previously unselected package session-migration.\n",
      "Preparing to unpack .../70-session-migration_0.3.6_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [######################....................................] \u001b8Unpacking session-migration (0.3.6) ...\n",
      "Selecting previously unselected package gsettings-desktop-schemas.\n",
      "Preparing to unpack .../71-gsettings-desktop-schemas_42.0-1ubuntu1_all.deb ...\n",
      "Unpacking gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
      "Selecting previously unselected package at-spi2-core.\n",
      "Preparing to unpack .../72-at-spi2-core_2.44.0-3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 39%]\u001b[49m\u001b[39m [######################....................................] \u001b8Unpacking at-spi2-core (2.44.0-3) ...\n",
      "Selecting previously unselected package fonts-dejavu-extra.\n",
      "Preparing to unpack .../73-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
      "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
      "Selecting previously unselected package libfontenc1:amd64.\n",
      "Preparing to unpack .../74-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
      "Selecting previously unselected package libice6:amd64.\n",
      "Preparing to unpack .../75-libice6_2%3a1.0.10-1build2_amd64.deb ...\n",
      "Unpacking libice6:amd64 (2:1.0.10-1build2) ...\n",
      "Selecting previously unselected package libsm6:amd64.\n",
      "Preparing to unpack .../76-libsm6_2%3a1.2.3-1build2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 41%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Unpacking libsm6:amd64 (2:1.2.3-1build2) ...\n",
      "Selecting previously unselected package libxt6:amd64.\n",
      "Preparing to unpack .../77-libxt6_1%3a1.2.1-1_amd64.deb ...\n",
      "Unpacking libxt6:amd64 (1:1.2.1-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 42%]\u001b[49m\u001b[39m [########################..................................] \u001b8Selecting previously unselected package libxmu6:amd64.\n",
      "Preparing to unpack .../78-libxmu6_2%3a1.1.3-3_amd64.deb ...\n",
      "Unpacking libxmu6:amd64 (2:1.1.3-3) ...\n",
      "Selecting previously unselected package libxaw7:amd64.\n",
      "Preparing to unpack .../79-libxaw7_2%3a1.0.14-1_amd64.deb ...\n",
      "Unpacking libxaw7:amd64 (2:1.0.14-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 43%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Selecting previously unselected package libxcb-shape0:amd64.\n",
      "Preparing to unpack .../80-libxcb-shape0_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-shape0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxft2:amd64.\n",
      "Preparing to unpack .../81-libxft2_2.3.4-1_amd64.deb ...\n",
      "Unpacking libxft2:amd64 (2.3.4-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 44%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Selecting previously unselected package libxkbfile1:amd64.\n",
      "Preparing to unpack .../82-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
      "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
      "Selecting previously unselected package libxv1:amd64.\n",
      "Preparing to unpack .../83-libxv1_2%3a1.0.11-1build2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 45%]\u001b[49m\u001b[39m [##########################................................] \u001b8Unpacking libxv1:amd64 (2:1.0.11-1build2) ...\n",
      "Selecting previously unselected package libxxf86dga1:amd64.\n",
      "Preparing to unpack .../84-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
      "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
      "Selecting previously unselected package x11-utils.\n",
      "Preparing to unpack .../85-x11-utils_7.7+5build2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 46%]\u001b[49m\u001b[39m [##########################................................] \u001b8Unpacking x11-utils (7.7+5build2) ...\n",
      "Selecting previously unselected package libatk-wrapper-java.\n",
      "Preparing to unpack .../86-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
      "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
      "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
      "Preparing to unpack .../87-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 47%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
      "Selecting previously unselected package libgdk-pixbuf2.0-bin.\n",
      "Preparing to unpack .../88-libgdk-pixbuf2.0-bin_2.42.8+dfsg-1ubuntu0.3_amd64.deb ...\n",
      "Unpacking libgdk-pixbuf2.0-bin (2.42.8+dfsg-1ubuntu0.3) ...\n",
      "Selecting previously unselected package libgtk-3-bin.\n",
      "Preparing to unpack .../89-libgtk-3-bin_3.24.33-1ubuntu2.2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 48%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Unpacking libgtk-3-bin (3.24.33-1ubuntu2.2) ...\n",
      "Selecting previously unselected package librsvg2-2:amd64.\n",
      "Preparing to unpack .../90-librsvg2-2_2.52.5+dfsg-3ubuntu0.2_amd64.deb ...\n",
      "Unpacking librsvg2-2:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 49%]\u001b[49m\u001b[39m [############################..............................] \u001b8Selecting previously unselected package librsvg2-common:amd64.\n",
      "Preparing to unpack .../91-librsvg2-common_2.52.5+dfsg-3ubuntu0.2_amd64.deb ...\n",
      "Unpacking librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
      "Setting up libgraphite2-3:amd64 (1.3.14-1build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [#############################.............................] \u001b8Setting up liblcms2-2:amd64 (2.12~rc1-2build2) ...\n",
      "Setting up libpixman-1-0:amd64 (0.40.0-1ubuntu0.22.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 51%]\u001b[49m\u001b[39m [#############################.............................] \u001b8Setting up session-migration (0.3.6) ...\n",
      "Created symlink /etc/systemd/user/graphical-session-pre.target.wants/session-migration.service → /usr/lib/systemd/user/session-migration.service.\n",
      "Setting up fontconfig (2.13.1-4.2ubuntu5) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 52%]\u001b[49m\u001b[39m [##############################............................] \u001b8Regenerating fonts cache... done.\n",
      "Setting up libxdamage1:amd64 (1:1.1.5-2build2) ...\n",
      "Setting up hicolor-icon-theme (0.17-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 53%]\u001b[49m\u001b[39m [##############################............................] \u001b8Setting up libxi6:amd64 (2:1.8-1build1) ...\n",
      "Setting up java-common (0.72build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 54%]\u001b[49m\u001b[39m [###############################...........................] \u001b8Setting up libxrender1:amd64 (1:0.9.10-1build4) ...\n",
      "Setting up libdatrie1:amd64 (0.2.13-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 55%]\u001b[49m\u001b[39m [################################..........................] \u001b8Setting up psmisc (23.4-2build3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8Setting up libxcb-render0:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up libxcursor1:amd64 (1:1.2.0-2build4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 57%]\u001b[49m\u001b[39m [#################################.........................] \u001b8Setting up libgdk-pixbuf2.0-common (2.42.8+dfsg-1ubuntu0.3) ...\n",
      "Setting up libxcb-shape0:amd64 (1.14-3ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 58%]\u001b[49m\u001b[39m [#################################.........................] \u001b8Setting up x11-common (1:7.7+23ubuntu2) ...\n",
      "invoke-rc.d: could not determine current runlevel\n",
      "invoke-rc.d: policy-rc.d denied execution of start.\n",
      "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 59%]\u001b[49m\u001b[39m [##################################........................] \u001b8Setting up xkb-data (2.33-1) ...\n",
      "Setting up libatspi2.0-0:amd64 (2.44.0-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8Setting up libcairo2:amd64 (1.16.0-5ubuntu2) ...\n",
      "Setting up libcolord2:amd64 (1.4.6-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 61%]\u001b[49m\u001b[39m [###################################.......................] \u001b8Setting up libdconf1:amd64 (0.40.0-3) ...\n",
      "Setting up libasound2-data (1.2.6.1-1ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [####################################......................] \u001b8Setting up dbus-user-session (1.12.20-2ubuntu4.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 63%]\u001b[49m\u001b[39m [####################################......................] \u001b8Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
      "Setting up libepoxy0:amd64 (1.5.10-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 64%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8Setting up libnspr4:amd64 (2:4.35-0ubuntu0.22.04.1) ...\n",
      "Setting up libavahi-common-data:amd64 (0.8-5ubuntu5.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 65%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8Setting up libfribidi0:amd64 (1.0.8-2ubuntu3.1) ...\n",
      "Setting up shared-mime-info (2.1-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 66%]\u001b[49m\u001b[39m [######################################....................] \u001b8Setting up libxinerama1:amd64 (2:1.1.4-3) ...\n",
      "Setting up libxv1:amd64 (2:1.0.11-1build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 67%]\u001b[49m\u001b[39m [######################################....................] \u001b8Setting up libxrandr2:amd64 (2:1.5.2-1build1) ...\n",
      "Setting up daemon (0.8-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 68%]\u001b[49m\u001b[39m [#######################################...................] \u001b8Setting up libpcsclite1:amd64 (1.9.5-3ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 69%]\u001b[49m\u001b[39m [########################################..................] \u001b8Setting up libgif7:amd64 (5.1.9-2ubuntu0.1) ...\n",
      "Setting up libatk1.0-data (2.36.0-3build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 70%]\u001b[49m\u001b[39m [########################################..................] \u001b8Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
      "Setting up alsa-topology-conf (1.2.5.1-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 71%]\u001b[49m\u001b[39m [#########################################.................] \u001b8Setting up libasound2:amd64 (1.2.6.1-1ubuntu1) ...\n",
      "Setting up libharfbuzz0b:amd64 (2.7.4-1ubuntu3.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 72%]\u001b[49m\u001b[39m [#########################################.................] \u001b8Setting up libthai-data (0.1.29-1build1) ...\n",
      "Setting up libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 73%]\u001b[49m\u001b[39m [##########################################................] \u001b8Setting up libcairo-gobject2:amd64 (1.16.0-5ubuntu2) ...\n",
      "Setting up libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 74%]\u001b[49m\u001b[39m [##########################################................] \u001b8Setting up libwayland-egl1:amd64 (1.20.0-1ubuntu0.1) ...\n",
      "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 75%]\u001b[49m\u001b[39m [###########################################...............] \u001b8Setting up libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 76%]\u001b[49m\u001b[39m [############################################..............] \u001b8Setting up libxmuu1:amd64 (2:1.1.3-3) ...\n",
      "Setting up libxkbcommon0:amd64 (1.4.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 77%]\u001b[49m\u001b[39m [############################################..............] \u001b8Setting up libwayland-client0:amd64 (1.20.0-1ubuntu0.1) ...\n",
      "Setting up gtk-update-icon-cache (3.24.33-1ubuntu2.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 78%]\u001b[49m\u001b[39m [#############################################.............] \u001b8Setting up libice6:amd64 (2:1.0.10-1build2) ...\n",
      "Setting up libxft2:amd64 (2.3.4-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 79%]\u001b[49m\u001b[39m [#############################################.............] \u001b8Setting up alsa-ucm-conf (1.2.6.3-1ubuntu1.11) ...\n",
      "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8Setting up libavahi-common3:amd64 (0.8-5ubuntu5.2) ...\n",
      "Setting up dconf-service (0.40.0-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 81%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up libnss3:amd64 (2:3.98-0ubuntu0.22.04.2) ...\n",
      "Setting up libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 82%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up libthai0:amd64 (0.1.29-1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 83%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up libgdk-pixbuf2.0-bin (2.42.8+dfsg-1ubuntu0.3) ...\n",
      "Setting up libwayland-cursor0:amd64 (1.20.0-1ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 84%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up libsm6:amd64 (2:1.2.3-1build2) ...\n",
      "Setting up libavahi-client3:amd64 (0.8-5ubuntu5.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 85%]\u001b[49m\u001b[39m [#################################################.........] \u001b8Setting up dconf-gsettings-backend:amd64 (0.40.0-3) ...\n",
      "Setting up libpango-1.0-0:amd64 (1.50.6+ds-2ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 86%]\u001b[49m\u001b[39m [##################################################........] \u001b8Setting up libxt6:amd64 (1:1.2.1-1) ...\n",
      "Setting up libcups2:amd64 (2.4.1op1-1ubuntu4.11) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 87%]\u001b[49m\u001b[39m [##################################################........] \u001b8Setting up libpangoft2-1.0-0:amd64 (1.50.6+ds-2ubuntu1) ...\n",
      "Setting up libgtk-3-common (3.24.33-1ubuntu2.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 88%]\u001b[49m\u001b[39m [###################################################.......] \u001b8Setting up libpangocairo-1.0-0:amd64 (1.50.6+ds-2ubuntu1) ...\n",
      "Setting up gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
      "Setting up libxmu6:amd64 (2:1.1.3-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 89%]\u001b[49m\u001b[39m [###################################################.......] \u001b8Setting up libxaw7:amd64 (2:1.0.14-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 90%]\u001b[49m\u001b[39m [####################################################......] \u001b8Setting up librsvg2-2:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
      "Setting up librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 91%]\u001b[49m\u001b[39m [####################################################......] \u001b8Setting up x11-utils (7.7+5build2) ...\n",
      "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
      "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 92%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8Setting up adwaita-icon-theme (41.0-1ubuntu1) ...\n",
      "update-alternatives: using /usr/share/icons/Adwaita/cursor.theme to provide /usr/share/icons/default/index.theme (x-cursor-theme) in auto mode\n",
      "Setting up openjdk-21-jre-headless:amd64 (21.0.4+7-1ubuntu2~22.04) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 93%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/java to provide /usr/bin/java (java) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jpackage to provide /usr/bin/jpackage (jpackage) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/lib/jexec to provide /usr/bin/jexec (jexec) in auto mode\n",
      "Setting up humanity-icon-theme (0.6.16) ...\n",
      "Setting up ca-certificates-java (20190909ubuntu1.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 94%]\u001b[49m\u001b[39m [######################################################....] \u001b8head: cannot open '/etc/ssl/certs/java/cacerts' for reading: No such file or directory\n",
      "Adding debian:ACCVRAIZ1.pem\n",
      "Adding debian:AC_RAIZ_FNMT-RCM.pem\n",
      "Adding debian:AC_RAIZ_FNMT-RCM_SERVIDORES_SEGUROS.pem\n",
      "Adding debian:ANF_Secure_Server_Root_CA.pem\n",
      "Adding debian:Actalis_Authentication_Root_CA.pem\n",
      "Adding debian:AffirmTrust_Commercial.pem\n",
      "Adding debian:AffirmTrust_Networking.pem\n",
      "Adding debian:AffirmTrust_Premium.pem\n",
      "Adding debian:AffirmTrust_Premium_ECC.pem\n",
      "Adding debian:Amazon_Root_CA_1.pem\n",
      "Adding debian:Amazon_Root_CA_2.pem\n",
      "Adding debian:Amazon_Root_CA_3.pem\n",
      "Adding debian:Amazon_Root_CA_4.pem\n",
      "Adding debian:Atos_TrustedRoot_2011.pem\n",
      "Adding debian:Autoridad_de_Certificacion_Firmaprofesional_CIF_A62634068.pem\n",
      "Adding debian:Autoridad_de_Certificacion_Firmaprofesional_CIF_A62634068_2.pem\n",
      "Adding debian:Baltimore_CyberTrust_Root.pem\n",
      "Adding debian:Buypass_Class_2_Root_CA.pem\n",
      "Adding debian:Buypass_Class_3_Root_CA.pem\n",
      "Adding debian:CA_Disig_Root_R2.pem\n",
      "Adding debian:CFCA_EV_ROOT.pem\n",
      "Adding debian:COMODO_Certification_Authority.pem\n",
      "Adding debian:COMODO_ECC_Certification_Authority.pem\n",
      "Adding debian:COMODO_RSA_Certification_Authority.pem\n",
      "Adding debian:Certainly_Root_E1.pem\n",
      "Adding debian:Certainly_Root_R1.pem\n",
      "Adding debian:Certigna.pem\n",
      "Adding debian:Certigna_Root_CA.pem\n",
      "Adding debian:Certum_EC-384_CA.pem\n",
      "Adding debian:Certum_Trusted_Network_CA.pem\n",
      "Adding debian:Certum_Trusted_Network_CA_2.pem\n",
      "Adding debian:Certum_Trusted_Root_CA.pem\n",
      "Adding debian:Comodo_AAA_Services_root.pem\n",
      "Adding debian:D-TRUST_BR_Root_CA_1_2020.pem\n",
      "Adding debian:D-TRUST_EV_Root_CA_1_2020.pem\n",
      "Adding debian:D-TRUST_Root_Class_3_CA_2_2009.pem\n",
      "Adding debian:D-TRUST_Root_Class_3_CA_2_EV_2009.pem\n",
      "Adding debian:DigiCert_Assured_ID_Root_CA.pem\n",
      "Adding debian:DigiCert_Assured_ID_Root_G2.pem\n",
      "Adding debian:DigiCert_Assured_ID_Root_G3.pem\n",
      "Adding debian:DigiCert_Global_Root_CA.pem\n",
      "Adding debian:DigiCert_Global_Root_G2.pem\n",
      "Adding debian:DigiCert_Global_Root_G3.pem\n",
      "Adding debian:DigiCert_High_Assurance_EV_Root_CA.pem\n",
      "Adding debian:DigiCert_TLS_ECC_P384_Root_G5.pem\n",
      "Adding debian:DigiCert_TLS_RSA4096_Root_G5.pem\n",
      "Adding debian:DigiCert_Trusted_Root_G4.pem\n",
      "Adding debian:E-Tugra_Certification_Authority.pem\n",
      "Adding debian:E-Tugra_Global_Root_CA_ECC_v3.pem\n",
      "Adding debian:E-Tugra_Global_Root_CA_RSA_v3.pem\n",
      "Adding debian:Entrust.net_Premium_2048_Secure_Server_CA.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority_-_EC1.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority_-_G2.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority_-_G4.pem\n",
      "Adding debian:GDCA_TrustAUTH_R5_ROOT.pem\n",
      "Adding debian:GLOBALTRUST_2020.pem\n",
      "Adding debian:GTS_Root_R1.pem\n",
      "Adding debian:GTS_Root_R2.pem\n",
      "Adding debian:GTS_Root_R3.pem\n",
      "Adding debian:GTS_Root_R4.pem\n",
      "Adding debian:GlobalSign_ECC_Root_CA_-_R4.pem\n",
      "Adding debian:GlobalSign_ECC_Root_CA_-_R5.pem\n",
      "Adding debian:GlobalSign_Root_CA.pem\n",
      "Adding debian:GlobalSign_Root_CA_-_R3.pem\n",
      "Adding debian:GlobalSign_Root_CA_-_R6.pem\n",
      "Adding debian:GlobalSign_Root_E46.pem\n",
      "Adding debian:GlobalSign_Root_R46.pem\n",
      "Adding debian:Go_Daddy_Class_2_CA.pem\n",
      "Adding debian:Go_Daddy_Root_Certificate_Authority_-_G2.pem\n",
      "Adding debian:HARICA_TLS_ECC_Root_CA_2021.pem\n",
      "Adding debian:HARICA_TLS_RSA_Root_CA_2021.pem\n",
      "Adding debian:Hellenic_Academic_and_Research_Institutions_ECC_RootCA_2015.pem\n",
      "Adding debian:Hellenic_Academic_and_Research_Institutions_RootCA_2015.pem\n",
      "Adding debian:HiPKI_Root_CA_-_G1.pem\n",
      "Adding debian:Hongkong_Post_Root_CA_1.pem\n",
      "Adding debian:Hongkong_Post_Root_CA_3.pem\n",
      "Adding debian:ISRG_Root_X1.pem\n",
      "Adding debian:ISRG_Root_X2.pem\n",
      "Adding debian:IdenTrust_Commercial_Root_CA_1.pem\n",
      "Adding debian:IdenTrust_Public_Sector_Root_CA_1.pem\n",
      "Adding debian:Izenpe.com.pem\n",
      "Adding debian:Microsec_e-Szigno_Root_CA_2009.pem\n",
      "Adding debian:Microsoft_ECC_Root_Certificate_Authority_2017.pem\n",
      "Adding debian:Microsoft_RSA_Root_Certificate_Authority_2017.pem\n",
      "Adding debian:NAVER_Global_Root_Certification_Authority.pem\n",
      "Adding debian:NetLock_Arany_=Class_Gold=_Főtanúsítvány.pem\n",
      "Adding debian:OISTE_WISeKey_Global_Root_GB_CA.pem\n",
      "Adding debian:OISTE_WISeKey_Global_Root_GC_CA.pem\n",
      "Adding debian:QuoVadis_Root_CA_1_G3.pem\n",
      "Adding debian:QuoVadis_Root_CA_2.pem\n",
      "Adding debian:QuoVadis_Root_CA_2_G3.pem\n",
      "Adding debian:QuoVadis_Root_CA_3.pem\n",
      "Adding debian:QuoVadis_Root_CA_3_G3.pem\n",
      "Adding debian:SSL.com_EV_Root_Certification_Authority_ECC.pem\n",
      "Adding debian:SSL.com_EV_Root_Certification_Authority_RSA_R2.pem\n",
      "Adding debian:SSL.com_Root_Certification_Authority_ECC.pem\n",
      "Adding debian:SSL.com_Root_Certification_Authority_RSA.pem\n",
      "Adding debian:SZAFIR_ROOT_CA2.pem\n",
      "Adding debian:SecureSign_RootCA11.pem\n",
      "Adding debian:SecureTrust_CA.pem\n",
      "Adding debian:Secure_Global_CA.pem\n",
      "Adding debian:Security_Communication_ECC_RootCA1.pem\n",
      "Adding debian:Security_Communication_RootCA2.pem\n",
      "Adding debian:Security_Communication_RootCA3.pem\n",
      "Adding debian:Security_Communication_Root_CA.pem\n",
      "Adding debian:Starfield_Class_2_CA.pem\n",
      "Adding debian:Starfield_Root_Certificate_Authority_-_G2.pem\n",
      "Adding debian:Starfield_Services_Root_Certificate_Authority_-_G2.pem\n",
      "Adding debian:SwissSign_Gold_CA_-_G2.pem\n",
      "Adding debian:SwissSign_Silver_CA_-_G2.pem\n",
      "Adding debian:T-TeleSec_GlobalRoot_Class_2.pem\n",
      "Adding debian:T-TeleSec_GlobalRoot_Class_3.pem\n",
      "Adding debian:TUBITAK_Kamu_SM_SSL_Kok_Sertifikasi_-_Surum_1.pem\n",
      "Adding debian:TWCA_Global_Root_CA.pem\n",
      "Adding debian:TWCA_Root_Certification_Authority.pem\n",
      "Adding debian:TeliaSonera_Root_CA_v1.pem\n",
      "Adding debian:Telia_Root_CA_v2.pem\n",
      "Adding debian:Trustwave_Global_Certification_Authority.pem\n",
      "Adding debian:Trustwave_Global_ECC_P256_Certification_Authority.pem\n",
      "Adding debian:Trustwave_Global_ECC_P384_Certification_Authority.pem\n",
      "Adding debian:TunTrust_Root_CA.pem\n",
      "Adding debian:UCA_Extended_Validation_Root.pem\n",
      "Adding debian:UCA_Global_G2_Root.pem\n",
      "Adding debian:USERTrust_ECC_Certification_Authority.pem\n",
      "Adding debian:USERTrust_RSA_Certification_Authority.pem\n",
      "Adding debian:XRamp_Global_CA_Root.pem\n",
      "Adding debian:certSIGN_ROOT_CA.pem\n",
      "Adding debian:certSIGN_Root_CA_G2.pem\n",
      "Adding debian:e-Szigno_Root_CA_2017.pem\n",
      "Adding debian:ePKI_Root_Certification_Authority.pem\n",
      "Adding debian:emSign_ECC_Root_CA_-_C3.pem\n",
      "Adding debian:emSign_ECC_Root_CA_-_G3.pem\n",
      "Adding debian:emSign_Root_CA_-_C1.pem\n",
      "Adding debian:emSign_Root_CA_-_G1.pem\n",
      "Adding debian:vTrus_ECC_Root_CA.pem\n",
      "Adding debian:vTrus_Root_CA.pem\n",
      "done.\n",
      "Setting up ubuntu-mono (20.10-0ubuntu2) ...\n",
      "Processing triggers for ca-certificates (20230311ubuntu0.22.04.1) ...\n",
      "Updating certificates in /etc/ssl/certs...\n",
      "0 added, 0 removed; done.\n",
      "Running hooks in /etc/ca-certificates/update.d...\n",
      "\n",
      "done.\n",
      "done.\n",
      "Processing triggers for libglib2.0-0:amd64 (2.72.4-0ubuntu2.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 95%]\u001b[49m\u001b[39m [#######################################################...] \u001b8Setting up libgtk-3-0:amd64 (3.24.33-1ubuntu2.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 96%]\u001b[49m\u001b[39m [#######################################################...] \u001b8Processing triggers for libc-bin (2.35-0ubuntu3.6) ...\n",
      "Setting up libgtk-3-bin (3.24.33-1ubuntu2.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 97%]\u001b[49m\u001b[39m [########################################################..] \u001b8Setting up openjdk-21-jre:amd64 (21.0.4+7-1ubuntu2~22.04) ...\n",
      "Setting up at-spi2-core (2.44.0-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 98%]\u001b[49m\u001b[39m [########################################################..] \u001b8Setting up cypher-shell (1:5.24.1) ...\n",
      "Setting up neo4j (1:5.23.0) ...\n",
      "Adding system user `neo4j' (UID 105) ...\n",
      "Adding new user `neo4j' (UID 105) with group `neo4j' ...\n",
      "Not creating home directory `/var/lib/neo4j'.\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 99%]\u001b[49m\u001b[39m [#########################################################.] \u001b8Processing triggers for libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.3) ...\n",
      "\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  openjdk-17-jre-headless\n",
      "Suggested packages:\n",
      "  libnss-mdns fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
      "  | fonts-wqy-zenhei fonts-indic\n",
      "The following NEW packages will be installed:\n",
      "  openjdk-17-jre openjdk-17-jre-headless\n",
      "0 upgraded, 2 newly installed, 0 to remove and 95 not upgraded.\n",
      "Need to get 48.4 MB of archives.\n",
      "After this operation, 193 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-17-jre-headless amd64 17.0.12+7-1ubuntu2~22.04 [48.2 MB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-17-jre amd64 17.0.12+7-1ubuntu2~22.04 [203 kB]\n",
      "Fetched 48.4 MB in 5s (9302 kB/s)        \u001b[0m\u001b[33m\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package openjdk-17-jre-headless:amd64.\n",
      "(Reading database ... 39391 files and directories currently installed.)\n",
      "Preparing to unpack .../openjdk-17-jre-headless_17.0.12+7-1ubuntu2~22.04_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 11%]\u001b[49m\u001b[39m [######....................................................] \u001b8Unpacking openjdk-17-jre-headless:amd64 (17.0.12+7-1ubuntu2~22.04) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 22%]\u001b[49m\u001b[39m [############..............................................] \u001b8Selecting previously unselected package openjdk-17-jre:amd64.\n",
      "Preparing to unpack .../openjdk-17-jre_17.0.12+7-1ubuntu2~22.04_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Unpacking openjdk-17-jre:amd64 (17.0.12+7-1ubuntu2~22.04) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 44%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Setting up openjdk-17-jre-headless:amd64 (17.0.12+7-1ubuntu2~22.04) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 67%]\u001b[49m\u001b[39m [######################################....................] \u001b8Setting up openjdk-17-jre:amd64 (17.0.12+7-1ubuntu2~22.04) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 78%]\u001b[49m\u001b[39m [#############################################.............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 89%]\u001b[49m\u001b[39m [###################################################.......] \u001b8Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J--2024-10-05 01:51:42--  https://github.com/neo4j/apoc/releases/download/5.23.0/apoc-5.23.0-core.jar\n",
      "Resolving github.com (github.com)... 140.82.114.3\n",
      "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/525054099/8ebcaa24-9228-4a23-9419-232e6d5c295d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241005%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241005T015142Z&X-Amz-Expires=300&X-Amz-Signature=0ed97850178a282331fa63bbef24e66a3950e8554f98c676caabda3985e482a5&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dapoc-5.23.0-core.jar&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-10-05 01:51:42--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/525054099/8ebcaa24-9228-4a23-9419-232e6d5c295d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241005%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241005T015142Z&X-Amz-Expires=300&X-Amz-Signature=0ed97850178a282331fa63bbef24e66a3950e8554f98c676caabda3985e482a5&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dapoc-5.23.0-core.jar&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2890267 (2.8M) [application/octet-stream]\n",
      "Saving to: ‘apoc-5.23.0-core.jar’\n",
      "\n",
      "apoc-5.23.0-core.ja 100%[===================>]   2.76M  --.-KB/s    in 0.09s   \n",
      "\n",
      "2024-10-05 01:51:43 (31.7 MB/s) - ‘apoc-5.23.0-core.jar’ saved [2890267/2890267]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up Neo4j\n",
    "!apt install dialog apt-utils -y \n",
    "!wget -O - https://debian.neo4j.com/neotechnology.gpg.key | gpg --dearmor -o /etc/apt/keyrings/neotechnology.gpg\n",
    "!echo 'deb [signed-by=/etc/apt/keyrings/neotechnology.gpg] https://debian.neo4j.com stable latest' | tee -a /etc/apt/sources.list.d/neo4j.list\n",
    "!apt list -a neo4j\n",
    "!add-apt-repository universe -y\n",
    "!apt install neo4j=1:5.23.0 -y\n",
    "!echo \"neo4j-enterprise neo4j/question select I ACCEPT\" | debconf-set-selections\n",
    "!echo \"neo4j-enterprise neo4j/license note\" | debconf-set-selections\n",
    "!apt install openjdk-17-jre -y\n",
    "!cd /var/lib/neo4j/plugins/ && wget https://github.com/neo4j/apoc/releases/download/5.23.0/apoc-5.23.0-core.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up llama.cpp 22m 32s\n",
    "!cd /workspace/repos/ && git clone https://github.com/ggerganov/llama.cpp\n",
    "!cd /workspace/repos/llama.cpp && git pull && make clean && LLAMA_CUDA=0 make\n",
    "!chmod 755 /workspace/repos/llama.cpp/requirements.txt && pip3 install -r /workspace/repos/llama.cpp/requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "save_dir = \"/workspace/data/compliance/proposal_embed\"\n",
    "\n",
    "# tokenizer_name = \"dunzhang/stella_en_1.5B_v5\"\n",
    "tokenizer_name = \"Qwen/Qwen2-1.5B\"\n",
    "model_name = \"/workspace/data/compliance/rule_proposal_embedding_model_gpt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             return_dict=True,\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device_map=\"auto\",\n",
    "                                             )\n",
    "print('Saving model')\n",
    "model.save_pretrained(save_dir)\n",
    "print('Saving tokenizer')\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /workspace/repos/llama.cpp/convert_hf_to_gguf.py /workspace/data/compliance/proposal_embed\n",
    "!cd /workspace/data/compliance/proposal_embed && echo 'FROM \"/workspace/data/compliance/proposal_embed/Proposal_Embed-1.8B-F16.gguf\"' >> Modelfile && ollama create proposal_embed -f Modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: notify if ollama server is running with model loaded\n",
    "import subprocess, os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.openai import OpenAI as LOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "# model_name, ctx_len = \"gpt-4o-2024-08-06\", 128000\n",
    "model_name, ctx_len = \"llama3.1:8b-instruct-q8_0\", 128000\n",
    "# model_name, ctx_len = \"solar-pro:latest\", 128000\n",
    "# model_name, ctx_len = \"hermes3:8b\", 128000\n",
    "# model_name, ctx_len = \"gemma2:latest\", 8192\n",
    "\n",
    "\n",
    "if \"gpt-4o\" in model_name:\n",
    "    openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "    \n",
    "    print(f\"Using OpenAI {model_name}...\")\n",
    "    llm = LOpenAI(model=model_name, max_tokens=8000)\n",
    "else:\n",
    "    subout = subprocess.run(['ollama', 'list'], capture_output=True, text=True)\n",
    "    if model_name in subout.stdout:\n",
    "        print('Model loaded...')\n",
    "    else:\n",
    "        try: \n",
    "            print(\"Pulling Ollama model...\")\n",
    "            sub_out = subprocess.run(['ollama', 'pull', model_name], capture_output=True, text=True)\n",
    "        except Exception as e: \n",
    "            print(f\"Error pulling model: Is the Ollama server running?\\n{e}\")\n",
    "    \n",
    "    addtion_kwargs = {\"max_new_tokens\": 8000, 'temperature': 0.9}\n",
    "    # system_prompt = \"You are an expert at answering questions about rules and regulations regarding Title 17—Commodity and Securities Exchanges: CHAPTER II—SECURITIES AND EXCHANGE COMMISSION. Please provide a summary of the following text, and cite any sections, rules, acts or laws (e.g. § 230.503, § 240.13a-15, Act (15 U.S.C. 781), Investment Company Act of 1940) from context that support the answer. Be detailed in your response.\"\n",
    "    llm = Ollama(model=model_name, url=\"http://127.0.0.1:11434\", context_window=ctx_len, model_type=\"chat\", is_function_calling_model=False, \n",
    "                 request_timeout=4000.0, **addtion_kwargs) #, system_prompt=system_prompt) additional_kwargs=addtion_kwargs,\n",
    "    print(llm.metadata)\n",
    "\n",
    "\n",
    "# Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEC rules and regulations\n",
    "!cd /workspace/data && curl -X GET \"https://www.ecfr.gov/api/versioner/v1/full/2024-07-23/title-17.xml?chapter=II\" -H \"accept: application/xml\" > title-17.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from sec_utils import get_tree_data\n",
    "\n",
    "# Path to your XML file\n",
    "xml_file_path = '/workspace/data/title-17.xml'\n",
    "\n",
    "# Parse the XML file\n",
    "tree = ET.parse(xml_file_path)\n",
    "\n",
    "# Get the root element of the XML document\n",
    "root = tree.getroot()\n",
    "\n",
    "sec_data = get_tree_data(root)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sec_utils import get_metadata\n",
    "from llama_index.core import Document\n",
    "\n",
    "documents = [Document(text=t, \n",
    "                          text_template='{metadata_str}\\n\\n{content}',\n",
    "                          metadata=get_metadata(m, t, metadata={\"section\":None, \"description\":None, \"mentioned_sections\":None})) \\\n",
    "                            for m,t in sec_data.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from llama_index.readers.file import PDFReader\n",
    "# from llamaindex_data_utils import extract_text_from_pdf\n",
    "from llama_index.core import Document\n",
    "from utils import fix_hyphenated_words\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "access_token = os.getenv('HF_TOKEN')\n",
    "llama_api_key = os.getenv('LLAMA_API_KEY')\n",
    "\n",
    "parser = PDFReader(return_full_document=True)\n",
    "documents_proposal = parser.load_data(\"/workspace/data/compliance/FSI_Rule_proposal.pdf\")\n",
    "\n",
    "# fixes poorly formatted hyphenated words\n",
    "doc_text = documents_proposal[0].text\n",
    "documents_proposal[0].text = fix_hyphenated_words(doc_text)\n",
    "\n",
    "# Remove the page numbers\n",
    "step=8000\n",
    "start=0\n",
    "track=[]\n",
    "for i in range(1, 95):  # Adjust the range as needed\n",
    "    # Regular expression pattern to match the specific number preceded by \\n \\n or \\n\\n and followed by a space\n",
    "    if i == 1:\n",
    "        pattern = rf'(\\n \\n|\\n\\n| \\n)({i})( )'\n",
    "    else:\n",
    "        pattern = rf'(\\n \\n|\\n\\n)({i})( )'\n",
    "    # Check if the pattern exists in the doc_text\n",
    "    while start<len(doc_text):\n",
    "        text_slice = doc_text[start:start+step]\n",
    "        if re.search(pattern, text_slice):\n",
    "            # Replace the matching patterns with an empty string\n",
    "            matches = list(re.finditer(pattern, text_slice))\n",
    "            # print(f\"Found {i} {matches[0].span()}\")\n",
    "            track.append(i)\n",
    "            bingo=matches[0].span()\n",
    "            text_slice = text_slice[:bingo[0]]+\"\\n\\n\"+text_slice[bingo[1]:]\n",
    "            doc_text = doc_text[:start] + text_slice + doc_text[start+step:]\n",
    "            start = bingo[1]\n",
    "            break\n",
    "        else:\n",
    "            start += step-100\n",
    "            text_slice = doc_text[start:start+step]\n",
    "\n",
    "\n",
    "documents_proposal[0].text = doc_text\n",
    "pattern = r'(?<=\\n)([IVX]+\\..*?)(?=\\n[IVX]+\\.\\s|\\Z)'\n",
    "sections=[]\n",
    "sections.append(\"Proposal Information\"+documents_proposal[0].text.split(\"I. Background A. Sta\")[0])\n",
    "sections += re.findall(pattern, documents_proposal[0].text, re.DOTALL)\n",
    "section4_5split = sections[4].split(\"V. Paperwork Reduction Act\")\n",
    "sections[4]=section4_5split[0]\n",
    "sections = sections[:5]+[\"V. Paperwork Reduction Act\"+section4_5split[-1]]+sections[5:]\n",
    "for shead in [x.split('\\n')[0].strip() for x in sections]:\n",
    "    print(shead)\n",
    "\n",
    "# # pdf_urls = [\"/workspace/data/compliance/FSI_Factsheet.pdf\", \"/workspace/data/compliance/FSI_Press_Release.pdf\", \"/workspace/data/compliance/FSI_Rule_proposal.pdf\"]\n",
    "# # descriptions = [\"Factsheet about newly proposed SEC rule.\", \"Press release regarding newly proposed SEC rule.\", \"The full text of the newly proposed SEC rule.\"]\n",
    "# # documents_proposal = extract_text_from_pdf(pdf_urls, llama_api_key, llamaparse_kwargs={\"split_by_page\":False}, save_json_path=None)\n",
    "documents_proposal = [Document(text=t, \n",
    "                          text_template='{metadata_str}\\n\\n{content}') for t in sections]\n",
    "\n",
    "# add metadata to the documents_proposal\n",
    "for i in range(len(documents_proposal)):\n",
    "    documents_proposal[i].metadata[\"section\"] = documents_proposal[i].text.split(\"\\n\")[0].strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Distillation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "parser = SentenceSplitter(chunk_size=300, chunk_overlap=50)\n",
    "nodes = parser.get_nodes_from_documents(documents_proposal, show_progress=True)\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = 'Context information is below.\\n\\n---------------------\\n{context_str}\\n---------------------\\n\\nGiven the context information and no prior knowledge, generate {num_questions_per_chunk} questions based on the below query.\\n\\nYou are a FINRA certified Compliance Specialist that writes exams for firm compliance professionals. Your task is to write precise questions for an upcoming Compliance Officer certification examination. The questions should be diverse across the context with no multiple choice. Restrict the questions to the context information provided.\"\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.finetuning import generate_qa_embedding_pairs\n",
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import random\n",
    "\n",
    "output_path = \"/workspace/data/train_dataset_proposal.json\"\n",
    "if not os.path.exists(output_path):\n",
    "    rand_index = random.sample(range(len(nodes)), len(nodes))\n",
    "    train_perc=1.0\n",
    "    train_size = int(len(rand_index)*train_perc)\n",
    "\n",
    "    train_dataset = generate_qa_embedding_pairs(\n",
    "        qa_generate_prompt_tmpl = system_prompt,\n",
    "        num_questions_per_chunk=5,\n",
    "        save_every=20,\n",
    "        output_path=output_path,\n",
    "        llm=llm, \n",
    "        nodes=[nodes[x] for x in rand_index[:train_size]],\n",
    "        verbose=False\n",
    "    )\n",
    "    train_dataset.save_json(output_path)\n",
    "\n",
    "    # val_dataset = generate_qa_embedding_pairs(\n",
    "    #     qa_generate_prompt_tmpl = system_prompt,\n",
    "    #     num_questions_per_chunk=5,\n",
    "    #     save_every=500,\n",
    "    #     output_path=\"/workspace/data/val_dataset.json\",\n",
    "    #     llm=llm, \n",
    "    #     nodes=[nodes[x] for x in rand_index[train_size:]],\n",
    "    #     verbose=False\n",
    "    # )\n",
    "    # # assert len(val_dataset.relevant_docs) == len(val_dataset.queries)\n",
    "    # assert (np.unique(list(val_dataset.relevant_docs.values()), return_counts=True)[1]==5).all()\n",
    "    # val_dataset.save_json(\"/workspace/data/val_dataset.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#####################\n",
    "# in llama_index/embeddings/huggingface/base need to add \"show_progress_bar=False\" to 204, 213\n",
    "#####################\n",
    "\n",
    "import os, json\n",
    "\n",
    "from llama_index.finetuning import EmbeddingAdapterFinetuneEngine, SentenceTransformersFinetuneEngine\n",
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "\n",
    "train_path = \"/workspace/data/train_dataset_proposal.json\"\n",
    "val_path = None #\"/workspace/data/val_dataset.json\"\n",
    "if os.path.exists(train_path):\n",
    "    train_dataset = EmbeddingQAFinetuneDataset.from_json(train_path)\n",
    "if val_path is not None and os.path.exists(val_path):\n",
    "    val_dataset = EmbeddingQAFinetuneDataset.from_json(val_path)\n",
    "else:\n",
    "    val_dataset = None\n",
    "\n",
    "embed_model_name = \"dunzhang/stella_en_1.5B_v5\" #7b params\n",
    "print(\"loading embed model...\")\n",
    "\n",
    "finetune_engine = SentenceTransformersFinetuneEngine(\n",
    "    train_dataset,\n",
    "    embed_model_name,\n",
    "    batch_size=6,\n",
    "    model_output_path=\"/workspace/data/rule_proposal_embedding_model\",\n",
    "    val_dataset=val_dataset,\n",
    "    epochs=4,\n",
    "    show_progress_bar=True,\n",
    "    # can optionally pass along any parameters that go into `train_model`\n",
    "    # optimizer_class=torch.optim.SGD,\n",
    "    # optimizer_params={\"lr\": 0.0001, \"weight_decay\": 0.01}\n",
    ")\n",
    "\n",
    "\n",
    "# finetune_engine = EmbeddingAdapterFinetuneEngine(\n",
    "#     train_dataset,\n",
    "#     base_embed_model,\n",
    "#     batch_size=10,\n",
    "#     model_output_path=\"/workspace/data/adapter_model\",\n",
    "#     # val_dataset=val_dataset,\n",
    "#     epochs=4,\n",
    "#     verbose=False,\n",
    "#     # can optionally pass along any parameters that go into `train_model`\n",
    "#     # optimizer_class=torch.optim.SGD,\n",
    "#     # optimizer_params={\"lr\": 0.001}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_engine.finetune(**{\"lr\": 0.001, \"weight_decay\": 0.0001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "\n",
    "# Clear memory\n",
    "del finetune_engine, train_dataset\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# From second tmux screen start a new ollama server with the model\n",
    "# OLLAMA_HOST=\"http://127.0.0.1:11435\" ollama start \n",
    "proposal_embed_model = OllamaEmbedding(\n",
    "    model_name=\"proposal_embed:latest\",\n",
    "    base_url=\"http://127.0.0.1:11435\",\n",
    "    # ollama_additional_kwargs={\"mirostat\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "embed_model_name = \"dunzhang/stella_en_1.5B_v5\" #7b params\n",
    "finetuned_embed_model_name = \"/workspace/data/compliance/rule_proposal_embedding_model_gpt\"\n",
    "print(\"loading embed model...\")\n",
    "proposal_embed_model = HuggingFaceEmbedding(model_name=finetuned_embed_model_name)\n",
    "# rules_embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "\n",
    "# Settings.embed_model = embed_model\n",
    "# Settings.chunk_size = 300\n",
    "# Settings.chunk_overlap = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "entities = Literal[#\"PROPOSED_RULE\", \n",
    "                   \"ACTS\",\n",
    "                   \"SECTIONS\",\n",
    "                   \"RULES\"\n",
    "                   #\"AMENDMENTS\"\n",
    "]\n",
    "\n",
    "relations = Literal[\n",
    "    \"MENTIONS\",\n",
    "    #\"CHANGES\",\n",
    "    # \"AMENDS\", \n",
    "    \"REFERS_TO\"\n",
    "]\n",
    "\n",
    "validation_schema = {\n",
    "    # \"Proposed_Rules\": [\"CHANGES\", \"AMENDS\"],\n",
    "    \"Acts\": [\"MENTIONS\",\"REFERS_TO\"],\n",
    "    \"Sections\": [\"MENTIONS\",\"REFERS_TO\"],\n",
    "    \"Rules\": [\"MENTIONS\",\"REFERS_TO\"],\n",
    "    # \"Amendments\": [\"REFERS_TO\", \"AMENDS\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_utils import set_neo4j_password, add_lines_to_conf\n",
    "set_neo4j_password('bewaretheneo')\n",
    "# add_lines_to_conf()\n",
    "\n",
    "###### START NEO4J SERVER ######\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphRAG Database\n",
    "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor, SimpleLLMPathExtractor\n",
    "from rag_utils import create_neo4j_graph_store, create_neo4j_graphrag, neo4j_query, dump_neo4j_database\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "access_token = os.getenv('HF_TOKEN')\n",
    "llama_api_key = os.getenv('LLAMA_API_KEY')\n",
    "\n",
    "graph_idx_persist_dir = \"/workspace/data/compliance/graph_idx_testfull\"\n",
    "graph_store_persist_dir= None #\"/workspace/data/graph_store\"\n",
    "\n",
    "Settings.chunk_size = 500\n",
    "Settings.chunk_overlap = 20\n",
    "\n",
    "# kg_extractor = SchemaLLMPathExtractor(\n",
    "#     llm=llm,\n",
    "#     possible_entities=entities,\n",
    "#     possible_relations=relations,\n",
    "#     kg_validation_schema=validation_schema,\n",
    "#     strict=False,  # if false, will allow triples outside of the schema``\n",
    "#     num_workers=10,\n",
    "#     max_triplets_per_chunk=10,\n",
    "# )\n",
    "\n",
    "\n",
    "# extract_prompt = \"You are an expert compliance officer with vast knowledge of SEC Title 17 Chapter II. Your job is to read each section and link mentions of other sections, rules, or acts (e.g. § 230.503, § 240.13a-15, Act (15 U.S.C. 781), Investment Company Act of 1940) mentioned. If there are no mentions of other sections, rules, or acts, return an empty list.\"\n",
    "# extract_prompt = \"You are an expert compliance officer with vast knowledge of SEC Title 17 Chapter II. Your job is to read each section and link other sections, rules, and acts mentioned. What sections, rules, and acts (e.g. § 230.503, § 240.13a-15, Act (15 U.S.C. 781), Investment Company Act of 1940) are mentioned in the content? If there are no mentions, return an empty list.\"\n",
    "llm.is_function_calling_model = False\n",
    "extract_prompt = \"You are an expert compliance officer with vast knowledge of SEC Title 17 Chapter II. Your job is to find semantic, referential, and literal relationships between the sections. If there are no relationships, return an empty list.\"\n",
    "kg_extractor = SimpleLLMPathExtractor(\n",
    "        extract_prompt=extract_prompt,\n",
    "        llm=llm,\n",
    "        max_paths_per_chunk=10,\n",
    "        num_workers=6,\n",
    "    )\n",
    "\n",
    "# random.shuffle(documents)\n",
    "\n",
    "print(\"Creating graph store...\")\n",
    "graph_store = create_neo4j_graph_store(neo_url=\"bolt://localhost:7687\", \n",
    "                                       password=os.getenv(\"NEO4J_PWD\"), \n",
    "                                       config={\"connection_timeout\": 1000, \"connection_acquisition_timeout\": 1000, \"max_connection_pool_size\": 1000})\n",
    "\n",
    "if not os.path.exists(graph_idx_persist_dir):\n",
    "    print(\"Deleting all nodes and relationships...\")\n",
    "    neo4j_query(graph_store, query=\"\"\"MATCH n=() DETACH DELETE n\"\"\")\n",
    "\n",
    "print(\"Creating graphrag index...\")\n",
    "graph_index = create_neo4j_graphrag(documents, llm, rules_embed_model, kg_extractor, graph_store, graph_idx_persist_dir=graph_idx_persist_dir, graph_store_persist_dir=graph_store_persist_dir, similarity_top_k=3)\n",
    "\n",
    "dump_neo4j_database('neo4j', '/workspace/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = graph_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_query(graph_store, query=\"\"\"MATCH n=() DETACH DELETE n\"\"\")\n",
    "graph_store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Database RAG\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "from rag_utils import create_llama_vector_index_rag\n",
    "\n",
    "# Settings.llm = llm\n",
    "# Settings.embed_model = proposal_embed_model\n",
    "Settings.chunk_size = 300\n",
    "Settings.chunk_overlap = 20\n",
    "vector_index = create_llama_vector_index_rag(None, proposal_embed_model, documents=documents_proposal, persist_dir=\"/workspace/data/compliance/vector_idx_proposed\")\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    llm=llm,\n",
    "    similarity_top_k=5,\n",
    "    # node_postprocessors=[\n",
    "    #     LLMRerank(\n",
    "    #         choice_batch_size=5,\n",
    "    #         top_n=2,\n",
    "    #     )\n",
    "    # ],\n",
    "    # see https://github.com/run-llama/llama_index/blob/f7c5ee5efbb6172e819f26d1705fcdf6114b11a3/llama-index-core/llama_index/core/response_synthesizers/type.py#L4\n",
    "    response_mode=\"tree_summarize\", # \"accumulate\", \"compact_accumulate\", \"compact\", \"simple_summarize\", \"tree_summarize\"\n",
    ")\n",
    "\n",
    "# Settings.chunk_size = 500\n",
    "# Settings.chunk_overlap = 20\n",
    "# rules_vector_index = create_llama_vector_index_rag(llm, rules_embed_model, documents=documents, persist_dir=\"/workspace/data/compliance/vector_idx_sec\")\n",
    "# graph_index = rules_vector_index.as_query_engine(\n",
    "#     similarity_top_k=5,\n",
    "#     # node_postprocessors=[\n",
    "#     #     LLMRerank(\n",
    "#     #         choice_batch_size=5,\n",
    "#     #         top_n=2,\n",
    "#     #     )\n",
    "#     # ],\n",
    "#     # see https://github.com/run-llama/llama_index/blob/f7c5ee5efbb6172e819f26d1705fcdf6114b11a3/llama-index-core/llama_index/core/response_synthesizers/type.py#L4\n",
    "#     response_mode=\"tree_summarize\", # \"accumulate\", \"compact_accumulate\", \"compact\", \"simple_summarize\", \"tree_summarize\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Summarize section IX.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize each section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "\n",
    "def parse_list_from_output_string(output_string):\n",
    "    \"\"\"\n",
    "    Parses and extracts a Python list from the given output string.\n",
    "\n",
    "    Args:\n",
    "    output_string (str): The output string containing the list representation.\n",
    "\n",
    "    Returns:\n",
    "    list: The extracted Python list.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use regex to find the list portion within the string\n",
    "        list_match = re.search(r'(\\[[^\\]]*\\])', output_string, re.DOTALL)\n",
    "        if list_match:\n",
    "            list_str = list_match.group(1)\n",
    "            # Safely evaluate the list string\n",
    "            data = ast.literal_eval(list_str)\n",
    "            if isinstance(data, list):\n",
    "                return data\n",
    "            else:\n",
    "                raise ValueError(\"The extracted portion is not a list.\")\n",
    "        else:\n",
    "            raise ValueError(\"No list found in the output string.\")\n",
    "    except (ValueError, SyntaxError):\n",
    "        raise ValueError(\"The output string is not a valid list representation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "summarizer = TreeSummarize(llm=llm, verbose=True)\n",
    "# original prompt\n",
    "# prompt_summary = f\"\"\"\n",
    "# Summarize this section of a new SEC rule proposal and/or amendment. \n",
    "# Make sure the summary includes every mention of any regulatory section, rule, or act (e.g. 12 U.S.C. 1843(k)(4)(C), 240.13a-15, Investment Company Act of 1940) in the text. \n",
    "# Do not use any prior knowledge except what is in the text to generate the summary.\n",
    "# Be detailed and verbose in your response.\n",
    "# Do not output any premable.\n",
    "# \"\"\"\n",
    "\n",
    "# gpt enhanced prompt\n",
    "prompt_summary = \"\"\"\n",
    "Summarize the content of the following section from a new SEC rule proposal or amendment. Ensure that the summary:\n",
    "\n",
    "1. Includes every reference to any specific regulatory section, rule, or act mentioned in the text (e.g., 12 U.S.C. 1843(k)(4)(C), 240.13a-15, Investment Company Act of 1940).\n",
    "2. Stays strictly within the information presented in the text, without incorporating any outside or prior knowledge.\n",
    "3. Is detailed, thorough, and specific in its coverage of all key points.\n",
    "4. Avoids adding any introductory or concluding remarks outside the scope of the summary itself.\n",
    "\"\"\"\n",
    "\n",
    "section_summaries = []\n",
    "extracted_regs = []\n",
    "\n",
    "for i,sec in enumerate(sections):\n",
    "    rule_prompt = f\"\"\"\n",
    "    Extract all mentions of any regulatory sections, rules, and acts in the following text.\n",
    "    Compile all extracted items into a single Python List of Strings object.\n",
    "    For example, [\"Item 1\", \"Item 2\", \"Item 3\"].\n",
    "\n",
    "    Here is the text:\n",
    "    {sec}\n",
    "    \"\"\"\n",
    "    response = llm.complete(rule_prompt)\n",
    "\n",
    "    # List checker\n",
    "    rewrite_counter=0\n",
    "    while True:\n",
    "        try: \n",
    "            extracted_reg = parse_list_from_output_string(response.text)\n",
    "            break\n",
    "        except:\n",
    "            print(\"Correcting list format...\")\n",
    "            response = llm.complete(f\"\"\"The following text does not contain a valid Python List of Strings? \n",
    "                                        Rewrite the text so that the List is in a valid Python format.\n",
    "                                        For example, [\"Item 1\", \"Item 2\", \"Item 3\"].\\n\\n{response.text}\n",
    "                                    \"\"\")\n",
    "            rewrite_counter+=1\n",
    "            print(f\"   Rewrote list {rewrite_counter} times.\")\n",
    "            if rewrite_counter>5:\n",
    "                raise ValueError(\"Could not correct list format.\")\n",
    "\n",
    "    extracted_regs.append(extracted_reg)\n",
    "    prompt_summary = f\"\"\"\n",
    "    Summarize the content of the following section from a new SEC rule proposal or amendment. Ensure that the summary:\n",
    "\n",
    "    1. Includes every reference to any specific regulatory section, rule, or act mentioned in the text (e.g., 12 U.S.C. 1843(k)(4)(C), 240.13a-15, Unfunded Mandates Reform Act (section 202(a)), Investment Company Act of 1940).\n",
    "    2. Stays strictly within the information presented in the text, without incorporating any outside or prior knowledge.\n",
    "    3. Is detailed, thorough, and specific in its coverage of all key points.\n",
    "    4. Avoids adding any introductory or concluding remarks outside the scope of the summary itself.\n",
    "\n",
    "    Here are some of the regulatory sections, rules, and acts:\n",
    "    {extracted_reg} \n",
    "    \"\"\"\n",
    "    print(f\"Summarizing section {i+1}/{len(sections)}...\")\n",
    "    parser = SentenceSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    nodes = parser.get_nodes_from_documents([documents_proposal[i]], show_progress=True)\n",
    "    response = await summarizer.aget_response(prompt_summary, [doc.text for doc in nodes])\n",
    "    section_summaries.append(response)\n",
    "    # if i==3:\n",
    "    #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goods=0\n",
    "bads=0\n",
    "for i,(sec,summary) in enumerate(zip(sections, section_summaries)):\n",
    "    numbers = re.findall(r'(?<!\\w)(\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?|\\d+\\.\\d+|\\d+)(?!\\w)', summary)\n",
    "    numbers = [x for x in numbers if len(x)>1]\n",
    "    print(numbers)\n",
    "    for number in numbers:\n",
    "        if number not in sec:\n",
    "            bads+=1\n",
    "            print(f\"{number} not in section {i}!\")\n",
    "        else:\n",
    "            goods+=1\n",
    "\n",
    "print(f\"Goods: {goods}, Bads: {bads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "\n",
    "class PDF(FPDF):\n",
    "    def header(self):\n",
    "        self.set_font('Arial', 'B', 12)\n",
    "        self.cell(0, 10, 'Summary of Sections', 0, 1, 'C')\n",
    "\n",
    "    def chapter_title(self, title):\n",
    "        self.set_font('Arial', 'B', 14)\n",
    "        self.cell(0, 10, title, 0, 1, 'L')\n",
    "        self.ln(5)\n",
    "\n",
    "    def chapter_body(self, body):\n",
    "        self.set_font('Arial', '', 12)\n",
    "        self.multi_cell(0, 10, body)\n",
    "        self.ln()\n",
    "\n",
    "def create_pdf(summaries, section_headers, output_filename):\n",
    "    pdf = PDF()\n",
    "\n",
    "    pdf.add_page()\n",
    "\n",
    "    for header, summary in zip(section_headers, summaries):\n",
    "        print(header)\n",
    "        pdf.chapter_title(header.replace(\"’\", \"'\"))\n",
    "        pdf.chapter_body(summary.replace(\"–\", \"-\"))\n",
    "\n",
    "    pdf.output(output_filename)\n",
    "\n",
    "# Save the summaries to a PDF\n",
    "output_filename = \"/workspace/data/summary_of_sections.pdf\"\n",
    "section_headers = [x.split('\\n')[0].strip() for x in sections]\n",
    "create_pdf(section_summaries, section_headers, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_summary_of_summaries = f\"\"\"\n",
    "Provide a high-level summary of the following section summaries from a new SEC rule proposal. Ensure that the summary:\n",
    "\n",
    "1. Focuses on the most relevant points for a financial executive, emphasizing practical implications, compliance requirements, and potential business impacts.\n",
    "2. Is thorough yet concise, presenting detailed insights that are actionable and strategic.\n",
    "3. Avoids any preamble or extraneous details not directly related to the financial executive’s decision-making needs.\n",
    "\"\"\"\n",
    "summary_of_section_summaries = await summarizer.aget_response(prompt_summary_of_summaries, [doc for doc in section_summaries])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_of_section_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(section_summaries[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sections[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Securities Exchange Act of 1934\n",
    "# Investment Advisers Act of 1940\n",
    "# Gramm-Leach-Bliley Act\n",
    "# 31 U.S.C. § 5311-5336\n",
    "# 12 U.S.C. § 1843(k)(4)(C)\n",
    "# 15 U.S.C. 80b (Investment Advisers Act of 1940)\n",
    "# 15 U.S.C. 6809(2) (Gramm-Leach-Bliley Act)\n",
    "# Securities Exchange Act of 1934\n",
    "# 31 CFR part 1032\n",
    "# 31 CFR 1023.220 (Customer Identification Program rule for broker-dealers)\n",
    "# 31 CFR 1020.220 (CIP regulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comment section (Ollama embeddings)\n",
    "    - What are the main points?\n",
    "    - What are the company opinions?\n",
    "    - What are some of our opinions about main points? => RAG or GraphRAG\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import subprocess, os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.openai import OpenAI as LOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "from utils import parse_list_from_output_string, extract_list_from_string\n",
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    ")\n",
    "\n",
    "from llama_index.core.agent.react import ReActAgent\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import ToolSelection, ToolOutput\n",
    "\n",
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.openai import OpenAI as LOpenAI\n",
    "from rag_utils import create_llama_vector_index_rag\n",
    "\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from sec_utils import get_tree_data, get_metadata\n",
    "from llamaindex_data_utils import extract_text_from_pdf\n",
    "\n",
    "import os, re\n",
    "from llama_index.readers.file import PDFReader\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "class SummaryStartEvent(Event):\n",
    "    pass\n",
    "\n",
    "# class InitializationEvent(Event):\n",
    "#     pass\n",
    "\n",
    "class InitializationCleanupEvent(Event):\n",
    "    result: str\n",
    "\n",
    "class RegulationsExtractionEvent(Event):\n",
    "    pass\n",
    "\n",
    "class FormatCorrectionEvent(Event):\n",
    "    result: list\n",
    "\n",
    "class SummarizationEvent(Event):\n",
    "    result: list\n",
    "\n",
    "class SummarizationNumericalValidationEvent(Event):\n",
    "    result: list\n",
    "    summaries: list\n",
    "\n",
    "\n",
    "class RuleSummarizationFlow(Workflow):\n",
    "\n",
    "    @step\n",
    "    async def initialize(self, ctx: Context, ev: StartEvent) -> RegulationsExtractionEvent:\n",
    "        \n",
    "        # Open source work horse model\n",
    "        model_name, ctx_len = \"llama3.1:8b-instruct-q8_0\", 128000\n",
    "        addtion_kwargs = {\"max_new_tokens\": 8000}\n",
    "        ctx.data[\"llm\"] = Ollama(model=model_name, url=\"http://127.0.0.1:11434\", context_window=ctx_len, model_type=\"chat\", is_function_calling_model=True, \n",
    "                                    request_timeout=4000.0, **addtion_kwargs)\n",
    "        \n",
    "        # Expert closed model\n",
    "        ctx.data[\"expert_llm\"] = ctx.data[\"llm\"] # OpenAI(model=\"gpt-4o\",temperature=0.1)\n",
    "        \n",
    "        # embed_model = OllamaEmbedding(model_name, base_url=\"http://localhost:11434\")\n",
    "        \n",
    "        parser = PDFReader(return_full_document=True)\n",
    "        documents_proposal = parser.load_data(\"/workspace/data/compliance/FSI_Rule_proposal.pdf\")\n",
    "\n",
    "        # remove page numbers. they mess up later parsing.\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Create a regex pattern for the current page number\n",
    "            pattern = rf'^{page_number}+ '\n",
    "            # Substitute the pattern with an empty string\n",
    "            new_text = re.sub(pattern, '', documents_proposal[0].text, flags=re.MULTILINE)\n",
    "            # If no substitution was made, break the loop\n",
    "            if new_text == documents_proposal[0].text:\n",
    "                break\n",
    "            # Update the text and increment the page number\n",
    "            documents_proposal[0].text = new_text\n",
    "            page_number += 1\n",
    "\n",
    "        # TODO: Generalize (Very manual splitting of sections)\n",
    "        pattern = r'(?<=\\n)([IVX]+\\..*?)(?=\\n[IVX]+\\.\\s|\\Z)'\n",
    "        sections=[]\n",
    "        sections.append(\"Proposal Information\"+documents_proposal[0].text.split(\"I. Background A. Sta\")[0])\n",
    "        sections += re.findall(pattern, documents_proposal[0].text, re.DOTALL)\n",
    "        section4_5split = sections[4].split(\"V. Paperwork Reduction Act\")\n",
    "        sections[4]=section4_5split[0]\n",
    "        sections = sections[:5]+[\"V. Paperwork Reduction Act\"+section4_5split[-1]]+sections[5:]\n",
    "        for shead in [x.split('\\n')[0].strip() for x in sections]:\n",
    "            print(shead)\n",
    "\n",
    "\n",
    "        documents_proposal = [Document(text=t, \n",
    "                                text_template='{metadata_str}\\n\\n{content}') for t in sections]\n",
    "\n",
    "        # add metadata to the documents_proposal\n",
    "        for i in range(len(documents_proposal)):\n",
    "            documents_proposal[i].metadata[\"section\"] = documents_proposal[i].text.split(\"\\n\")[0].strip()\n",
    "            \n",
    "        # documents_proposal.extend(docs)\n",
    "        \n",
    "        # Global state context\n",
    "        ctx.data[\"num_sections\"] = len(sections)\n",
    "        ctx.data[\"bad_summaries\"] = [x for x in range(len(sections))]\n",
    "        ctx.data[\"chat\"] = SimpleChatEngine.from_defaults(llm=ctx.data[\"expert_llm\"])\n",
    "        ctx.data[\"summarizer\"] = TreeSummarize(llm=ctx.data[\"expert_llm\"], verbose=False)\n",
    "        ctx.data[\"new_rule_documents\"] = documents_proposal\n",
    "        \n",
    "        ctx.data[\"initialized\"] = True\n",
    "        return RegulationsExtractionEvent()\n",
    "    \n",
    "    # @step\n",
    "    # async def check_initialization(self, ctx: Context, ev: InitializationEvent) -> InitializationCleanupEvent:\n",
    "    #     assert ctx.data[\"initialized\"], \"Workflow not initialized.\"\n",
    "    #     for doc in ctx.data[\"new_rule_documents\"]:\n",
    "    #         response = await ctx.data[\"chat\"].chat(f\"\"\"\n",
    "    #         Given text, determine if there are any formatting issues. \n",
    "    #         If it's good, return the original text.\n",
    "    #         If it's bad, rewrite the text so that it the text is clean.\n",
    "    #         Bad formatting contains accidental spaces (e.g. \"Co mputer\", \"tit- for-tat\") and .\n",
    "\n",
    "    #         Here is the text: {doc.text}\n",
    "    #         \"\"\")\n",
    "        \n",
    "    #         print(\"Query Judge response:\", response)\n",
    "    #         if response == \"bad\":\n",
    "    #             # try again\n",
    "    #             return BadQueryEvent(query=ev.query)\n",
    "    #         else:\n",
    "    #             pass\n",
    "\n",
    "    #     return SummaryStartEvent()\n",
    "\n",
    "    @step\n",
    "    async def extract_regulations(self, ctx: Context, ev: RegulationsExtractionEvent) -> FormatCorrectionEvent:\n",
    "        assert ctx.data[\"initialized\"], \"Workflow not initialized.\"\n",
    "        \n",
    "        regs_extraction=[]\n",
    "        for i,sec in enumerate(ctx.data[\"new_rule_documents\"]):\n",
    "\n",
    "            rule_prompt = f\"\"\"\n",
    "            Extract all mentions of any regulatory sections, rules, and acts in the following text.\n",
    "            Compile all extracted items into a single Python List of Strings object.\n",
    "            For example, [\"Item 1\", \"Item 2\", \"Item 3\"].\n",
    "            Only return the Python List of Strings.\n",
    "\n",
    "            Here is the text:\n",
    "            {sec}\n",
    "            \"\"\"\n",
    "            response = ctx.data[\"chat\"].chat(rule_prompt)\n",
    "            regs_extraction.append(response)\n",
    "        \n",
    "        return FormatCorrectionEvent(result=regs_extraction)\n",
    "    \n",
    "    @step\n",
    "    async def correct_format(self, ctx: Context, ev: FormatCorrectionEvent) -> SummarizationEvent:\n",
    "        \n",
    "        regs_list = []\n",
    "        for response in ev.result:\n",
    "            # List checker\n",
    "            rewrite_counter=0\n",
    "            while True:\n",
    "                try: \n",
    "                    \n",
    "                    # extracted_reg = parse_list_from_output_string(response)\n",
    "                    extracted_reg = extract_list_from_string(response)\n",
    "                    break\n",
    "                except:\n",
    "                    print(\"Correcting list format...\")\n",
    "                    response = ctx.data[\"chat\"].chat(f\"\"\"The following text does not contain a valid Python List of Strings? \n",
    "                                                Rewrite the text so that the List is in a valid Python format.\n",
    "                                                For example, [\"Item 1\", \"Item 2\", \"Item 3\"].\\nText:\\n\\n{response}\n",
    "                                            \"\"\")\n",
    "                    rewrite_counter+=1\n",
    "                    print(f\"   Rewrote list {rewrite_counter} times.\")\n",
    "                    if rewrite_counter>5:\n",
    "                        raise ValueError(\"Could not correct list format.\")\n",
    "            regs_list.append(extracted_reg)\n",
    "\n",
    "        return SummarizationEvent(result=regs_list)\n",
    "\n",
    "\n",
    "    @step\n",
    "    async def summarize_sections(self, ctx: Context, ev: SummarizationEvent) -> SummarizationNumericalValidationEvent | StopEvent: #SummarizationValidationEvent:\n",
    "        \n",
    "        if len(ctx.data[\"bad_summaries\"])==0:\n",
    "            return StopEvent(result=ctx.data[\"section_summaries\"])\n",
    "        elif \"section_summaries\" in ctx.data:\n",
    "            print(\"   Bad summaries found. Correcting...\")\n",
    "            section_summaries = ctx.data[\"section_summaries\"]\n",
    "            summary_count = ctx.data[\"bad_summaries\"]\n",
    "            prompt_suffix = f\"\"\"\\nThe first attempt at summarizing this section had numerical copy mistakes. Copy numbers exactly as they appear in the text.\"\"\"\n",
    "        else:\n",
    "            section_summaries = [None]*ctx.data[\"num_sections\"]\n",
    "            summary_count = range(ctx.data[\"num_sections\"])\n",
    "            prompt_suffix = \"\"\n",
    "        \n",
    "        for i in summary_count:\n",
    "            extracted_reg = ev.result[i]\n",
    "            prompt_summary = f\"\"\"\n",
    "            Summarize the content of the following section from a new SEC rule proposal or amendment. Ensure that the summary:\n",
    "\n",
    "            1. Includes every reference to any specific regulatory section, rule, or act mentioned in the text (e.g., 12 U.S.C. 1843(k)(4)(C), 240.13a-15, Unfunded Mandates Reform Act (section 202(a)), Investment Company Act of 1940).\n",
    "            2. Stays strictly within the information presented in the text, without incorporating any outside or prior knowledge.\n",
    "            3. Is detailed, thorough, and specific in its coverage of all key points, definitions and exclusions.\n",
    "            4. Avoids adding any introductory or concluding remarks outside the scope of the summary itself.\n",
    "\n",
    "            Here are some of the regulatory sections, rules, and acts:\n",
    "            {extracted_reg} \n",
    "            \"\"\"\n",
    "            print(f\"   Summarizing section {i+1}/{ctx.data['num_sections']}...\")\n",
    "            parser = SentenceSplitter(chunk_size=500, chunk_overlap=20)\n",
    "            nodes = parser.get_nodes_from_documents(ctx.data[\"new_rule_documents\"], show_progress=True)\n",
    "            response = ctx.data[\"summarizer\"].get_response(prompt_summary+prompt_suffix, [doc.text for doc in nodes])\n",
    "            section_summaries[i] = response\n",
    "        \n",
    "        ctx.data[\"section_summaries\"] = section_summaries\n",
    "        return SummarizationNumericalValidationEvent(result=ev.result, summaries=section_summaries)\n",
    "    \n",
    "    @step\n",
    "    async def validate_summaries(self, ctx: Context, ev: SummarizationNumericalValidationEvent) -> SummarizationEvent:\n",
    "        \n",
    "        bad_summaries = []\n",
    "        sections = [x.text for x in ctx.data[\"new_rule_documents\"]]\n",
    "        for i in ctx.data[\"bad_summaries\"]:\n",
    "            goods=0\n",
    "            bads=0\n",
    "            numbers = re.findall(r'(?<!\\w)(\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?|\\d+\\.\\d+|\\d+)(?!\\w)', ev.summaries[i])\n",
    "            numbers = [x for x in numbers if len(x)>1]\n",
    "            for number in numbers:\n",
    "                if number not in sections[i]:\n",
    "                    bads+=1\n",
    "                    # print(f\"{number} not in section {i}!\")\n",
    "                else:\n",
    "                    goods+=1\n",
    "        \n",
    "            # TODO: this is a manual param. put in ctx.data\n",
    "            if bads>7:\n",
    "                bad_summaries.append(i)\n",
    "            print(f\"   Section {i} Goods: {goods}, Bads: {bads}\")\n",
    "\n",
    "        ctx.data[\"bad_summaries\"] = bad_summaries            \n",
    "        return SummarizationEvent(result=ev.result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_all_possible_flows(RuleSummarizationFlow,filename=\"rule_flows.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = RuleSummarizationFlow(timeout=12000, verbose=True)\n",
    "result = await c.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "\n",
    "class PDF(FPDF):\n",
    "    def header(self):\n",
    "        self.set_font('Arial', 'B', 12)\n",
    "        self.cell(0, 10, 'Summary of Sections', 0, 1, 'C')\n",
    "\n",
    "    def chapter_title(self, title):\n",
    "        self.set_font('Arial', 'B', 14)\n",
    "        self.cell(0, 10, title, 0, 1, 'L')\n",
    "        self.ln(5)\n",
    "\n",
    "    def chapter_body(self, body):\n",
    "        self.set_font('Arial', '', 12)\n",
    "        self.multi_cell(0, 10, body)\n",
    "        self.ln()\n",
    "\n",
    "def create_pdf(summaries, section_headers, output_filename):\n",
    "    pdf = PDF()\n",
    "\n",
    "    pdf.add_page()\n",
    "\n",
    "    for header, summary in zip(section_headers, summaries):\n",
    "        print(header)\n",
    "        pdf.chapter_title(header.replace(\"’\", \"'\"))\n",
    "        # try:\n",
    "        pdf.chapter_body(summary.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"-\").replace(\"–\", \"-\").replace(\"—\", \"-\"))\n",
    "        # except:\n",
    "            # print(summary)\n",
    "        # pdf.chapter_body(summary.)\n",
    "        # pdf.chapter_body(summary)\n",
    "        \n",
    "\n",
    "    pdf.output(output_filename)\n",
    "\n",
    "# Save the summaries to a PDF\n",
    "output_filename = \"/workspace/data/summary_of_sections.pdf\"\n",
    "# section_headers = [x.split('\\n')[0].strip() for x in sections]\n",
    "section_headers = [\"Proposal Information\",\n",
    "\"I. Background A. Statutory Provisions\",\n",
    "\"II. Section -by-Section Analysis\",\n",
    "\"III. Request for Comments\",\n",
    "\"IV. Analysis of the Costs and Benefits Associated with the Proposed Rule\",\n",
    "\"V. Paperwork Reduction Act\",\n",
    "\"VI. Regulatory Flexibility Act\",\n",
    "\"VII. Considerations of th e Impact on the Economy\",\n",
    "\"IX. FinCEN’s Unfunded Mandates Reform Act Determination\"\n",
    "]\n",
    "create_pdf(result, section_headers, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic First Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.react import ReActAgent\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import ToolSelection, ToolOutput\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    Settings,\n",
    "    Document,\n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.core.workflow import (\n",
    "    step,\n",
    "    Context,\n",
    "    Workflow,\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent\n",
    ")\n",
    "from llama_index.core.postprocessor.rankGPT_rerank import RankGPTRerank\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.openai import OpenAI as LOpenAI\n",
    "from rag_utils import create_llama_vector_index_rag\n",
    "\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from sec_utils import get_tree_data, get_metadata\n",
    "from llamaindex_data_utils import extract_text_from_pdf\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from llama_index.readers.file import PDFReader\n",
    "from llamaindex_data_utils import extract_text_from_pdf\n",
    "from llama_index.core import Document\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "access_token = os.getenv('HF_TOKEN')\n",
    "llama_api_key = os.getenv('LLAMA_API_KEY')\n",
    "\n",
    "parser = PDFReader(return_full_document=True)\n",
    "documents_proposal = parser.load_data(\"/workspace/data/compliance/FSI_Rule_proposal.pdf\")\n",
    "# Regular expression to find numbers followed by a space at the beginning of a new line\n",
    "pattern = r'^\\d+ '\n",
    "# Replace page number pattern with an empty string\n",
    "documents_proposal[0].text = re.sub(pattern, '', documents_proposal[0].text, flags=re.MULTILINE)\n",
    "\n",
    "pattern = r'(?<=\\n)([IVX]+\\..*?)(?=\\n[IVX]+\\.\\s|\\Z)'\n",
    "sections=[]\n",
    "sections.append(\"Proposal Information\"+documents_proposal[0].text.split(\"I. Background A. Sta\")[0])\n",
    "sections += re.findall(pattern, documents_proposal[0].text, re.DOTALL)\n",
    "section4_5split = sections[4].split(\"V. Paperwork Reduction Act\")\n",
    "sections[4]=section4_5split[0]\n",
    "sections.append(\"V. Paperwork Reduction Act\"+section4_5split[-1])\n",
    "\n",
    "# pdf_urls = [\"/workspace/data/compliance/FSI_Factsheet.pdf\", \"/workspace/data/compliance/FSI_Press_Release.pdf\", \"/workspace/data/compliance/FSI_Rule_proposal.pdf\"]\n",
    "# descriptions = [\"Factsheet about newly proposed SEC rule.\", \"Press release regarding newly proposed SEC rule.\", \"The full text of the newly proposed SEC rule.\"]\n",
    "# pdf_urls = [\"/workspace/data/compliance/FSI_Rule_proposal.pdf\"]\n",
    "# descriptions = [\"The full text of the newly proposed SEC rule.\"]\n",
    "# documents_proposal = extract_text_from_pdf(pdf_urls, llama_api_key, llamaparse_kwargs={\"split_by_page\":False}, save_json_path=None)\n",
    "documents_proposal = [Document(text=t, \n",
    "                          text_template='{metadata_str}\\n\\n{content}') for t in sections]\n",
    "\n",
    "# add metadata to the documents_proposal\n",
    "for i in range(len(documents_proposal)):\n",
    "    documents_proposal[i].metadata[\"section\"] = documents_proposal[i].text.split(\"\\n\")[0].strip()\n",
    "    # documents_proposal[i].metadata[\"source\"] = pdf_urls[i]\n",
    "    # documents_proposal[i].metadata[\"description\"] = descriptions[i]\n",
    "    \n",
    "# documents_proposal.extend(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define the workflow and all events\n",
    "# class InputEvent(Event):\n",
    "#     input: list[ChatMessage]\n",
    "\n",
    "# class ToolCallEvent(Event):\n",
    "#     tool_calls: list[ToolSelection]\n",
    "\n",
    "# class FunctionOutputEvent(Event):\n",
    "#     output: ToolOutput\n",
    "\n",
    "class InitializeEvent(Event):\n",
    "    pass\n",
    "\n",
    "class QueryEvalEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class BadQueryEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class NaiveRAGEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class HighTopKEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class RerankEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class ComparisonEvent(Event):\n",
    "    query: str\n",
    "    documents: str\n",
    "\n",
    "class ResponseEvent(Event):\n",
    "    query: str\n",
    "    response: str\n",
    "\n",
    "class SummarizeEvent(Event):\n",
    "    query: str\n",
    "    documents: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class ComplianceWorkflow(Workflow):\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def initialize(self, ctx: Context, ev: InitializeEvent) -> QueryEvalEvent:\n",
    "        \n",
    "        # Open source work horse model\n",
    "        model_name, ctx_len = \"llama3.1:latest\", 128000\n",
    "        addtion_kwargs = {\"max_new_tokens\": 4000}\n",
    "        # system_prompt = \"You are an expert at answering questions about rules and regulations regarding Title 17—Commodity and Securities Exchanges: CHAPTER II—SECURITIES AND EXCHANGE COMMISSION. Please provide a summary of the following text, and cite any sections, rules, acts or laws (e.g. § 230.503, § 240.13a-15, Act (15 U.S.C. 781), Investment Company Act of 1940) from context that support the answer. Be detailed in your response.\"\n",
    "        ctx.data[\"llm\"] = Ollama(model=model_name, url=\"http://127.0.0.1:11434\", context_window=ctx_len, model_type=\"chat\", is_function_calling_model=True, \n",
    "                                    request_timeout=4000.0, additional_kwargs=addtion_kwargs) #, system_prompt=system_prompt)\n",
    "        \n",
    "        # Expert closed model\n",
    "        ctx.data[\"expert_llm\"] = ctx.data[\"llm\"] # OpenAI(model=\"gpt-4o\",temperature=0.1)\n",
    "        \n",
    "        # Settings.llm = ctx.data[\"llm\"]\n",
    "        # Embedding models\n",
    "        embed_model_name = \"dunzhang/stella_en_1.5B_v5\" \n",
    "        # finetuned_embed_model_name = \"/workspace/data/compliance/rule_proposal_embedding_model_gpt\"\n",
    "        # proposal_embed_model = HuggingFaceEmbedding(model_name=finetuned_embed_model_name)\n",
    "        sec_embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "        \n",
    "        # ctx.data[\"new_rule_index\"] = create_llama_vector_index_rag(llm=ctx.data[\"llm\"], \n",
    "        #                                                             embed_model=proposal_embed_model, \n",
    "        #                                                             persist_dir=\"/workspace/data/compliance/vector_idx_proposed\",\n",
    "        #                                                             vector_store_kwargs={\"chunk_size\": 300, \"chunk_overlap\": 20})\n",
    "        \n",
    "        ctx.data[\"sec_index\"] = create_llama_vector_index_rag(llm=ctx.data[\"llm\"], \n",
    "                                                                embed_model=sec_embed_model, \n",
    "                                                                persist_dir=\"/workspace/data/compliance/vector_idx_sec\",\n",
    "                                                                vector_store_kwargs={\"chunk_size\": 500, \"chunk_overlap\": 20})\n",
    "        \n",
    "        # we use a chat engine so it remembers previous interactions\n",
    "        ctx.data[\"judge\"] = SimpleChatEngine.from_defaults(llm=ctx.data[\"expert_llm\"])\n",
    "        ctx.data[\"summarizer\"] = TreeSummarize(llm=ctx.data[\"expert_llm\"], verbose=False)\n",
    "        ctx.data[\"new_rule_documents\"] = documents_proposal\n",
    "\n",
    "        ctx.data[\"initialized\"] = True\n",
    "        return QueryEvalEvent()\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def judge_query(self, ctx: Context, ev: StartEvent | QueryEvalEvent ) -> BadQueryEvent | RerankEvent | SummarizeEvent: #NaiveRAGEvent | HighTopKEvent |\n",
    "        # TODO: make initialize its own event\n",
    "        # initialize\n",
    "        if 'initialized' not in ctx.data:\n",
    "            return InitializeEvent()\n",
    "\n",
    "        response = await ctx.data[\"judge\"].chat(f\"\"\"\n",
    "            Given a user query, determine if this is likely to yield precise results from a financial RAG system as-is. \n",
    "            If it's good, return 'good', if it's bad, return 'bad'.\n",
    "            Good queries use a lot of relevant keywords and are detailed. Bad queries are vague or ambiguous.\n",
    "\n",
    "            Here is the query: {ev.query}\n",
    "            \"\"\")\n",
    "        \n",
    "        print(\"Query Judge response:\", response)\n",
    "        if response == \"bad\":\n",
    "            # try again\n",
    "            return BadQueryEvent(query=ev.query)\n",
    "        else:\n",
    "            # send query to all 3 strategies\n",
    "            # self.send_event(NaiveRAGEvent(query=ev.query))\n",
    "            # self.send_event(HighTopKEvent(query=ev.query))\n",
    "            await self.send_event(RerankEvent(query=ev.query))\n",
    "            await self.send_event(SummarizeEvent(query=ev.query, documents=ctx.data[\"new_rule_documents\"]))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def improve_query(self, ctx: Context, ev: BadQueryEvent) -> QueryEvalEvent:\n",
    "        response = await ctx.data[\"llm\"].complete(f\"\"\"\n",
    "            This is a query to a RAG system: {ev.query}\n",
    "\n",
    "            The query is bad because it is too vague. Please provide a more detailed query that includes specific keywords and removes any ambiguity.\n",
    "        \"\"\")\n",
    "        return QueryEvalEvent(query=str(response))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def naive_rag(self, ctx: Context, ev: NaiveRAGEvent) -> ResponseEvent:\n",
    "        index = ctx.data[\"new_rule_index\"]\n",
    "        engine = index.as_query_engine(llm=ctx.data[\"expert_llm\"], similarity_top_k=3)\n",
    "        response = await engine.query(ev.query)\n",
    "        print(\"Naive response:\", response)\n",
    "        return ResponseEvent(query=ev.query, source=\"Naive\", response=str(response))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def high_top_k(self, ctx: Context, ev: HighTopKEvent) -> ResponseEvent:\n",
    "        index = ctx.data[\"new_rule_index\"]\n",
    "        engine = index.as_query_engine(llm=ctx.data[\"expert_llm\"], similarity_top_k=5)\n",
    "        response = await engine.query(ev.query)\n",
    "        print(\"High top k response:\", response)\n",
    "        return ResponseEvent(query=ev.query, source=\"High top k\", response=str(response))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def extract_sec_sections(self, ctx: Context, ev: RerankEvent) -> ResponseEvent:\n",
    "        pass\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def rerank(self, ctx: Context, ev: RerankEvent) -> ResponseEvent:\n",
    "        index = ctx.data[\"new_rule_index\"]\n",
    "        reranker = RankGPTRerank(\n",
    "            top_n=2,\n",
    "            llm=ctx.data[\"expert_llm\"]\n",
    "        )\n",
    "        retriever = index.as_retriever(llm=ctx.data[\"expert_llm\"], similarity_top_k=5)\n",
    "        engine = RetrieverQueryEngine.from_args(\n",
    "            retriever=retriever,\n",
    "            node_postprocessors=[reranker],\n",
    "        )\n",
    "        response = await engine.query(ev.query)\n",
    "        print(\"Reranker response:\", response)\n",
    "        return ResponseEvent(query=ev.query, source=\"Reranker\", response=str(response))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def summarize(self, ctx: Context, ev: SummarizeEvent) -> ResponseEvent:\n",
    "        prompt_summary = f\"\"\"\n",
    "        A long document of a newly proposed SEC rule has been provided. \n",
    "        Please provide a summary of the document with details on key points and sections\n",
    "        that would be relevant to a compliance officer.\n",
    "        \"\"\"\n",
    "\n",
    "        response = await ctx.data[\"summarizer\"].aget_response(prompt_summary, [doc.text for doc in ev.documents])\n",
    "        print(\"Summarizer response:\", response)\n",
    "        return ResponseEvent(query=ev.query, source=\"Summary\", response=str(response))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def judge(self, ctx: Context, ev: ResponseEvent) -> StopEvent:\n",
    "        ready = ctx.collect_events(ev, [ResponseEvent]*2)\n",
    "        if ready is None:\n",
    "            return None\n",
    "\n",
    "        response = ctx.data[\"judge\"].chat(f\"\"\"\n",
    "            A user has provided a query and 3 different strategies have been used\n",
    "            to try to answer the query. Your job is to decide which strategy best\n",
    "            answered the query. The query was: {ev.query}\n",
    "\n",
    "            Response 1 ({ready[0].source}): {ready[0].response}\n",
    "            Response 2 ({ready[1].source}): {ready[1].response}\n",
    "            Response 3 ({ready[2].source}): {ready[2].response}\n",
    "\n",
    "            Please provide the number of the best response (1, 2, or 3).\n",
    "            Just provide the number, with no other text or preamble.\n",
    "        \"\"\")\n",
    "\n",
    "        summary_idx = [i for i, r in enumerate(ready) if r.source == \"Summary\"][0]\n",
    "        best_response = int(str(response))\n",
    "        print(f\"Best response was number {best_response}, which was from {ready[best_response-1].source}\")\n",
    "        return StopEvent(result=[str(ready[best_response-1].response), ready[summary_idx].response])\n",
    "        # return StopEvent(result=str(ready[best_response-1].response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_all_possible_flows(ComplianceWorkflow,filename=\"concierge_flows.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ComplianceWorkflow(timeout=120, verbose=True)\n",
    "result = await c.run(\n",
    "    query=\"What SEC sections does the newly proposed rule affect?\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "file_path=\"/workspace/data/\"\n",
    "transcript_files=[\"GMT20240920-170331_Recording.transcript.vtt\",\n",
    " \"GMT20240924-170654_Recording.transcript.vtt\",\n",
    " \"GMT20240927-170540_Recording.transcript.vtt\",\n",
    " \"GMT20241001-170813_Recording.transcript.vtt\"\n",
    "]\n",
    "summarizer = TreeSummarize(llm=llm, verbose=True)\n",
    "collection = []\n",
    "for file in transcript_files:\n",
    "    with open(os.path.join(file_path, file), \"r\") as f:\n",
    "        transcript = f.read()\n",
    "    clean_transcript = \"\\n\".join([x.split(\"\\n\")[-1] for x in transcript.split(\"\\n\\n\")][1:])\n",
    "    prompt_summary = \"\"\"\n",
    "    Summarize the content of the following meeting transcript.\n",
    "    Be sure to include any action items or decisions made during the meeting.\n",
    "    \"\"\"\n",
    "    response = await summarizer.aget_response(prompt_summary, [clean_transcript])\n",
    "    collection.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [\"Sept 20\",\n",
    "         \"Sept 24\",\n",
    "         \"Sept 27\",\n",
    "         \"Oct 1\"\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date, summary in zip(dates, collection):\n",
    "    print(date)\n",
    "    print(summary)\n",
    "    print()\n",
    "    print(\"-\"*100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "parser = SentenceSplitter(chunk_size=500, chunk_overlap=50)\n",
    "nodes = parser.get_nodes_from_documents(documents_proposal, show_progress=True)\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "summarizer = TreeSummarize(llm=llm, verbose=True)\n",
    "# prompt_summary = \"You are a professional executive of AlphaTrAI. Your job is to summarize this text in great detail from a video transcription. The summary will be distributed to investors and stakeholders, so give a lot of details and examples from the transcription.\"\n",
    "prompt_summary = \"\"\"\n",
    "You are an expert compliance officer who specializes at examining multiple documents for the purpose of extracting novelty, importance, and summarizing findings. Your task is to read the following documents and provide a summary of the key points, sections, and rules that are relevant to a compliance officer. Be detailed in your response.\n",
    "\"\"\"\n",
    "\n",
    "response = await summarizer.aget_response(prompt_summary, [doc.text for doc in nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.function_calling \n",
    "response_information_tool = []\n",
    "llm.chat_with_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define the workflow and all events\n",
    "# class InputEvent(Event):\n",
    "#     input: list[ChatMessage]\n",
    "\n",
    "# class ToolCallEvent(Event):\n",
    "#     tool_calls: list[ToolSelection]\n",
    "\n",
    "# class FunctionOutputEvent(Event):\n",
    "#     output: ToolOutput\n",
    "\n",
    "class InitializeEvent(Event):\n",
    "    pass\n",
    "\n",
    "class JudgeEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class BadQueryEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class NaiveRAGEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class HighTopKEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class RerankEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class ComparisonEvent(Event):\n",
    "    query: str\n",
    "    documents: str\n",
    "\n",
    "class ResponseEvent(Event):\n",
    "    query: str\n",
    "    response: str\n",
    "\n",
    "class SummarizeEvent(Event):\n",
    "    query: str\n",
    "    documents: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplianceWorkflow(Workflow):\n",
    "\n",
    "    def load_or_create_index(self, persist_dir, documents=None):\n",
    "        # Check if the index already exists\n",
    "        if os.path.exists(persist_dir):\n",
    "            print(\"Loading existing index...\")\n",
    "            # Load the index from disk\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "            index = load_index_from_storage(storage_context)\n",
    "        else:\n",
    "            print(\"Creating new index...\")\n",
    "            # Load documents from the specified directory\n",
    "            # TODO: add a check for the documents\n",
    "\n",
    "            # Create a new index from the documents\n",
    "            index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "            # Persist the index to disk\n",
    "            index.storage_context.persist(persist_dir=persist_dir)\n",
    "\n",
    "        return index\n",
    "    \n",
    "    @step(pass_context=True)\n",
    "    async def initialize(self, ctx: Context, ev: InitializeEvent) -> JudgeEvent:\n",
    "        \n",
    "        # Open source work horse model\n",
    "        model_name, ctx_len = \"hermes3:8b\", 128000\n",
    "        addtion_kwargs = {\"max_new_tokens\": 2000}\n",
    "        # system_prompt = \"You are an expert at answering questions about rules and regulations regarding Title 17—Commodity and Securities Exchanges: CHAPTER II—SECURITIES AND EXCHANGE COMMISSION. Please provide a summary of the following text, and cite any sections, rules, acts or laws (e.g. § 230.503, § 240.13a-15, Act (15 U.S.C. 781), Investment Company Act of 1940) from context that support the answer. Be detailed in your response.\"\n",
    "        ctx.data[\"llm\"] = Ollama(model=model_name, url=\"http://127.0.0.1:11434\", context_window=ctx_len, model_type=\"chat\", is_function_calling_model=True, \n",
    "                                    request_timeout=4000.0, additional_kwargs=addtion_kwargs) #, system_prompt=system_prompt)\n",
    "        \n",
    "        # Expert closed model\n",
    "        ctx.data[\"expert_llm\"] = ctx.data[\"llm\"] # OpenAI(model=\"gpt-4o\",temperature=0.1)\n",
    "        \n",
    "        # Settings.llm = ctx.data[\"llm\"]\n",
    "        # Embedding models\n",
    "        embed_model_name = \"dunzhang/stella_en_1.5B_v5\" \n",
    "        finetuned_embed_model_name = \"/workspace/data/compliance/rule_proposal_embedding_model_gpt\"\n",
    "        proposal_embed_model = HuggingFaceEmbedding(model_name=finetuned_embed_model_name)\n",
    "        sec_embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "        \n",
    "        ctx.data[\"new_rule_index\"] = create_llama_vector_index_rag(llm=ctx.data[\"llm\"], \n",
    "                                                                    embed_model=proposal_embed_model, \n",
    "                                                                    persist_dir=\"/workspace/data/compliance/vector_idx_proposed\",\n",
    "                                                                    vector_store_kwargs={\"chunk_size\": 300, \"chunk_overlap\": 20})\n",
    "        \n",
    "        ctx.data[\"sec_index\"] = create_llama_vector_index_rag(llm=ctx.data[\"llm\"], \n",
    "                                                                embed_model=sec_embed_model, \n",
    "                                                                persist_dir=\"/workspace/data/compliance/vector_idx_sec\",\n",
    "                                                                vector_store_kwargs={\"chunk_size\": 500, \"chunk_overlap\": 20})\n",
    "        \n",
    "        # we use a chat engine so it remembers previous interactions\n",
    "        ctx.data[\"judge\"] = SimpleChatEngine.from_defaults(llm=ctx.data[\"expert_llm\"])\n",
    "        ctx.data[\"summarizer\"] = TreeSummarize(llm=ctx.data[\"expert_llm\"], verbose=False)\n",
    "        ctx.data[\"new_rule_documents\"] = documents_proposal\n",
    "\n",
    "        ctx.data[\"initialized\"] = True\n",
    "        return JudgeEvent()\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def judge_query(self, ctx: Context, ev: StartEvent | JudgeEvent ) -> BadQueryEvent | NaiveRAGEvent | HighTopKEvent | RerankEvent | SummarizeEvent:\n",
    "        # TODO: make initialize its own event\n",
    "        # initialize\n",
    "        if 'initialized' not in ctx.data:\n",
    "            return InitializeEvent()\n",
    "\n",
    "        response = ctx.data[\"judge\"].chat(f\"\"\"\n",
    "            Given a user query, determine if this is likely to yield good results from a RAG system as-is. If it's good, return 'good', if it's bad, return 'bad'.\n",
    "            Good queries use a lot of relevant keywords and are detailed. Bad queries are vague or ambiguous.\n",
    "\n",
    "            Here is the query: {ev.query}\n",
    "            \"\"\")\n",
    "        if response == \"bad\":\n",
    "            # try again\n",
    "            return BadQueryEvent(query=ev.query)\n",
    "        else:\n",
    "            # send query to all 3 strategies\n",
    "            self.send_event(NaiveRAGEvent(query=ev.query))\n",
    "            self.send_event(HighTopKEvent(query=ev.query))\n",
    "            self.send_event(RerankEvent(query=ev.query))\n",
    "            self.send_event(SummarizeEvent(query=ev.query, documents=ctx.data[\"new_rule_documents\"]))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def improve_query(self, ctx: Context, ev: BadQueryEvent) -> JudgeEvent:\n",
    "        response = ctx.data[\"llm\"].complete(f\"\"\"\n",
    "            This is a query to a RAG system: {ev.query}\n",
    "\n",
    "            The query is bad because it is too vague. Please provide a more detailed query that includes specific keywords and removes any ambiguity.\n",
    "        \"\"\")\n",
    "        return JudgeEvent(query=str(response))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def naive_rag(self, ctx: Context, ev: NaiveRAGEvent) -> ResponseEvent:\n",
    "        index = ctx.data[\"new_rule_index\"]\n",
    "        engine = index.as_query_engine(llm=ctx.data[\"expert_llm\"], similarity_top_k=3)\n",
    "        response = engine.query(ev.query)\n",
    "        print(\"Naive response:\", response)\n",
    "        return ResponseEvent(query=ev.query, source=\"Naive\", response=str(response))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def high_top_k(self, ctx: Context, ev: HighTopKEvent) -> ResponseEvent:\n",
    "        index = ctx.data[\"new_rule_index\"]\n",
    "        engine = index.as_query_engine(llm=ctx.data[\"expert_llm\"], similarity_top_k=5)\n",
    "        response = engine.query(ev.query)\n",
    "        print(\"High top k response:\", response)\n",
    "        return ResponseEvent(query=ev.query, source=\"High top k\", response=str(response))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def rerank(self, ctx: Context, ev: RerankEvent) -> ResponseEvent:\n",
    "        index = ctx.data[\"new_rule_index\"]\n",
    "        reranker = RankGPTRerank(\n",
    "            top_n=2,\n",
    "            llm=ctx.data[\"expert_llm\"]\n",
    "        )\n",
    "        retriever = index.as_retriever(llm=ctx.data[\"expert_llm\"], similarity_top_k=5)\n",
    "        engine = RetrieverQueryEngine.from_args(\n",
    "            retriever=retriever,\n",
    "            node_postprocessors=[reranker],\n",
    "        )\n",
    "        response = engine.query(ev.query)\n",
    "        print(\"Reranker response:\", response)\n",
    "        return ResponseEvent(query=ev.query, source=\"Reranker\", response=str(response))\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def summarize(self, ctx: Context, ev: SummarizeEvent) -> ResponseEvent:\n",
    "        prompt_summary = f\"\"\"\n",
    "        A long document of a newly proposed SEC rule has been provided. \n",
    "        Please provide a summary of the document with details on key points and sections\n",
    "        that would be relevant to a compliance officer.\n",
    "        \"\"\"\n",
    "\n",
    "        response = await ctx.data[\"summarizer\"].aget_response(prompt_summary, [doc.text for doc in ev.documents])\n",
    "        print(\"Summarizer response:\", response)\n",
    "        return ResponseEvent(query=ev.query, source=\"Summary\", response=str(response))\n",
    "\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def judge(self, ctx: Context, ev: ResponseEvent) -> StopEvent:\n",
    "        ready = ctx.collect_events(ev, [ResponseEvent]*4)\n",
    "        if ready is None:\n",
    "            return None\n",
    "\n",
    "        response = ctx.data[\"judge\"].chat(f\"\"\"\n",
    "            A user has provided a query and 3 different strategies have been used\n",
    "            to try to answer the query. Your job is to decide which strategy best\n",
    "            answered the query. The query was: {ev.query}\n",
    "\n",
    "            Response 1 ({ready[0].source}): {ready[0].response}\n",
    "            Response 2 ({ready[1].source}): {ready[1].response}\n",
    "            Response 3 ({ready[2].source}): {ready[2].response}\n",
    "\n",
    "            Please provide the number of the best response (1, 2, or 3).\n",
    "            Just provide the number, with no other text or preamble.\n",
    "        \"\"\")\n",
    "\n",
    "        summary_idx = [i for i, r in enumerate(ready) if r.source == \"Summary\"][0]\n",
    "        best_response = int(str(response))\n",
    "        print(f\"Best response was number {best_response}, which was from {ready[best_response-1].source}\")\n",
    "        return StopEvent(result=[str(ready[best_response-1].response), ready[-1].response])\n",
    "        # return StopEvent(result=str(ready[best_response-1].response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph_engine_tools = QueryEngineTool(\n",
    "            query_engine=graph_index,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"sec_title_17_chapter_ii_tool\",\n",
    "                description=(\n",
    "                    \"Contains all the current sections, rules, and relationships of SEC Title 17 Chapter II.\"\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "query_engine_tools = QueryEngineTool(\n",
    "            query_engine=query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"new_rule_proposal_tool\",\n",
    "                description=(\n",
    "                    \"Contains all the information about the newly proposed SEC rule.\"\n",
    "                ),\n",
    "            ),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "\n",
    "from llama_index.core.llms.function_calling import FunctionCallingLLM\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.tools.types import BaseTool\n",
    "from llama_index.core.workflow import Workflow, StartEvent, StopEvent, step\n",
    "\n",
    "\n",
    "class FuncationCallingAgent(Workflow):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args: Any,\n",
    "        llm: FunctionCallingLLM | None = None,\n",
    "        tools: List[BaseTool] | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tools = tools or []\n",
    "\n",
    "        self.llm = llm\n",
    "        assert self.llm.metadata.is_function_calling_model\n",
    "\n",
    "        self.memory = ChatMemoryBuffer.from_defaults(llm=llm)\n",
    "        self.sources = []\n",
    "\n",
    "    @step\n",
    "    async def prepare_chat_history(self, ev: StartEvent) -> InputEvent:\n",
    "        # clear sources\n",
    "        self.sources = []\n",
    "\n",
    "        # get user input\n",
    "        user_input = ev.input\n",
    "        user_msg = ChatMessage(role=\"user\", content=user_input)\n",
    "        self.memory.put(user_msg)\n",
    "\n",
    "        # get chat history\n",
    "        chat_history = self.memory.get()\n",
    "        return InputEvent(input=chat_history)\n",
    "\n",
    "    @step\n",
    "    async def handle_llm_input(\n",
    "        self, ev: InputEvent\n",
    "    ) -> ToolCallEvent | StopEvent:\n",
    "        chat_history = ev.input\n",
    "        '''\n",
    "        Takes in the chat history, and uses tools to generate a response.\n",
    "        '''\n",
    "        response = await self.llm.achat_with_tools(\n",
    "            self.tools, chat_history=chat_history\n",
    "        )\n",
    "        self.memory.put(response.message)\n",
    "\n",
    "        tool_calls = self.llm.get_tool_calls_from_response(\n",
    "            response, error_on_no_tool_call=False\n",
    "        )\n",
    "\n",
    "        if not tool_calls:\n",
    "            return StopEvent(\n",
    "                result={\"response\": response, \"sources\": [*self.sources]}\n",
    "            )\n",
    "        else:\n",
    "            return ToolCallEvent(tool_calls=tool_calls)\n",
    "\n",
    "    @step\n",
    "    async def handle_tool_calls(self, ev: ToolCallEvent) -> InputEvent:\n",
    "        tool_calls = ev.tool_calls\n",
    "        tools_by_name = {tool.metadata.get_name(): tool for tool in self.tools}\n",
    "\n",
    "        tool_msgs = []\n",
    "\n",
    "        # call tools -- safely!\n",
    "        for tool_call in tool_calls:\n",
    "            tool = tools_by_name.get(tool_call.tool_name)\n",
    "            additional_kwargs = {\n",
    "                \"tool_call_id\": tool_call.tool_id,\n",
    "                \"name\": tool.metadata.get_name(),\n",
    "            }\n",
    "            if not tool:\n",
    "                tool_msgs.append(\n",
    "                    ChatMessage(\n",
    "                        role=\"tool\",\n",
    "                        content=f\"Tool {tool_call.tool_name} does not exist\",\n",
    "                        additional_kwargs=additional_kwargs,\n",
    "                    )\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                tool_output = tool(**tool_call.tool_kwargs)\n",
    "                self.sources.append(tool_output)\n",
    "                tool_msgs.append(\n",
    "                    ChatMessage(\n",
    "                        role=\"tool\",\n",
    "                        content=tool_output.content,\n",
    "                        additional_kwargs=additional_kwargs,\n",
    "                    )\n",
    "                )\n",
    "            except Exception as e:\n",
    "                tool_msgs.append(\n",
    "                    ChatMessage(\n",
    "                        role=\"tool\",\n",
    "                        content=f\"Encountered error in tool call: {e}\",\n",
    "                        additional_kwargs=additional_kwargs,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        for msg in tool_msgs:\n",
    "            self.memory.put(msg)\n",
    "\n",
    "        chat_history = self.memory.get()\n",
    "        return InputEvent(input=chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm.is_function_calling_model = True\n",
    "\n",
    "agent = FuncationCallingAgent(\n",
    "    llm=llm, tools=all_tools, timeout=120, verbose=True\n",
    ")\n",
    "\n",
    "ret = await agent.run(input=\"What is a summary of the proposed rule, and what SEC rules does it change?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ret[\"response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "from llama_index.utils.workflow import draw_most_recent_execution\n",
    "from IPython.display import display, HTML\n",
    "draw_all_possible_flows(FuncationCallingAgent, \"first_func_agent.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='/workspace/repos/agentic-ai/first_func_agent.html', width=700, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Read the HTML file content\n",
    "with open('/workspace/repos/agentic-ai/first_func_agent.html', 'r') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Render the HTML content in a Jupyter cell\n",
    "display(HTML(html_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import asyncio\n",
    "from pydantic_settings import BaseSettings\n",
    "import signal\n",
    "from llama_deploy.deploy.deploy import (\n",
    "                        _get_message_queue_client,\n",
    "                        _deploy_local_message_queue,\n",
    "                        _get_shutdown_handler\n",
    "                    )\n",
    "\n",
    "from llama_deploy import (\n",
    "    deploy_core,\n",
    "    ControlPlaneConfig,\n",
    "    SimpleMessageQueueConfig,\n",
    "    SimpleOrchestratorConfig,\n",
    "    ControlPlaneServer,\n",
    "    SimpleOrchestrator,\n",
    "    LlamaDeployClient\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_deploy import (\n",
    "    deploy_core,\n",
    "    ControlPlaneConfig,\n",
    "    SimpleMessageQueueConfig,\n",
    ")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    await deploy_core(\n",
    "        control_plane_config=ControlPlaneConfig(port=8002),\n",
    "        message_queue_config=SimpleMessageQueueConfig(port=8003),\n",
    "    )\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import asyncio\n",
    "\n",
    "#     asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_deploy import (\n",
    "    deploy_workflow,\n",
    "    WorkflowServiceConfig,\n",
    "    ControlPlaneConfig,\n",
    "    SimpleMessageQueueConfig,\n",
    ")\n",
    "from llama_index.core.workflow import Workflow, StartEvent, StopEvent, step\n",
    "\n",
    "\n",
    "# create a dummy workflow\n",
    "class MyWorkflow(Workflow):\n",
    "    @step()\n",
    "    async def run_step(self, ev: StartEvent) -> StopEvent:\n",
    "        # Your workflow logic here\n",
    "        arg1 = str(ev.get(\"arg1\", \"\"))\n",
    "        result = arg1 + \"_result\"\n",
    "        return StopEvent(result=result)\n",
    "\n",
    "\n",
    "async def main():\n",
    "    await deploy_workflow(\n",
    "        workflow=MyWorkflow(),\n",
    "        workflow_config=WorkflowServiceConfig(\n",
    "            host=\"127.0.0.1\", port=8004, service_name=\"my_workflow\"\n",
    "        ),\n",
    "        control_plane_config=ControlPlaneConfig(),\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context,\n",
    ")\n",
    "import random\n",
    "from llama_index.core.workflow import draw_all_possible_flows\n",
    "from llama_index.utils.workflow import draw_most_recent_execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import asyncio\n",
    "from pydantic_settings import BaseSettings\n",
    "import signal\n",
    "from llama_deploy.deploy.deploy import (\n",
    "                        _get_message_queue_client,\n",
    "                        _deploy_local_message_queue,\n",
    "                        _get_shutdown_handler\n",
    "                    )\n",
    "\n",
    "from llama_deploy import (\n",
    "    deploy_core,\n",
    "    ControlPlaneConfig,\n",
    "    SimpleMessageQueueConfig,\n",
    "    SimpleOrchestratorConfig,\n",
    "    ControlPlaneServer,\n",
    "    SimpleOrchestrator,\n",
    "    LlamaDeployClient\n",
    ")\n",
    "\n",
    "\n",
    "async def deploy_core(\n",
    "    control_plane_config: ControlPlaneConfig,\n",
    "    message_queue_config: BaseSettings,\n",
    "    orchestrator_config: Optional[SimpleOrchestratorConfig] = None,\n",
    ") -> None:\n",
    "    orchestrator_config = orchestrator_config or SimpleOrchestratorConfig()\n",
    "\n",
    "    message_queue_client = _get_message_queue_client(message_queue_config)\n",
    "\n",
    "    control_plane = ControlPlaneServer(\n",
    "        message_queue_client,\n",
    "        SimpleOrchestrator(**orchestrator_config.model_dump()),\n",
    "        **control_plane_config.model_dump(),\n",
    "    )\n",
    "\n",
    "    message_queue_task = None\n",
    "    if isinstance(message_queue_config, SimpleMessageQueueConfig):\n",
    "        message_queue_task = _deploy_local_message_queue(message_queue_config)\n",
    "\n",
    "    control_plane_task = asyncio.create_task(control_plane.launch_server())\n",
    "\n",
    "    # let services spin up\n",
    "    await asyncio.sleep(1)\n",
    "\n",
    "    # register the control plane as a consumer\n",
    "    control_plane_consumer_fn = await control_plane.register_to_message_queue()\n",
    "\n",
    "    consumer_task = asyncio.create_task(control_plane_consumer_fn())\n",
    "\n",
    "    # let things sync up\n",
    "    await asyncio.sleep(1)\n",
    "\n",
    "    # let things run\n",
    "    if message_queue_task:\n",
    "        all_tasks = [control_plane_task, consumer_task, message_queue_task]\n",
    "    else:\n",
    "        all_tasks = [control_plane_task, consumer_task]\n",
    "\n",
    "    shutdown_handler = _get_shutdown_handler(all_tasks)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    while loop.is_running():\n",
    "        await asyncio.sleep(0.1)\n",
    "        signal.signal(signal.SIGINT, shutdown_handler)\n",
    "\n",
    "        for task in all_tasks:\n",
    "            if task.done() and task.exception():  # type: ignore\n",
    "                raise task.exception()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from llama_index.core.workflow import Workflow\n",
    "from llama_deploy import (\n",
    "    WorkflowServiceConfig,\n",
    "    WorkflowService,\n",
    ")\n",
    "\n",
    "from llama_deploy.deploy.deploy import (\n",
    "                        _get_message_queue_config,\n",
    "                    )\n",
    "\n",
    "async def deploy_workflow(\n",
    "    workflow: Workflow,\n",
    "    workflow_config: WorkflowServiceConfig,\n",
    "    control_plane_config: ControlPlaneConfig,\n",
    ") -> None:\n",
    "    control_plane_url = control_plane_config.url\n",
    "\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.get(f\"{control_plane_url}/queue_config\")\n",
    "        queue_config_dict = response.json()\n",
    "\n",
    "    message_queue_config = _get_message_queue_config(queue_config_dict)\n",
    "    message_queue_client = _get_message_queue_client(message_queue_config)\n",
    "\n",
    "    service = WorkflowService(\n",
    "        workflow=workflow,\n",
    "        message_queue=message_queue_client,\n",
    "        **workflow_config.model_dump(),\n",
    "    )\n",
    "\n",
    "    service_task = asyncio.create_task(service.launch_server())\n",
    "\n",
    "    # let service spin up\n",
    "    await asyncio.sleep(1)\n",
    "\n",
    "    # register to message queue\n",
    "    consumer_fn = await service.register_to_message_queue()\n",
    "\n",
    "    # register to control plane\n",
    "    control_plane_url = (\n",
    "        f\"http://{control_plane_config.host}:{control_plane_config.port}\"\n",
    "    )\n",
    "    await service.register_to_control_plane(control_plane_url)\n",
    "\n",
    "    # create consumer task\n",
    "    consumer_task = asyncio.create_task(consumer_fn())\n",
    "\n",
    "    # let things sync up\n",
    "    await asyncio.sleep(1)\n",
    "\n",
    "    all_tasks = [consumer_task, service_task]\n",
    "\n",
    "    shutdown_handler = _get_shutdown_handler(all_tasks)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    while loop.is_running():\n",
    "        await asyncio.sleep(0.1)\n",
    "        signal.signal(signal.SIGINT, shutdown_handler)\n",
    "\n",
    "        for task in all_tasks:\n",
    "            if task.done() and task.exception():  # type: ignore\n",
    "                raise task.exception()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents import (\n",
    "    AgentService,\n",
    "    ToolService,\n",
    "    MetaServiceTool,\n",
    "    ControlPlaneServer,\n",
    "    SimpleMessageQueue,\n",
    "    AgentOrchestrator,\n",
    ")\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import ReActAgentWorker, ReActAgent\n",
    "\n",
    "\n",
    "\n",
    "# create our multi-agent framework components\n",
    "message_queue = SimpleMessageQueue()\n",
    "control_plane = ControlPlaneServer(\n",
    "    message_queue=message_queue,\n",
    "    orchestrator=AgentOrchestrator(llm=llm),\n",
    ")\n",
    "\n",
    "# define Tool Service\n",
    "tool_service = ToolService(\n",
    "    message_queue=message_queue,\n",
    "    tools=all_tools,\n",
    "    running=True,\n",
    "    step_interval=0.5,\n",
    ")\n",
    "\n",
    "# define meta-tools here\n",
    "meta_tools = [\n",
    "    await MetaServiceTool.from_tool_service(\n",
    "        t.metadata.name,\n",
    "        message_queue=message_queue,\n",
    "        tool_service=tool_service,\n",
    "    )\n",
    "    for t in all_tools\n",
    "]\n",
    "\n",
    "\n",
    "# define Agent and agent service\n",
    "# worker1 = FunctionCallingAgentWorker.from_tools(\n",
    "worker1 = ReActAgentWorker.from_tools(\n",
    "    meta_tools,\n",
    "    llm=llm,\n",
    ")\n",
    "agent1 = worker1.as_agent()\n",
    "agent_server_1 = AgentService(\n",
    "    agent=agent1,\n",
    "    message_queue=message_queue,\n",
    "    description=\"Agent that answers questions based on the newly proposed SEC rule.\",\n",
    "    service_name=\"rule_proposal_agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent1.chat('When are written comments on this notice of joint proposed rulemaking need to be submitted?')\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions\n",
    "# Name of rule \n",
    "# Is it a proposed rule or a final rule\n",
    "# Issue date and Federal Register date\n",
    "# What agency(ies) is the Rule coming from\n",
    "# If a proposed rule, when are public comments due by and where should they be sent (this info is in the Rule document under Dates and Addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "summarizer = TreeSummarize(llm=llm, verbose=True)\n",
    "# prompt_summary = \"You are a professional executive of AlphaTrAI. Your job is to summarize this text in great detail from a video transcription. The summary will be distributed to investors and stakeholders, so give a lot of details and examples from the transcription.\"\n",
    "prompt_summary = f\"\"\"You are a professional executive at AlphaTrAI. Your job is to summarize the text from a video transcription. The summary will be a memo distributed to investors and stakeholders. Be sure it the memo has the following items:\n",
    "1. Extract all the names of new hires and their position, and/or new advisors mentioned in the transcription.\n",
    "2. Create a section to mention the personnel new to AlphaTrAI.\n",
    "3. Include other highlights and progress made by AlphaTrAI.\n",
    "4. Ensure the memo and ensure it is factual, optimistic, and any values mention come directly from the text. \n",
    "\n",
    "The transcription is as follows:\\n{full_doc}\"\"\"\n",
    "\n",
    "response = await summarizer.aget_response(prompt_summary, [doc.text for doc in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM direct summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_summary = f\"\"\"You are a professional executive at AlphaTrAI. Your job is to summarize the text from a video transcription. The summary will be a memo distributed to investors and stakeholders. Be sure it the memo has the following items:\n",
    "1. Extract all the names of new hires and their position, and/or new advisors mentioned in the transcription.\n",
    "2. Include other highlights and progress made by AlphaTrAI.\n",
    "3. Ensure the memo is professional, fluid, factual, and optimistic. \n",
    "\n",
    "The transcription is as follows:\\n{full_doc}\"\"\"\n",
    "\n",
    "response = llm.complete(prompt_summary, max_tokens=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install llama-index-embeddings-huggingface llama-index-vector-stores-neo4jvector llama-index-graph-stores-neo4j\n",
    "!apt install dialog apt-utils -y (done above)\n",
    "!wget -O - https://debian.neo4j.com/neotechnology.gpg.key | gpg --dearmor -o /etc/apt/keyrings/neotechnology.gpg\n",
    "!echo 'deb [signed-by=/etc/apt/keyrings/neotechnology.gpg] https://debian.neo4j.com stable latest' | tee -a /etc/apt/sources.list.d/neo4j.list\n",
    "!apt list -a neo4j\n",
    "!add-apt-repository universe -y\n",
    "!apt install neo4j=1:5.22.0 -y\n",
    "!echo \"neo4j-enterprise neo4j/question select I ACCEPT\" | debconf-set-selections\n",
    "!echo \"neo4j-enterprise neo4j/license note\" | debconf-set-selections\n",
    "!apt install openjdk-17-jre -y\n",
    "!cd /var/lib/neo4j/plugins/ && wget https://github.com/neo4j/apoc/releases/download/5.22.0/apoc-5.22.0-core.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_neo4j_password('bewaretheneo')\n",
    "add_lines_to_conf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from llama_index.core.agent import ReActAgent\n",
    "# from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from rag_utils import create_neo4j_graph_store, create_neo4j_graphrag, neo4j_query, set_neo4j_password, add_lines_to_conf\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm.is_function_calling_model = True\n",
    "\n",
    "embed_model_name = \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\"\n",
    "print(\"loading embed model...\")\n",
    "embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 300\n",
    "Settings.chunk_overlap = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor\n",
    "\n",
    "entities = Literal[\"PEOPLE\", \n",
    "                   \"PLACE\"\n",
    "]\n",
    "\n",
    "relations = Literal[\n",
    "    \"ROLE\",\n",
    "    \"COMPANY\"\n",
    "]\n",
    "\n",
    "validation_schema = {\n",
    "    \"People\": [\"ROLE\"],\n",
    "    \"Place\": [\"COMPANY\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.chunk_size = 300\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "kg_extractor = SchemaLLMPathExtractor(\n",
    "    llm=llm,\n",
    "    possible_entities=entities,\n",
    "    possible_relations=relations,\n",
    "    kg_validation_schema=validation_schema,\n",
    "    strict=True,  # if false, will allow triples outside of the schema\n",
    "    num_workers=4,\n",
    "    max_triplets_per_chunk=10,\n",
    ")\n",
    "\n",
    "graph_store = create_neo4j_graph_store(neo_url=\"bolt://localhost:7687\", \n",
    "                                       password=os.getenv(\"NEO4J_PWD\"), \n",
    "                                       config={\"connection_timeout\": 240, \"connection_acquisition_timeout\": 240, \"max_connection_pool_size\": 1000})\n",
    "neo4j_query(graph_store, query=\"\"\"MATCH (n) DETACH DELETE n\"\"\")\n",
    "\n",
    "\n",
    "graph_index = create_neo4j_graphrag(documents, llm, embed_model, kg_extractor, graph_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "query_engine_tools = QueryEngineTool(\n",
    "            query_engine=graph_index,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"graph_tool\",\n",
    "                description=(\n",
    "                    \"Useful for finding people names and roles, and the company they work for.\"\n",
    "                ),\n",
    "            ),\n",
    "        ),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install llama-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents import (\n",
    "    AgentService,\n",
    "    ToolService,\n",
    "    MetaServiceTool,\n",
    "    ControlPlaneServer,\n",
    "    SimpleMessageQueue,\n",
    "    AgentOrchestrator,\n",
    ")\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker, ReActAgentWorker, ReActAgent, LATSAgentWorker\n",
    "\n",
    "\n",
    "\n",
    "# create our multi-agent framework components\n",
    "message_queue = SimpleMessageQueue()\n",
    "control_plane = ControlPlaneServer(\n",
    "    message_queue=message_queue,\n",
    "    orchestrator=AgentOrchestrator(llm=llm),\n",
    ")\n",
    "\n",
    "# define Tool Service\n",
    "tool_service = ToolService(\n",
    "    message_queue=message_queue,\n",
    "    tools=[query_engine_tools],#, adding_tool],\n",
    "    running=True,\n",
    "    step_interval=0.5,\n",
    ")\n",
    "\n",
    "# define meta-tools here\n",
    "meta_tools = [\n",
    "    await MetaServiceTool.from_tool_service(\n",
    "        t.metadata.name,\n",
    "        message_queue=message_queue,\n",
    "        tool_service=tool_service,\n",
    "    )\n",
    "    for t in [query_engine_tools]#, adding_tool]\n",
    "]\n",
    "\n",
    "\n",
    "# define Agent and agent service\n",
    "# worker1 = FunctionCallingAgentWorker.from_tools(\n",
    "worker1 = ReActAgentWorker.from_tools(\n",
    "    meta_tools,\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "worker2 = LATSAgentWorker.from_tools(\n",
    "    meta_tools,\n",
    "    llm=llm,\n",
    "    num_expansions=2,\n",
    "    max_rollouts=3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent1 = worker1.as_agent()\n",
    "agent_server_1 = AgentService(\n",
    "    agent=agent1,\n",
    "    message_queue=message_queue,\n",
    "    description=\"Summarize a transcription as a memo for investors and stakeholders.\",\n",
    "    service_name=\"summarize_transcription\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
