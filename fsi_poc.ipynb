{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short list of objectives\n",
    "\n",
    "* Import pdf of Proposed Rule XXXXXXX\n",
    "* Query rule for baseline responses \n",
    "* Fine-tune embedding model, recheck responses\n",
    "* Summarize rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up ollama\n",
    "!apt-get update && apt-get install tmux vim -y\n",
    "!pip3 install llama-index llama-parse llama_deploy llama-index-llms-huggingface llama-index-embeddings-huggingface llama-index-llms-ollama llama-index-embeddings-ollama llama-index-vector-stores-neo4jvector llama-index-graph-stores-neo4j llama-index-finetuning llama-index-utils-workflow \n",
    "!pip3 install sentencepiece protobuf evaluate rouge_score absl-py tensorboardX bitsandbytes peft accelerate python-dotenv graspologic\n",
    "!pip3 install flash-attn --no-build-isolation\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Neo4j\n",
    "!apt install dialog apt-utils -y \n",
    "!wget -O - https://debian.neo4j.com/neotechnology.gpg.key | gpg --dearmor -o /etc/apt/keyrings/neotechnology.gpg\n",
    "!echo 'deb [signed-by=/etc/apt/keyrings/neotechnology.gpg] https://debian.neo4j.com stable latest' | tee -a /etc/apt/sources.list.d/neo4j.list\n",
    "!apt list -a neo4j\n",
    "!add-apt-repository universe -y\n",
    "!apt install neo4j=1:5.23.0 -y\n",
    "!echo \"neo4j-enterprise neo4j/question select I ACCEPT\" | debconf-set-selections\n",
    "!echo \"neo4j-enterprise neo4j/license note\" | debconf-set-selections\n",
    "!apt install openjdk-17-jre -y\n",
    "!cd /var/lib/neo4j/plugins/ && wget https://github.com/neo4j/apoc/releases/download/5.23.0/apoc-5.23.0-core.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.openai import OpenAI as LOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "# model_name, ctx_len = \"gpt-4o\", 128000\n",
    "# model_name, ctx_len = \"llama3.1:latest\", 128000\n",
    "model_name, ctx_len = \"hermes3:8b\", 128000\n",
    "# model_name, ctx_len = \"gemma2:27b\", 8192\n",
    "\n",
    "\n",
    "if model_name == \"gpt-4o\":\n",
    "    openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "    \n",
    "    print(f\"Using OpenAI {model_name}...\")\n",
    "    llm = LOpenAI(model=model_name, max_tokens=4000)\n",
    "else:\n",
    "    try: \n",
    "        print(\"Pulling Ollama model...\")\n",
    "        sub_out = subprocess.run(['ollama', 'pull', model_name], capture_output=True, text=True)\n",
    "    except Exception as e: \n",
    "        print(f\"Error pulling model: Is the Ollama server running?\\n{e}\")\n",
    "    \n",
    "    addtion_kwargs = {\"max_new_tokens\": 2000}\n",
    "    # system_prompt = \"You are an expert at answering questions about rules and regulations regarding Title 17—Commodity and Securities Exchanges: CHAPTER II—SECURITIES AND EXCHANGE COMMISSION. Please provide a summary of the following text, and cite any sections, rules, acts or laws (e.g. § 230.503, § 240.13a-15, Act (15 U.S.C. 781), Investment Company Act of 1940) from context that support the answer. Be detailed in your response.\"\n",
    "    llm = Ollama(model=model_name, url=\"http://127.0.0.1:11434\", context_window=ctx_len, model_type=\"chat\", is_function_calling_model=False, \n",
    "                 request_timeout=4000.0, additional_kwargs=addtion_kwargs) #, system_prompt=system_prompt)\n",
    "    print(llm.metadata)\n",
    "\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEC rules and regulations\n",
    "!cd /workspace/data && curl -X GET \"https://www.ecfr.gov/api/versioner/v1/full/2024-07-23/title-17.xml?chapter=II\" -H \"accept: application/xml\" > title-17.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from sec_utils import get_tree_data\n",
    "\n",
    "# Path to your XML file\n",
    "xml_file_path = '/workspace/data/title-17.xml'\n",
    "\n",
    "# Parse the XML file\n",
    "tree = ET.parse(xml_file_path)\n",
    "\n",
    "# Get the root element of the XML document\n",
    "root = tree.getroot()\n",
    "\n",
    "sec_data = get_tree_data(root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sec_utils import get_metadata\n",
    "from llama_index.core import Document\n",
    "\n",
    "documents = [Document(text=t, \n",
    "                          text_template='{metadata_str}\\n\\n{content}',\n",
    "                          metadata=get_metadata(m, t, metadata={\"section\":None, \"description\":None, \"mentioned_sections\":None})) \\\n",
    "                            for m,t in sec_data.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamaindex_data_utils import extract_text_from_pdf\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "access_token = os.getenv('HF_TOKEN')\n",
    "llama_api_key = os.getenv('LLAMA_API_KEY')\n",
    "\n",
    "pdf_urls = [\"/workspace/data/compliance/FSI_Factsheet.pdf\", \"/workspace/data/compliance/FSI_Press_Release.pdf\", \"/workspace/data/compliance/FSI_Rule_proposal.pdf\"]\n",
    "descriptions = [\"Factsheet about newly proposed SEC rule.\", \"Press release regarding newly proposed SEC rule.\", \"The full text of the newly proposed SEC rule.\"]\n",
    "documents_proposal = extract_text_from_pdf(pdf_urls, llama_api_key, llamaparse_kwargs={\"split_by_page\":False}, save_json_path=None)\n",
    "# add metadata to the documents_proposal\n",
    "for i in range(len(documents_proposal)):\n",
    "    documents_proposal[i].metadata[\"source\"] = pdf_urls[i]\n",
    "    documents_proposal[i].metadata[\"description\"] = descriptions[i]\n",
    "    \n",
    "# documents_proposal.extend(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Distillation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "parser = SentenceSplitter(chunk_size=300, chunk_overlap=50)\n",
    "nodes = parser.get_nodes_from_documents(documents_proposal, show_progress=True)\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = 'Context information is below.\\n\\n---------------------\\n{context_str}\\n---------------------\\n\\nGiven the context information and no prior knowledge, generate {num_questions_per_chunk} questions based on the below query.\\n\\nYou are a FINRA certified Compliance Specialist that writes exams for firm compliance professionals. Your task is to write precise questions for an upcoming Compliance Officer certification examination. The questions should be diverse across the context with no multiple choice. Restrict the questions to the context information provided.\"\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.finetuning import generate_qa_embedding_pairs\n",
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import random\n",
    "\n",
    "output_path = \"/workspace/data/train_dataset_proposal.json\"\n",
    "if not os.path.exists(output_path):\n",
    "    rand_index = random.sample(range(len(nodes)), len(nodes))\n",
    "    train_perc=1.0\n",
    "    train_size = int(len(rand_index)*train_perc)\n",
    "\n",
    "    train_dataset = generate_qa_embedding_pairs(\n",
    "        qa_generate_prompt_tmpl = system_prompt,\n",
    "        num_questions_per_chunk=5,\n",
    "        save_every=20,\n",
    "        output_path=output_path,\n",
    "        llm=llm, \n",
    "        nodes=[nodes[x] for x in rand_index[:train_size]],\n",
    "        verbose=False\n",
    "    )\n",
    "    train_dataset.save_json(output_path)\n",
    "\n",
    "    # val_dataset = generate_qa_embedding_pairs(\n",
    "    #     qa_generate_prompt_tmpl = system_prompt,\n",
    "    #     num_questions_per_chunk=5,\n",
    "    #     save_every=500,\n",
    "    #     output_path=\"/workspace/data/val_dataset.json\",\n",
    "    #     llm=llm, \n",
    "    #     nodes=[nodes[x] for x in rand_index[train_size:]],\n",
    "    #     verbose=False\n",
    "    # )\n",
    "    # # assert len(val_dataset.relevant_docs) == len(val_dataset.queries)\n",
    "    # assert (np.unique(list(val_dataset.relevant_docs.values()), return_counts=True)[1]==5).all()\n",
    "    # val_dataset.save_json(\"/workspace/data/val_dataset.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#####################\n",
    "# in llama_index/embeddings/huggingface/base need to add \"show_progress_bar=False\" to 204, 213\n",
    "#####################\n",
    "\n",
    "import os, json\n",
    "\n",
    "from llama_index.finetuning import EmbeddingAdapterFinetuneEngine, SentenceTransformersFinetuneEngine\n",
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "\n",
    "train_path = \"/workspace/data/train_dataset_proposal.json\"\n",
    "val_path = None #\"/workspace/data/val_dataset.json\"\n",
    "if os.path.exists(train_path):\n",
    "    train_dataset = EmbeddingQAFinetuneDataset.from_json(train_path)\n",
    "if val_path is not None and os.path.exists(val_path):\n",
    "    val_dataset = EmbeddingQAFinetuneDataset.from_json(val_path)\n",
    "else:\n",
    "    val_dataset = None\n",
    "\n",
    "embed_model_name = \"dunzhang/stella_en_1.5B_v5\" #7b params\n",
    "print(\"loading embed model...\")\n",
    "\n",
    "finetune_engine = SentenceTransformersFinetuneEngine(\n",
    "    train_dataset,\n",
    "    embed_model_name,\n",
    "    batch_size=6,\n",
    "    model_output_path=\"/workspace/data/rule_proposal_embedding_model\",\n",
    "    val_dataset=val_dataset,\n",
    "    epochs=4,\n",
    "    show_progress_bar=True,\n",
    "    # can optionally pass along any parameters that go into `train_model`\n",
    "    # optimizer_class=torch.optim.SGD,\n",
    "    # optimizer_params={\"lr\": 0.0001, \"weight_decay\": 0.01}\n",
    ")\n",
    "\n",
    "\n",
    "# finetune_engine = EmbeddingAdapterFinetuneEngine(\n",
    "#     train_dataset,\n",
    "#     base_embed_model,\n",
    "#     batch_size=10,\n",
    "#     model_output_path=\"/workspace/data/adapter_model\",\n",
    "#     # val_dataset=val_dataset,\n",
    "#     epochs=4,\n",
    "#     verbose=False,\n",
    "#     # can optionally pass along any parameters that go into `train_model`\n",
    "#     # optimizer_class=torch.optim.SGD,\n",
    "#     # optimizer_params={\"lr\": 0.001}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_engine.finetune(**{\"lr\": 0.001, \"weight_decay\": 0.0001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "\n",
    "# Clear memory\n",
    "del finetune_engine, train_dataset\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "embed_model_name = \"dunzhang/stella_en_1.5B_v5\" #7b params\n",
    "finetuned_embed_model_name = \"/workspace/data/compliance/rule_proposal_embedding_model_gpt\"\n",
    "print(\"loading embed model...\")\n",
    "# proposal_embed_model = HuggingFaceEmbedding(model_name=finetuned_embed_model_name)\n",
    "rules_embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "\n",
    "# Settings.embed_model = embed_model\n",
    "# Settings.chunk_size = 300\n",
    "# Settings.chunk_overlap = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "entities = Literal[#\"PROPOSED_RULE\", \n",
    "                   \"ACTS\",\n",
    "                   \"SECTIONS\",\n",
    "                   \"RULES\"\n",
    "                   #\"AMENDMENTS\"\n",
    "]\n",
    "\n",
    "relations = Literal[\n",
    "    \"MENTIONS\",\n",
    "    #\"CHANGES\",\n",
    "    # \"AMENDS\", \n",
    "    \"REFERS_TO\"\n",
    "]\n",
    "\n",
    "validation_schema = {\n",
    "    # \"Proposed_Rules\": [\"CHANGES\", \"AMENDS\"],\n",
    "    \"Acts\": [\"MENTIONS\",\"REFERS_TO\"],\n",
    "    \"Sections\": [\"MENTIONS\",\"REFERS_TO\"],\n",
    "    \"Rules\": [\"MENTIONS\",\"REFERS_TO\"],\n",
    "    # \"Amendments\": [\"REFERS_TO\", \"AMENDS\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_utils import set_neo4j_password, add_lines_to_conf\n",
    "set_neo4j_password('bewaretheneo')\n",
    "# add_lines_to_conf()\n",
    "\n",
    "###### START NEO4J SERVER ######\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphRAG Database\n",
    "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor, SimpleLLMPathExtractor\n",
    "from rag_utils import create_neo4j_graph_store, create_neo4j_graphrag, neo4j_query, dump_neo4j_database\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "access_token = os.getenv('HF_TOKEN')\n",
    "llama_api_key = os.getenv('LLAMA_API_KEY')\n",
    "\n",
    "graph_idx_persist_dir = \"/workspace/data/compliance/graph_idx_testfull\"\n",
    "graph_store_persist_dir= None #\"/workspace/data/graph_store\"\n",
    "\n",
    "Settings.chunk_size = 500\n",
    "Settings.chunk_overlap = 20\n",
    "\n",
    "# kg_extractor = SchemaLLMPathExtractor(\n",
    "#     llm=llm,\n",
    "#     possible_entities=entities,\n",
    "#     possible_relations=relations,\n",
    "#     kg_validation_schema=validation_schema,\n",
    "#     strict=False,  # if false, will allow triples outside of the schema``\n",
    "#     num_workers=10,\n",
    "#     max_triplets_per_chunk=10,\n",
    "# )\n",
    "\n",
    "\n",
    "# extract_prompt = \"You are an expert compliance officer with vast knowledge of SEC Title 17 Chapter II. Your job is to read each section and link mentions of other sections, rules, or acts (e.g. § 230.503, § 240.13a-15, Act (15 U.S.C. 781), Investment Company Act of 1940) mentioned. If there are no mentions of other sections, rules, or acts, return an empty list.\"\n",
    "# extract_prompt = \"You are an expert compliance officer with vast knowledge of SEC Title 17 Chapter II. Your job is to read each section and link other sections, rules, and acts mentioned. What sections, rules, and acts (e.g. § 230.503, § 240.13a-15, Act (15 U.S.C. 781), Investment Company Act of 1940) are mentioned in the content? If there are no mentions, return an empty list.\"\n",
    "llm.is_function_calling_model = False\n",
    "extract_prompt = \"You are an expert compliance officer with vast knowledge of SEC Title 17 Chapter II. Your job is to find semantic, referential, and literal relationships between the sections. If there are no relationships, return an empty list.\"\n",
    "kg_extractor = SimpleLLMPathExtractor(\n",
    "        extract_prompt=extract_prompt,\n",
    "        llm=llm,\n",
    "        max_paths_per_chunk=10,\n",
    "        num_workers=6,\n",
    "    )\n",
    "\n",
    "# random.shuffle(documents)\n",
    "\n",
    "print(\"Creating graph store...\")\n",
    "graph_store = create_neo4j_graph_store(neo_url=\"bolt://localhost:7687\", \n",
    "                                       password=os.getenv(\"NEO4J_PWD\"), \n",
    "                                       config={\"connection_timeout\": 1000, \"connection_acquisition_timeout\": 1000, \"max_connection_pool_size\": 1000})\n",
    "\n",
    "if not os.path.exists(graph_idx_persist_dir):\n",
    "    print(\"Deleting all nodes and relationships...\")\n",
    "    neo4j_query(graph_store, query=\"\"\"MATCH n=() DETACH DELETE n\"\"\")\n",
    "\n",
    "print(\"Creating graphrag index...\")\n",
    "graph_index = create_neo4j_graphrag(documents, llm, rules_embed_model, kg_extractor, graph_store, graph_idx_persist_dir=graph_idx_persist_dir, graph_store_persist_dir=graph_store_persist_dir, similarity_top_k=3)\n",
    "\n",
    "dump_neo4j_database('neo4j', '/workspace/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = graph_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_query(graph_store, query=\"\"\"MATCH n=() DETACH DELETE n\"\"\")\n",
    "graph_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Database RAG\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "from rag_utils import create_llama_vector_index_rag\n",
    "\n",
    "vector_index = create_llama_vector_index_rag(llm, proposal_embed_model, documents=documents_proposal)\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    # node_postprocessors=[\n",
    "    #     LLMRerank(\n",
    "    #         choice_batch_size=5,\n",
    "    #         top_n=2,\n",
    "    #     )\n",
    "    # ],\n",
    "    # see https://github.com/run-llama/llama_index/blob/f7c5ee5efbb6172e819f26d1705fcdf6114b11a3/llama-index-core/llama_index/core/response_synthesizers/type.py#L4\n",
    "    response_mode=\"tree_summarize\", # \"accumulate\", \"compact_accumulate\", \"compact\", \"simple_summarize\", \"tree_summarize\"\n",
    ")\n",
    "\n",
    "# Settings.chunk_size = 500\n",
    "# Settings.chunk_overlap = 20\n",
    "\n",
    "# rules_vector_index = create_llama_vector_index_rag(llm, rules_embed_model, documents=documents)\n",
    "# graph_index = rules_vector_index.as_query_engine(\n",
    "#     similarity_top_k=3,\n",
    "#     # node_postprocessors=[\n",
    "#     #     LLMRerank(\n",
    "#     #         choice_batch_size=5,\n",
    "#     #         top_n=2,\n",
    "#     #     )\n",
    "#     # ],\n",
    "#     # see https://github.com/run-llama/llama_index/blob/f7c5ee5efbb6172e819f26d1705fcdf6114b11a3/llama-index-core/llama_index/core/response_synthesizers/type.py#L4\n",
    "#     response_mode=\"tree_summarize\", # \"accumulate\", \"compact_accumulate\", \"compact\", \"simple_summarize\", \"tree_summarize\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Give a summary of the entire proposed rule. What are the key points?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "graph_engine_tools = QueryEngineTool(\n",
    "            query_engine=graph_index,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"sec_title_17_chapter_ii_tool\",\n",
    "                description=(\n",
    "                    \"Contains all the current sections, rules, and relationships of SEC Title 17 Chapter II.\"\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "query_engine_tools = QueryEngineTool(\n",
    "            query_engine=query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"new_rule_proposal_tool\",\n",
    "                description=(\n",
    "                    \"Contains all the information about the newly proposed SEC rule.\"\n",
    "                ),\n",
    "            ),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tools = [query_engine_tools, graph_engine_tools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import ToolSelection, ToolOutput\n",
    "from llama_index.core.workflow import Event\n",
    "\n",
    "#### Define the workflow and all events\n",
    "class InputEvent(Event):\n",
    "    input: list[ChatMessage]\n",
    "\n",
    "class ToolCallEvent(Event):\n",
    "    tool_calls: list[ToolSelection]\n",
    "\n",
    "class FunctionOutputEvent(Event):\n",
    "    output: ToolOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "\n",
    "from llama_index.core.llms.function_calling import FunctionCallingLLM\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.tools.types import BaseTool\n",
    "from llama_index.core.workflow import Workflow, StartEvent, StopEvent, step\n",
    "\n",
    "\n",
    "class FuncationCallingAgent(Workflow):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args: Any,\n",
    "        llm: FunctionCallingLLM | None = None,\n",
    "        tools: List[BaseTool] | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tools = tools or []\n",
    "\n",
    "        self.llm = llm\n",
    "        assert self.llm.metadata.is_function_calling_model\n",
    "\n",
    "        self.memory = ChatMemoryBuffer.from_defaults(llm=llm)\n",
    "        self.sources = []\n",
    "\n",
    "    @step\n",
    "    async def prepare_chat_history(self, ev: StartEvent) -> InputEvent:\n",
    "        # clear sources\n",
    "        self.sources = []\n",
    "\n",
    "        # get user input\n",
    "        user_input = ev.input\n",
    "        user_msg = ChatMessage(role=\"user\", content=user_input)\n",
    "        self.memory.put(user_msg)\n",
    "\n",
    "        # get chat history\n",
    "        chat_history = self.memory.get()\n",
    "        return InputEvent(input=chat_history)\n",
    "\n",
    "    @step\n",
    "    async def handle_llm_input(\n",
    "        self, ev: InputEvent\n",
    "    ) -> ToolCallEvent | StopEvent:\n",
    "        chat_history = ev.input\n",
    "        '''\n",
    "        Takes in the chat history, and uses tools to generate a response.\n",
    "        '''\n",
    "        response = await self.llm.achat_with_tools(\n",
    "            self.tools, chat_history=chat_history\n",
    "        )\n",
    "        self.memory.put(response.message)\n",
    "\n",
    "        tool_calls = self.llm.get_tool_calls_from_response(\n",
    "            response, error_on_no_tool_call=False\n",
    "        )\n",
    "\n",
    "        if not tool_calls:\n",
    "            return StopEvent(\n",
    "                result={\"response\": response, \"sources\": [*self.sources]}\n",
    "            )\n",
    "        else:\n",
    "            return ToolCallEvent(tool_calls=tool_calls)\n",
    "\n",
    "    @step\n",
    "    async def handle_tool_calls(self, ev: ToolCallEvent) -> InputEvent:\n",
    "        tool_calls = ev.tool_calls\n",
    "        tools_by_name = {tool.metadata.get_name(): tool for tool in self.tools}\n",
    "\n",
    "        tool_msgs = []\n",
    "\n",
    "        # call tools -- safely!\n",
    "        for tool_call in tool_calls:\n",
    "            tool = tools_by_name.get(tool_call.tool_name)\n",
    "            additional_kwargs = {\n",
    "                \"tool_call_id\": tool_call.tool_id,\n",
    "                \"name\": tool.metadata.get_name(),\n",
    "            }\n",
    "            if not tool:\n",
    "                tool_msgs.append(\n",
    "                    ChatMessage(\n",
    "                        role=\"tool\",\n",
    "                        content=f\"Tool {tool_call.tool_name} does not exist\",\n",
    "                        additional_kwargs=additional_kwargs,\n",
    "                    )\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                tool_output = tool(**tool_call.tool_kwargs)\n",
    "                self.sources.append(tool_output)\n",
    "                tool_msgs.append(\n",
    "                    ChatMessage(\n",
    "                        role=\"tool\",\n",
    "                        content=tool_output.content,\n",
    "                        additional_kwargs=additional_kwargs,\n",
    "                    )\n",
    "                )\n",
    "            except Exception as e:\n",
    "                tool_msgs.append(\n",
    "                    ChatMessage(\n",
    "                        role=\"tool\",\n",
    "                        content=f\"Encountered error in tool call: {e}\",\n",
    "                        additional_kwargs=additional_kwargs,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        for msg in tool_msgs:\n",
    "            self.memory.put(msg)\n",
    "\n",
    "        chat_history = self.memory.get()\n",
    "        return InputEvent(input=chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm.is_function_calling_model = True\n",
    "\n",
    "agent = FuncationCallingAgent(\n",
    "    llm=llm, tools=all_tools, timeout=120, verbose=True\n",
    ")\n",
    "\n",
    "ret = await agent.run(input=\"What is a summary of the proposed rule, and what SEC rules does it change?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ret[\"response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "from llama_index.utils.workflow import draw_most_recent_execution\n",
    "from IPython.display import display, HTML\n",
    "draw_all_possible_flows(FuncationCallingAgent, \"first_func_agent.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='/workspace/repos/agentic-ai/first_func_agent.html', width=700, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Read the HTML file content\n",
    "with open('/workspace/repos/agentic-ai/first_func_agent.html', 'r') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Render the HTML content in a Jupyter cell\n",
    "display(HTML(html_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import asyncio\n",
    "from pydantic_settings import BaseSettings\n",
    "import signal\n",
    "from llama_deploy.deploy.deploy import (\n",
    "                        _get_message_queue_client,\n",
    "                        _deploy_local_message_queue,\n",
    "                        _get_shutdown_handler\n",
    "                    )\n",
    "\n",
    "from llama_deploy import (\n",
    "    deploy_core,\n",
    "    ControlPlaneConfig,\n",
    "    SimpleMessageQueueConfig,\n",
    "    SimpleOrchestratorConfig,\n",
    "    ControlPlaneServer,\n",
    "    SimpleOrchestrator,\n",
    "    LlamaDeployClient\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_deploy import (\n",
    "    deploy_core,\n",
    "    ControlPlaneConfig,\n",
    "    SimpleMessageQueueConfig,\n",
    ")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    await deploy_core(\n",
    "        control_plane_config=ControlPlaneConfig(port=8002),\n",
    "        message_queue_config=SimpleMessageQueueConfig(port=8003),\n",
    "    )\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import asyncio\n",
    "\n",
    "#     asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_deploy import (\n",
    "    deploy_workflow,\n",
    "    WorkflowServiceConfig,\n",
    "    ControlPlaneConfig,\n",
    "    SimpleMessageQueueConfig,\n",
    ")\n",
    "from llama_index.core.workflow import Workflow, StartEvent, StopEvent, step\n",
    "\n",
    "\n",
    "# create a dummy workflow\n",
    "class MyWorkflow(Workflow):\n",
    "    @step()\n",
    "    async def run_step(self, ev: StartEvent) -> StopEvent:\n",
    "        # Your workflow logic here\n",
    "        arg1 = str(ev.get(\"arg1\", \"\"))\n",
    "        result = arg1 + \"_result\"\n",
    "        return StopEvent(result=result)\n",
    "\n",
    "\n",
    "async def main():\n",
    "    await deploy_workflow(\n",
    "        workflow=MyWorkflow(),\n",
    "        workflow_config=WorkflowServiceConfig(\n",
    "            host=\"127.0.0.1\", port=8004, service_name=\"my_workflow\"\n",
    "        ),\n",
    "        control_plane_config=ControlPlaneConfig(),\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context,\n",
    ")\n",
    "import random\n",
    "from llama_index.core.workflow import draw_all_possible_flows\n",
    "from llama_index.utils.workflow import draw_most_recent_execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import asyncio\n",
    "from pydantic_settings import BaseSettings\n",
    "import signal\n",
    "from llama_deploy.deploy.deploy import (\n",
    "                        _get_message_queue_client,\n",
    "                        _deploy_local_message_queue,\n",
    "                        _get_shutdown_handler\n",
    "                    )\n",
    "\n",
    "from llama_deploy import (\n",
    "    deploy_core,\n",
    "    ControlPlaneConfig,\n",
    "    SimpleMessageQueueConfig,\n",
    "    SimpleOrchestratorConfig,\n",
    "    ControlPlaneServer,\n",
    "    SimpleOrchestrator,\n",
    "    LlamaDeployClient\n",
    ")\n",
    "\n",
    "\n",
    "async def deploy_core(\n",
    "    control_plane_config: ControlPlaneConfig,\n",
    "    message_queue_config: BaseSettings,\n",
    "    orchestrator_config: Optional[SimpleOrchestratorConfig] = None,\n",
    ") -> None:\n",
    "    orchestrator_config = orchestrator_config or SimpleOrchestratorConfig()\n",
    "\n",
    "    message_queue_client = _get_message_queue_client(message_queue_config)\n",
    "\n",
    "    control_plane = ControlPlaneServer(\n",
    "        message_queue_client,\n",
    "        SimpleOrchestrator(**orchestrator_config.model_dump()),\n",
    "        **control_plane_config.model_dump(),\n",
    "    )\n",
    "\n",
    "    message_queue_task = None\n",
    "    if isinstance(message_queue_config, SimpleMessageQueueConfig):\n",
    "        message_queue_task = _deploy_local_message_queue(message_queue_config)\n",
    "\n",
    "    control_plane_task = asyncio.create_task(control_plane.launch_server())\n",
    "\n",
    "    # let services spin up\n",
    "    await asyncio.sleep(1)\n",
    "\n",
    "    # register the control plane as a consumer\n",
    "    control_plane_consumer_fn = await control_plane.register_to_message_queue()\n",
    "\n",
    "    consumer_task = asyncio.create_task(control_plane_consumer_fn())\n",
    "\n",
    "    # let things sync up\n",
    "    await asyncio.sleep(1)\n",
    "\n",
    "    # let things run\n",
    "    if message_queue_task:\n",
    "        all_tasks = [control_plane_task, consumer_task, message_queue_task]\n",
    "    else:\n",
    "        all_tasks = [control_plane_task, consumer_task]\n",
    "\n",
    "    shutdown_handler = _get_shutdown_handler(all_tasks)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    while loop.is_running():\n",
    "        await asyncio.sleep(0.1)\n",
    "        signal.signal(signal.SIGINT, shutdown_handler)\n",
    "\n",
    "        for task in all_tasks:\n",
    "            if task.done() and task.exception():  # type: ignore\n",
    "                raise task.exception()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from llama_index.core.workflow import Workflow\n",
    "from llama_deploy import (\n",
    "    WorkflowServiceConfig,\n",
    "    WorkflowService,\n",
    ")\n",
    "\n",
    "from llama_deploy.deploy.deploy import (\n",
    "                        _get_message_queue_config,\n",
    "                    )\n",
    "\n",
    "async def deploy_workflow(\n",
    "    workflow: Workflow,\n",
    "    workflow_config: WorkflowServiceConfig,\n",
    "    control_plane_config: ControlPlaneConfig,\n",
    ") -> None:\n",
    "    control_plane_url = control_plane_config.url\n",
    "\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.get(f\"{control_plane_url}/queue_config\")\n",
    "        queue_config_dict = response.json()\n",
    "\n",
    "    message_queue_config = _get_message_queue_config(queue_config_dict)\n",
    "    message_queue_client = _get_message_queue_client(message_queue_config)\n",
    "\n",
    "    service = WorkflowService(\n",
    "        workflow=workflow,\n",
    "        message_queue=message_queue_client,\n",
    "        **workflow_config.model_dump(),\n",
    "    )\n",
    "\n",
    "    service_task = asyncio.create_task(service.launch_server())\n",
    "\n",
    "    # let service spin up\n",
    "    await asyncio.sleep(1)\n",
    "\n",
    "    # register to message queue\n",
    "    consumer_fn = await service.register_to_message_queue()\n",
    "\n",
    "    # register to control plane\n",
    "    control_plane_url = (\n",
    "        f\"http://{control_plane_config.host}:{control_plane_config.port}\"\n",
    "    )\n",
    "    await service.register_to_control_plane(control_plane_url)\n",
    "\n",
    "    # create consumer task\n",
    "    consumer_task = asyncio.create_task(consumer_fn())\n",
    "\n",
    "    # let things sync up\n",
    "    await asyncio.sleep(1)\n",
    "\n",
    "    all_tasks = [consumer_task, service_task]\n",
    "\n",
    "    shutdown_handler = _get_shutdown_handler(all_tasks)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    while loop.is_running():\n",
    "        await asyncio.sleep(0.1)\n",
    "        signal.signal(signal.SIGINT, shutdown_handler)\n",
    "\n",
    "        for task in all_tasks:\n",
    "            if task.done() and task.exception():  # type: ignore\n",
    "                raise task.exception()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents import (\n",
    "    AgentService,\n",
    "    ToolService,\n",
    "    MetaServiceTool,\n",
    "    ControlPlaneServer,\n",
    "    SimpleMessageQueue,\n",
    "    AgentOrchestrator,\n",
    ")\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import ReActAgentWorker, ReActAgent\n",
    "\n",
    "\n",
    "\n",
    "# create our multi-agent framework components\n",
    "message_queue = SimpleMessageQueue()\n",
    "control_plane = ControlPlaneServer(\n",
    "    message_queue=message_queue,\n",
    "    orchestrator=AgentOrchestrator(llm=llm),\n",
    ")\n",
    "\n",
    "# define Tool Service\n",
    "tool_service = ToolService(\n",
    "    message_queue=message_queue,\n",
    "    tools=all_tools,\n",
    "    running=True,\n",
    "    step_interval=0.5,\n",
    ")\n",
    "\n",
    "# define meta-tools here\n",
    "meta_tools = [\n",
    "    await MetaServiceTool.from_tool_service(\n",
    "        t.metadata.name,\n",
    "        message_queue=message_queue,\n",
    "        tool_service=tool_service,\n",
    "    )\n",
    "    for t in all_tools\n",
    "]\n",
    "\n",
    "\n",
    "# define Agent and agent service\n",
    "# worker1 = FunctionCallingAgentWorker.from_tools(\n",
    "worker1 = ReActAgentWorker.from_tools(\n",
    "    meta_tools,\n",
    "    llm=llm,\n",
    ")\n",
    "agent1 = worker1.as_agent()\n",
    "agent_server_1 = AgentService(\n",
    "    agent=agent1,\n",
    "    message_queue=message_queue,\n",
    "    description=\"Agent that answers questions based on the newly proposed SEC rule.\",\n",
    "    service_name=\"rule_proposal_agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent1.chat('When are written comments on this notice of joint proposed rulemaking need to be submitted?')\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions\n",
    "# Name of rule \n",
    "# Is it a proposed rule or a final rule\n",
    "# Issue date and Federal Register date\n",
    "# What agency(ies) is the Rule coming from\n",
    "# If a proposed rule, when are public comments due by and where should they be sent (this info is in the Rule document under Dates and Addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "summarizer = TreeSummarize(llm=llm, verbose=True)\n",
    "# prompt_summary = \"You are a professional executive of AlphaTrAI. Your job is to summarize this text in great detail from a video transcription. The summary will be distributed to investors and stakeholders, so give a lot of details and examples from the transcription.\"\n",
    "prompt_summary = f\"\"\"You are a professional executive at AlphaTrAI. Your job is to summarize the text from a video transcription. The summary will be a memo distributed to investors and stakeholders. Be sure it the memo has the following items:\n",
    "1. Extract all the names of new hires and their position, and/or new advisors mentioned in the transcription.\n",
    "2. Create a section to mention the personnel new to AlphaTrAI.\n",
    "3. Include other highlights and progress made by AlphaTrAI.\n",
    "4. Ensure the memo and ensure it is factual, optimistic, and any values mention come directly from the text. \n",
    "\n",
    "The transcription is as follows:\\n{full_doc}\"\"\"\n",
    "\n",
    "response = await summarizer.aget_response(prompt_summary, [doc.text for doc in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM direct summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_summary = f\"\"\"You are a professional executive at AlphaTrAI. Your job is to summarize the text from a video transcription. The summary will be a memo distributed to investors and stakeholders. Be sure it the memo has the following items:\n",
    "1. Extract all the names of new hires and their position, and/or new advisors mentioned in the transcription.\n",
    "2. Include other highlights and progress made by AlphaTrAI.\n",
    "3. Ensure the memo is professional, fluid, factual, and optimistic. \n",
    "\n",
    "The transcription is as follows:\\n{full_doc}\"\"\"\n",
    "\n",
    "response = llm.complete(prompt_summary, max_tokens=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install llama-index-embeddings-huggingface llama-index-vector-stores-neo4jvector llama-index-graph-stores-neo4j\n",
    "!apt install dialog apt-utils -y (done above)\n",
    "!wget -O - https://debian.neo4j.com/neotechnology.gpg.key | gpg --dearmor -o /etc/apt/keyrings/neotechnology.gpg\n",
    "!echo 'deb [signed-by=/etc/apt/keyrings/neotechnology.gpg] https://debian.neo4j.com stable latest' | tee -a /etc/apt/sources.list.d/neo4j.list\n",
    "!apt list -a neo4j\n",
    "!add-apt-repository universe -y\n",
    "!apt install neo4j=1:5.22.0 -y\n",
    "!echo \"neo4j-enterprise neo4j/question select I ACCEPT\" | debconf-set-selections\n",
    "!echo \"neo4j-enterprise neo4j/license note\" | debconf-set-selections\n",
    "!apt install openjdk-17-jre -y\n",
    "!cd /var/lib/neo4j/plugins/ && wget https://github.com/neo4j/apoc/releases/download/5.22.0/apoc-5.22.0-core.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_neo4j_password('bewaretheneo')\n",
    "add_lines_to_conf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from llama_index.core.agent import ReActAgent\n",
    "# from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from rag_utils import create_neo4j_graph_store, create_neo4j_graphrag, neo4j_query, set_neo4j_password, add_lines_to_conf\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm.is_function_calling_model = True\n",
    "\n",
    "embed_model_name = \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\"\n",
    "print(\"loading embed model...\")\n",
    "embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 300\n",
    "Settings.chunk_overlap = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor\n",
    "\n",
    "entities = Literal[\"PEOPLE\", \n",
    "                   \"PLACE\"\n",
    "]\n",
    "\n",
    "relations = Literal[\n",
    "    \"ROLE\",\n",
    "    \"COMPANY\"\n",
    "]\n",
    "\n",
    "validation_schema = {\n",
    "    \"People\": [\"ROLE\"],\n",
    "    \"Place\": [\"COMPANY\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.chunk_size = 300\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "kg_extractor = SchemaLLMPathExtractor(\n",
    "    llm=llm,\n",
    "    possible_entities=entities,\n",
    "    possible_relations=relations,\n",
    "    kg_validation_schema=validation_schema,\n",
    "    strict=True,  # if false, will allow triples outside of the schema\n",
    "    num_workers=4,\n",
    "    max_triplets_per_chunk=10,\n",
    ")\n",
    "\n",
    "graph_store = create_neo4j_graph_store(neo_url=\"bolt://localhost:7687\", \n",
    "                                       password=os.getenv(\"NEO4J_PWD\"), \n",
    "                                       config={\"connection_timeout\": 240, \"connection_acquisition_timeout\": 240, \"max_connection_pool_size\": 1000})\n",
    "neo4j_query(graph_store, query=\"\"\"MATCH (n) DETACH DELETE n\"\"\")\n",
    "\n",
    "\n",
    "graph_index = create_neo4j_graphrag(documents, llm, embed_model, kg_extractor, graph_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "query_engine_tools = QueryEngineTool(\n",
    "            query_engine=graph_index,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"graph_tool\",\n",
    "                description=(\n",
    "                    \"Useful for finding people names and roles, and the company they work for.\"\n",
    "                ),\n",
    "            ),\n",
    "        ),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install llama-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents import (\n",
    "    AgentService,\n",
    "    ToolService,\n",
    "    MetaServiceTool,\n",
    "    ControlPlaneServer,\n",
    "    SimpleMessageQueue,\n",
    "    AgentOrchestrator,\n",
    ")\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker, ReActAgentWorker, ReActAgent, LATSAgentWorker\n",
    "\n",
    "\n",
    "\n",
    "# create our multi-agent framework components\n",
    "message_queue = SimpleMessageQueue()\n",
    "control_plane = ControlPlaneServer(\n",
    "    message_queue=message_queue,\n",
    "    orchestrator=AgentOrchestrator(llm=llm),\n",
    ")\n",
    "\n",
    "# define Tool Service\n",
    "tool_service = ToolService(\n",
    "    message_queue=message_queue,\n",
    "    tools=[query_engine_tools],#, adding_tool],\n",
    "    running=True,\n",
    "    step_interval=0.5,\n",
    ")\n",
    "\n",
    "# define meta-tools here\n",
    "meta_tools = [\n",
    "    await MetaServiceTool.from_tool_service(\n",
    "        t.metadata.name,\n",
    "        message_queue=message_queue,\n",
    "        tool_service=tool_service,\n",
    "    )\n",
    "    for t in [query_engine_tools]#, adding_tool]\n",
    "]\n",
    "\n",
    "\n",
    "# define Agent and agent service\n",
    "# worker1 = FunctionCallingAgentWorker.from_tools(\n",
    "worker1 = ReActAgentWorker.from_tools(\n",
    "    meta_tools,\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "worker2 = LATSAgentWorker.from_tools(\n",
    "    meta_tools,\n",
    "    llm=llm,\n",
    "    num_expansions=2,\n",
    "    max_rollouts=3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent1 = worker1.as_agent()\n",
    "agent_server_1 = AgentService(\n",
    "    agent=agent1,\n",
    "    message_queue=message_queue,\n",
    "    description=\"Summarize a transcription as a memo for investors and stakeholders.\",\n",
    "    service_name=\"summarize_transcription\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
