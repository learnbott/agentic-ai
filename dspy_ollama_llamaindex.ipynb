{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!cp '/workspace/repos/agentic-ai/PPM - MCG MADISON RIDGE DST.pdf' /workspace/data\n",
    "!cp '/workspace/repos/agentic-ai/MASTER - PYTHON - SCORING MODEL - MCG MADISON RIDGE DST - v2.0.xlsx' /workspace/data\n",
    "!pip3 install llama-index llama-parse llama-agents llama-index-llms-huggingface llama-index-embeddings-huggingface llama-index-llms-huggingface-api llama-index-llms-ollama llama-index-embeddings-ollama llama-index-vector-stores-neo4jvector llama-index-graph-stores-neo4j\n",
    "!pip3 install openpyxl sentencepiece protobuf evaluate rouge_score absl-py tensorboardX bitsandbytes peft accelerate python-dotenv dspy-ai InstructorEmbedding\n",
    "!pip3 install flash-attn --no-build-isolation\n",
    "!curl -L https://ollama.com/download/ollama-linux-amd64 -o /usr/bin/ollama\n",
    "!chmod +x /usr/bin/ollama\n",
    "!useradd -r -s /bin/false -m -d /usr/share/ollama ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install tmux, open new screen and run `ollama start`, and then `ollama pull qwen2:1.5b`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://www.ecfr.gov/developers/documentation/api/v1#/Versioner%20Service/get_api_versioner_v1_full__date__title__title__xml\n",
    "\n",
    "# !cd /workspace/data && curl -X GET \"https://www.ecfr.gov/api/versioner/v1/full/2024-07-23/title-17.xml?chapter=II&part=200&subpart=A&section=200.1\" -H \"accept: application/xml\"\n",
    "# !cd /workspace/data && curl -X GET \"https://www.ecfr.gov/api/versioner/v1/full/2024-07-23/title-17.xml?chapter=II&part=200\" -H \"accept: application/xml\"\n",
    "!cd /workspace/data && curl -X GET \"https://www.ecfr.gov/api/versioner/v1/full/2024-07-23/title-17.xml?chapter=II\" -H \"accept: application/xml\" > title-17.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!cd /workspace/data && curl -X GET \"https://api.finra.org/data/group/finra/name/finraRulebookMock?offset=0&limit=10000&delimiter=%2C&quoteValues=true&async=false&access_token=ae4bfc7741084e2c9944\" -H  \"accept: application/json\" -H  \"Content-Type: application/json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!wget -O - https://debian.neo4j.com/neotechnology.gpg.key | gpg --dearmor -o /etc/apt/keyrings/neotechnology.gpg\n",
    "!echo 'deb [signed-by=/etc/apt/keyrings/neotechnology.gpg] https://debian.neo4j.com stable latest' | tee -a /etc/apt/sources.list.d/neo4j.list\n",
    "\n",
    "!apt-get update\n",
    "!apt list -a neo4j\n",
    "!add-apt-repository universe -y\n",
    "!apt-get install neo4j=1:5.22.0 -y\n",
    "!echo \"neo4j-enterprise neo4j/question select I ACCEPT\" | debconf-set-selections\n",
    "!echo \"neo4j-enterprise neo4j/license note\" | debconf-set-selections\n",
    "!apt remove openjdk-21-jre-headless -y\n",
    "!apt install openjdk-17-jre -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apoc plugin needs to go in /var/lib/neo4j/plugins\n",
    "#### NOTE #### neo4j and apoc versions must match!!!!!!\n",
    "# wget https://github.com/neo4j/apoc/releases/download/5.22.0/apoc-5.22.0-core.jar\n",
    "\n",
    "# in vim /etc/neo4j/neo4j.conf add the following lines\n",
    "# dbms.security.procedures.allowlist=apoc.*\n",
    "# dbms.security.procedures.unrestricted=apoc.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "import dspy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from train_utils import get_csv_string, randomize_row_values, operators_dict, range_description_json, split_df_by_empty_columns, split_df_by_empty_rows, print_trainable_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Path to your XML file\n",
    "# xml_file_path = '/workspace/data/test.xml'\n",
    "xml_file_path = '/workspace/data/title-17.xml'\n",
    "\n",
    "# Parse the XML file\n",
    "tree = ET.parse(xml_file_path)\n",
    "\n",
    "# Get the root element of the XML document\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_tree_data(element, depth=0, list_depth=0, prev_depth=-1, lists=None, final_dict=None):\n",
    "    if lists is None:\n",
    "        lists = [[]]\n",
    "    if final_dict is None:\n",
    "        final_dict = {}\n",
    "\n",
    "    if element.tag == \"HEAD\":\n",
    "\n",
    "        if depth > list_depth:\n",
    "            lists[-1].append(f\"{element.text.strip()}\")\n",
    "        else:\n",
    "            lists.append(lists[-1][:depth-2])\n",
    "            lists[-1].append(f\"{element.text.strip()}\")\n",
    "        \n",
    "    elif element.tag == \"P\":\n",
    "        # \"P\" indicates that there is text associated with the element\n",
    "        # This only saves to the final_dict if the element has a child with tag \"P\"\n",
    "        try:\n",
    "            final_dict[str(lists[-1])] += ''.join(element.itertext())\n",
    "        except:\n",
    "            final_dict[str(lists[-1])] = ''.join(element.itertext())\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Recursively call get_tree_data on each child, increasing the depth\n",
    "    for child in element:\n",
    "        get_tree_data(child, depth + 1, len(lists[-1])+1, depth, lists, final_dict)\n",
    "\n",
    "    return final_dict\n",
    "\n",
    "data = get_tree_data(root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {k:v for k,v in data.items() if 'CHAPTER II—SECURITIES AND EXCHANGE COMMISSION' in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# model_name = \"mistral:latest\"\n",
    "model_name = \"llama3.1:latest\"\n",
    "# model_name = \"qwen2:1.5b\"\n",
    "# model_name = \"gemma:1.5b\"\n",
    "\n",
    "system_prompt = \"You are an expert at answering questions about rules and regulations regarding Title 17—Commodity and Securities Exchanges: CHAPTER II—SECURITIES AND EXCHANGE COMMISSION. Please provide a summary of the following text, and cite any rule or regulation codes (e.g. § 230.503, Rule 501(a)) from context that support the answer:\"\n",
    "llm = Ollama(model=model_name, url=\"http://127.0.0.1:11434\", system_prompt=system_prompt)\n",
    "llm.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_name, num_ctx = \"llama3.1\", 128000\n",
    "llm = dspy.OllamaLocal(model=model_name, max_new_tokens=1000, max_tokens=4000, temperature=0.0, num_ctx=num_ctx, model_type='chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, dotenv\n",
    "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
    "dotenv.load_dotenv(\"/workspace/repos/agentic-ai/.env\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(s, metadata={\"title\":None, \"chapter\":None, \"part\":None, \"subpart\":None, \"description\":None}):\n",
    "    s_split = s[2:-2].split(\"', '\")\n",
    "    key_lengths = {key: len(key) for key in metadata}\n",
    "    for x in s_split:\n",
    "        xsplit = x.lower().split(\"—\")\n",
    "        if len(xsplit) > 1:\n",
    "            prefix = xsplit[0]\n",
    "            for key, length in key_lengths.items():\n",
    "                if prefix[:length] == key:\n",
    "                    metadata[key] = x\n",
    "                    break\n",
    "    metadata[\"description\"] = s_split[-1]\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor\n",
    "\n",
    "entities = Literal[\"RULES\", \"REGULATIONS\"]\n",
    "relations = Literal[\n",
    "    \"RELATED_TO\",\n",
    "    \"REFERENCED_BY\",\n",
    "    \"REFERS_TO\",\n",
    "]\n",
    "all_relations = [\n",
    "    \"RELATED_TO\",\n",
    "    \"REFERENCED_BY\",\n",
    "    \"REFERS_TO\",\n",
    "]\n",
    "\n",
    "validation_schema = {\n",
    "    \"Rules\": all_relations,\n",
    "    \"Regulations\": all_relations\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# from llama_index.embeddings.instructor import InstructorEmbedding\n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, PropertyGraphIndex\n",
    "from llama_index.readers.file import PandasExcelReader\n",
    "from llama_index.core import set_global_tokenizer, Document\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "from transformers import AutoTokenizer\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "access_token = os.getenv('HF_TOKEN')\n",
    "llama_api_key = os.getenv('LLAMA_API_KEY')\n",
    "persist_dir = \"/workspace/data/storage/sec\"\n",
    "\n",
    "Settings.llm = llm\n",
    "\n",
    "Settings.chunk_size = 400\n",
    "Settings.chunk_overlap = 50\n",
    "embed_model_name = \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\"\n",
    "# embed_model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "# embed_model_name = \"hkunlp/instructor-base\"\n",
    "print(\"loading embed model...\")\n",
    "embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "# embed_model = InstructorEmbedding(model_name=embed_model_name)\n",
    "\n",
    "Settings.embed_model = embed_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('loading data to vector store...')\n",
    "\n",
    "if os.path.exists(persist_dir):\n",
    "    print(f'    loading from {persist_dir}...')\n",
    "    # rebuild storage context\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "    # load index\n",
    "    vector_index = load_index_from_storage(storage_context, embed_model=embed_model)\n",
    "else:\n",
    "    print(f'    building from scratch...')\n",
    "    documents = [Document(text=t, \n",
    "                        metadata=get_metadata(m),\n",
    "                        excluded_llm_metadata_keys=[\"title\", \"chapter\"],) for m,t in data.items()]\n",
    "    \n",
    "    from llama_index.core import PropertyGraphIndex\n",
    "\n",
    "    kg_extractor = SchemaLLMPathExtractor(\n",
    "        llm=llm,\n",
    "        possible_entities=entities,\n",
    "        possible_relations=relations,\n",
    "        kg_validation_schema=validation_schema,\n",
    "        # if false, allows for values outside of the schema\n",
    "        # useful for using the schema as a suggestion\n",
    "        num_workers=4,\n",
    "        strict=False,\n",
    "    )\n",
    "\n",
    "    # from llama_index.core.indices.property_graph import SimpleLLMPathExtractor\n",
    "\n",
    "    # kg_extractor = SimpleLLMPathExtractor(\n",
    "    #     llm=llm,\n",
    "    #     max_paths_per_chunk=10,\n",
    "    #     num_workers=10,\n",
    "    # )\n",
    "\n",
    "    NUMBER_OF_ARTICLES = 50\n",
    "\n",
    "    neo_url = \"bolt://localhost:7687\"\n",
    "    graph_store = Neo4jPropertyGraphStore(\n",
    "        username=\"neo4j\",\n",
    "        password=os.getenv(\"NEO4J_PWD\"),\n",
    "        url=neo_url\n",
    "    )\n",
    "\n",
    "    vector_index = PropertyGraphIndex.from_documents(documents[:NUMBER_OF_ARTICLES], \n",
    "                                                     llm=llm,\n",
    "                                                     property_graph_store=graph_store, \n",
    "                                                     embed_model=embed_model, \n",
    "                                                     kg_extractors=[kg_extractor],\n",
    "                                                     show_progress=True)\n",
    "    # vector_index = VectorStoreIndex.from_documents(documents, embed_model=embed_model, show_progress=True)\n",
    "    vector_index.storage_context.persist(persist_dir=persist_dir)\n",
    "\n",
    "# query_engine = vector_index.as_retriever(similarity_top_k=3)\n",
    "query_engine = vector_index.as_query_engine(similarity_top_k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"If an advertisement has performance data, what disclosures are required?\", \n",
    "    \"What disclosure is needed in an advertisement for a government money market?\",\n",
    "    \"What is the definition of sales literature?\",\n",
    "    \"What is considered materially misleading?\",\n",
    "    \"What is an accredited investor and how do they differ from an institutional buyer?\"\n",
    "]\n",
    "\n",
    "gt = [\n",
    "    \"230.482(B)(3)(i)\",\n",
    "    \"230.482(b)(4)(ii) or (iii). If the Government Money Market Fund that has the ability to impose fee when sold and/or temporarily suspend the ability to sell if the fund's liquidity falls below a certain level\",\n",
    "    \"sales literature shall be deemed to include any communication (whether in writing, by radio, or by television) used by any person to offer to sell or induce the sale of securities of any investment company. Communications between issuers, underwriters and dealers are included in this definition of sales literature if such communications, or the information contained therein, can be reasonably expected to be communicated to prospective investors in the offer or sale of securities or are designed to be employed in either written or oral form in the offer or sale of securities.\",\n",
    "    \"230.156(a)(1 and 2)\",\n",
    "    \"Definition of Accredited investor Rule 501(a)(1-13) / Definition of Qualified Institutional Buyer Rule 230.144A(a)(I - vi)\"\n",
    "]\n",
    "\n",
    "assert len(questions) == len(gt)\n",
    "\n",
    "for idx,question in enumerate(questions):\n",
    "    print(\"=====================================================\")\n",
    "    print(\"=====================================================\")\n",
    "    print(f'QUESTION {idx+1}:\\n', question)\n",
    "    print(\"=====================================================\")\n",
    "\n",
    "    answer = query_engine.query(question)\n",
    "    \n",
    "    print(\"MATT'S ANSWER:\\n\", gt[idx])\n",
    "    print(\"=====================================================\")\n",
    "    print('ANSWER:\\n', answer.response)\n",
    "    print(\"=====================================================\")\n",
    "    print('CONTEXT:')\n",
    "    for i,con in enumerate(answer.source_nodes):\n",
    "        print(f\"--Source {i}:\\n\", con.metadata)\n",
    "        print(f\"--Info {i}:\\n\", con.text)\n",
    "        print()\n",
    "        print('---')\n",
    "        print()\n",
    "   \n",
    "    print()\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from dspy.retrieve.llama_index_rm import LlamaIndexRM\n",
    "retriever = LlamaIndexRM(query_engine)\n",
    "dspy.settings.configure(lm=llm, rm=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from models_sec import SECQnA, SECQnAReAct, ReActGenerateAnswer\n",
    "\n",
    "sec_title17_analyzer = dspy.ReAct(ReActGenerateAnswer, tools=[dspy.Retrieve(k=3)], max_iters=3)\n",
    "# sec_title17_analyzer = SECQnA(num_passages=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"If an advertisement has performance data, what disclosures are required?\", \n",
    "    \"What disclosure is needed in an advertisement for a government money market?\",\n",
    "    \"What is the definition of sales literature?\",\n",
    "    \"What is considered materially misleading?\",\n",
    "    \"What is an accredited investor and how do they differ from an institutional buyer?\"\n",
    "]\n",
    "\n",
    "gt = [\n",
    "    \"230.482(B)(3)(i)\",\n",
    "    \"230.482(b)(4)(ii) or (iii). If the Government Money Market Fund that has the ability to impose fee when sold and/or temporarily suspend the ability to sell if the fund's liquidity falls below a certain level\",\n",
    "    \"sales literature shall be deemed to include any communication (whether in writing, by radio, or by television) used by any person to offer to sell or induce the sale of securities of any investment company. Communications between issuers, underwriters and dealers are included in this definition of sales literature if such communications, or the information contained therein, can be reasonably expected to be communicated to prospective investors in the offer or sale of securities or are designed to be employed in either written or oral form in the offer or sale of securities.\",\n",
    "    \"230.156(a)(1 and 2)\",\n",
    "    \"Definition of Accredited investor Rule 501(a)(1-13) / Definition of Qualified Institutional Buyer Rule 230.144A(a)(I - vi)\"\n",
    "]\n",
    "\n",
    "assert len(questions) == len(gt)\n",
    "\n",
    "for idx in range(len(questions)):\n",
    "    print(f'QUESTION {idx+1}:\\n', questions[idx])\n",
    "    print(\"=====================================================\")\n",
    "\n",
    "    answer = sec_title17_analyzer(question=questions[idx])\n",
    "    \n",
    "    print(\"MATT'S ANSWER:\\n\", gt[idx])\n",
    "    print(\"=====================================================\")\n",
    "    print('ANSWER:\\n', answer.answer)\n",
    "    print(\"=====================================================\")\n",
    "    # print('CONTEXT:')\n",
    "    # for con in answer.context:\n",
    "    #     print(con)\n",
    "    #     print()\n",
    "    #     print('---')\n",
    "    #     print()\n",
    "   \n",
    "    print()\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer.observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gt[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(questions[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_title17_analyzer.inspect_history(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer.context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule 230.482\n",
    "# Q.1 - If an advertisement has performance data, what disclosures are required?\n",
    "# A.1 - 230.482(B)(3)(i)\n",
    "\n",
    "# Q.2 - What disclosure is need in an advertisement for a government money market?\n",
    "# A.2 - 230.482(b)(4)(ii) or (iii). If the Government Money Market Fund that has the ability to impose fee when sold and/or temporarily suspend the ability to sell if the fund's liquidity falls below a certain level\n",
    "\n",
    "# Rule 230.156\n",
    "# Q.1 - What is the definition of sales literature?\n",
    "# A.1 - sales literature shall be deemed to include any communication (whether in writing, by radio, or by television) used by any person to offer to sell or induce the sale of securities of any investment company. Communications between issuers, underwriters and dealers are included in this definition of sales literature if such communications, or the information contained therein, can be reasonably expected to be communicated to prospective investors in the offer or sale of securities or are designed to be employed in either written or oral form in the offer or sale of securities.\n",
    "\n",
    "# Q.2 - What is considered materially misleading?\n",
    "# A.2 - 230.156(a)(1 and 2)\n",
    "# Rule 230.144A and 230.501(a)\n",
    "\n",
    "# Q.1 - What is an accredited investor and how do they differ from an institutional buyer?\n",
    "# A.1 - Definition of Accredited investor Rule 501(a)(1-13) / Definition of Qualified Institutional Buyer Rule 230.144A(a)(I - vi)\n",
    "\n",
    "# Once you play around with those, I can send some more complicated ones that cross reference other rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_str = get_csv_string(dfs)\n",
    "collection = []\n",
    "for value_to_extract in gt_collect:\n",
    "    # if value_to_extract==\"Selling Costs\":\n",
    "        # continue\n",
    "    question = f\"Extract the value for the variable name '{value_to_extract}'?\"\n",
    "    print(question)\n",
    "    pred = finetune_program(question, verbose=True)\n",
    "    print(pred.answer)\n",
    "    collection.append((pred, f\"{value_to_extract}: {gt_collect[value_to_extract]}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train_utils import operators_dict, range_description_json\n",
    "from models_testing import SpreadSheetAnalyzer\n",
    "spreadsheeet_ananlyst = SpreadSheetAnalyzer(range_description_json, operators_dict, num_passages=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_str = get_csv_string(dfs)\n",
    "collection = []\n",
    "for value_to_extract in gt_collect:\n",
    "    # if value_to_extract==\"Selling Costs\":\n",
    "        # continue\n",
    "    question = f\"Extract the value for the variable name '{value_to_extract}'.\"\n",
    "    print(question)\n",
    "    pred = spreadsheeet_ananlyst(question, verbose=True)\n",
    "    print(pred.answer)\n",
    "    collection.append((pred, f\"{value_to_extract}: {gt_collect[value_to_extract]}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "query_engine_tools = QueryEngineTool(\n",
    "    query_engine=query_engine,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"value_retriever\",\n",
    "        description=\"provides useful information about a query.\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# query_engine_tools=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Literal\n",
    "from llama_index.core.bridge.pydantic import BaseModel, Field\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def adding_values(values: List[float]):\n",
    "    return sum(values)\n",
    "\n",
    "class AddingArgs(BaseModel):\n",
    "    values: List = Field(\n",
    "        description=\"A list of values to add together.\"\n",
    "    )\n",
    "\n",
    "adding_tool = FunctionTool.from_defaults(\n",
    "    fn=adding_values,\n",
    "    name=\"sum_values\",\n",
    "    description=\"Add values together\",\n",
    "    fn_schema=AddingArgs,\n",
    ")\n",
    "\n",
    "# query_engine_tools = [adding_tool]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents import (\n",
    "    AgentService,\n",
    "    ToolService,\n",
    "    LocalLauncher,\n",
    "    MetaServiceTool,\n",
    "    ControlPlaneServer,\n",
    "    SimpleMessageQueue,\n",
    "    AgentOrchestrator,\n",
    ")\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import ReActAgentWorker, ReActAgent\n",
    "\n",
    "# create our multi-agent framework components\n",
    "message_queue = SimpleMessageQueue()\n",
    "control_plane = ControlPlaneServer(\n",
    "    message_queue=message_queue,\n",
    "    orchestrator=AgentOrchestrator(llm=Settings.llm),\n",
    "    vector_store=vector_index.storage_context.vector_store,\n",
    "    # port=8001\n",
    ")\n",
    "\n",
    "# define Tool Service\n",
    "tool_service = ToolService(\n",
    "    message_queue=message_queue,\n",
    "    tools=[query_engine_tools],\n",
    "    running=True,\n",
    "    step_interval=0.5,\n",
    ")\n",
    "\n",
    "# define meta-tools here\n",
    "meta_tools = [\n",
    "    await MetaServiceTool.from_tool_service(\n",
    "        t.metadata.name,\n",
    "        message_queue=message_queue,\n",
    "        tool_service=tool_service,\n",
    "    )\n",
    "    for t in [query_engine_tools]\n",
    "]\n",
    "\n",
    "\n",
    "# define Agent and agent service\n",
    "worker1 = FunctionCallingAgent.from_tools(\n",
    "worker1 = ReActAgentWorker.from_tools(\n",
    "# worker1 = ReActAgent.from_tools(\n",
    "    # [query_engine_tools],\n",
    "    meta_tools,\n",
    "    # tool_service,\n",
    "    llm=hf_llm,\n",
    "    # max_iterations=15\n",
    ")\n",
    "agent1 = worker1.as_agent()\n",
    "# agent_server_1 = AgentService(\n",
    "#     agent=agent1,\n",
    "#     message_queue=message_queue,\n",
    "#     description=\"Used to retrieve values from a spreadsheet that has been converted to a string.\",\n",
    "#     service_name=\"spreadsheet_reader_agent\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker2 = ReActAgent.from_llm(llm=hf_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = worker2.query(\"What is the value for Selling Costs? It is a value between 0 and 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "launcher = LocalLauncher(\n",
    "    [agent_server_1, tool_service],\n",
    "    control_plane,\n",
    "    message_queue,\n",
    ")\n",
    "query_str = \"What is the Disposition Fee?\"\n",
    "result = launcher.launch_single(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.readers.file import PandasExcelReader\n",
    "from llama_index.core import set_global_tokenizer\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "from transformers import AutoTokenizer\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# set_global_tokenizer(\n",
    "#     AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\").encode\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "filepath = \"/workspace/data/MASTER - PYTHON - SCORING MODEL - MCG MADISON RIDGE DST - v2.0.xlsx\"\n",
    "documents = PandasExcelReader(sheet_name=\"5 - Disposition Analysis\").load_data(filepath)\n",
    "# documents[0].text = documents[0].text.split(\"\\n\")\n",
    "# import os\n",
    "# os.environ['HF_TOKEN']=access_token\n",
    "# model_name = \"jmars/trithemius-mistral-0.3-7b\"\n",
    "# model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "tokenizer_name = model_name\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "hf_llm = HuggingFaceLLM(model=model, tokenizer_name=tokenizer_name, is_chat_model=True)\n",
    "Settings.llm = hf_llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Literal\n",
    "from llama_index.core.bridge.pydantic import BaseModel, Field\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.base.llms.types import (\n",
    "    ChatMessage,\n",
    "    MessageRole,\n",
    ")\n",
    "\n",
    "\n",
    "def adding_values(values: List[float]):\n",
    "    return sum(values)\n",
    "\n",
    "\n",
    "class AddingArgs(BaseModel):\n",
    "    values: List = Field(\n",
    "        description=\"A list of values to add together.\"\n",
    "    )\n",
    "\n",
    "adding_tool = FunctionTool.from_defaults(\n",
    "    fn=adding_values,\n",
    "    name=\"sum_values\",\n",
    "    description=\"Add a list of values together\",\n",
    "    fn_schema=AddingArgs,\n",
    ")\n",
    "\n",
    "data=documents[0].text\n",
    "usr_msg = ChatMessage(\n",
    "    role=MessageRole.USER,\n",
    "    # content=f\"What is the sum of Disposition Fee percentage and Sales Cost percentage from this spreadsheet?\\n\\n##SPREADSHEET\\n{data}\",\n",
    "    content=f\"Extract the percentage values for 'Disposition Fee' and 'Sales Cost' from this spreadsheet?\\n\\n##SPREADSHEET\\n{data}\",\n",
    ")\n",
    "\n",
    "response = hf_llm.chat(\n",
    "    messages=[usr_msg],\n",
    "    tools=[\n",
    "        adding_tool\n",
    "    ],\n",
    "    tool_choice=\"add_values\",\n",
    ")\n",
    "\n",
    "print(response.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.readers.file import PandasExcelReader\n",
    "from llama_index.core import set_global_tokenizer\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "from transformers import AutoTokenizer\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "os.environ['HF_TOKEN']=access_token\n",
    "\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "tokenizer_name = model_name\n",
    "embed_model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "hf_llm = HuggingFaceLLM(model_name=model_name, tokenizer_name=tokenizer_name, is_chat_model=True, device_map='auto', max_new_tokens=2000, context_window=8000)\n",
    "set_global_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(tokenizer_name).encode  # pass in the HuggingFace model org + repo\n",
    ")\n",
    "# hf_llm = HuggingFaceLLM(model_name=model_name, tokenizer_name=tokenizer_name)\n",
    "# hf_llm = HuggingFaceInferenceAPI(model_name=model_name, tokenizer_name=tokenizer_name, is_chat_model=True, is_function_calling_model=True)\n",
    "embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "\n",
    "filepath = \"/workspace/data/MASTER - PYTHON - SCORING MODEL - MCG MADISON RIDGE DST - v2.0.xlsx\"\n",
    "documents = PandasExcelReader(sheet_name=\"5 - Disposition Analysis\").load_data(filepath)\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "# vector_index.storage_context.persist(persist_dir=\"/workspace/data/storage/alpha\")\n",
    "query_engine = vector_index.as_query_engine(llm=hf_llm, top_k=3)\n",
    "\n",
    "Settings.llm = hf_llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from typing import List, Literal\n",
    "from llama_index.core.bridge.pydantic import BaseModel, Field\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.base.llms.types import (\n",
    "    ChatMessage,\n",
    "    MessageRole,\n",
    ")\n",
    "\n",
    "\n",
    "query_engine_tools = QueryEngineTool(\n",
    "    query_engine=query_engine,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"spreadsheet_value_retriever\",\n",
    "        description=\"contains the information of a spreadsheet, and is useful for retrieving specific values from a spreadsheet\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "def adding_values(values: List[float]):\n",
    "    return sum(values)\n",
    "\n",
    "\n",
    "class AddingArgs(BaseModel):\n",
    "    values: List = Field(\n",
    "        description=\"A list of values to add together.\"\n",
    "    )\n",
    "\n",
    "adding_tool = FunctionTool.from_defaults(\n",
    "    fn=adding_values,\n",
    "    name=\"sum_values\",\n",
    "    description=\"Add a list of values together\",\n",
    "    fn_schema=AddingArgs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents import (\n",
    "    AgentService,\n",
    "    ToolService,\n",
    "    LocalLauncher,\n",
    "    MetaServiceTool,\n",
    "    ControlPlaneServer,\n",
    "    SimpleMessageQueue,\n",
    "    AgentOrchestrator,\n",
    ")\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import ReActAgentWorker, ReActAgent\n",
    "\n",
    "\n",
    "\n",
    "# create our multi-agent framework components\n",
    "message_queue = SimpleMessageQueue()\n",
    "control_plane = ControlPlaneServer(\n",
    "    message_queue=message_queue,\n",
    "    orchestrator=AgentOrchestrator(llm=hf_llm),\n",
    ")\n",
    "\n",
    "# define Tool Service\n",
    "tool_service = ToolService(\n",
    "    message_queue=message_queue,\n",
    "    tools=[query_engine_tools],#, adding_tool],\n",
    "    running=True,\n",
    "    step_interval=0.5,\n",
    ")\n",
    "\n",
    "# define meta-tools here\n",
    "meta_tools = [\n",
    "    await MetaServiceTool.from_tool_service(\n",
    "        t.metadata.name,\n",
    "        message_queue=message_queue,\n",
    "        tool_service=tool_service,\n",
    "    )\n",
    "    for t in [query_engine_tools]#, adding_tool]\n",
    "]\n",
    "\n",
    "\n",
    "# define Agent and agent service\n",
    "# worker1 = FunctionCallingAgentWorker.from_tools(\n",
    "worker1 = ReActAgentWorker.from_tools(\n",
    "    meta_tools,\n",
    "    llm=hf_llm,\n",
    ")\n",
    "agent1 = worker1.as_agent()\n",
    "agent_server_1 = AgentService(\n",
    "    agent=agent1,\n",
    "    message_queue=message_queue,\n",
    "    description=\"Used to answer questions over Uber and Lyft 10K documents\",\n",
    "    service_name=\"uber_lyft_10k_analyst_agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "launcher = LocalLauncher(\n",
    "    [agent_server_1, tool_service],\n",
    "    control_plane,\n",
    "    message_queue,\n",
    ")\n",
    "query_str = \"What is the Disposition Fee?\"\n",
    "result = launcher.launch_single(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgentWorker, ReActAgent\n",
    "agent = ReActAgent.from_tools(\n",
    "    [query_engine_tools, adding_tool],\n",
    "    llm=hf_llm,\n",
    "    verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = ['Selling Costs',\n",
    "  'Disposition Fee',\n",
    "  'Net Operating Income',\n",
    "  'Loan Assumption/Payoff',\n",
    "  'Return of Forecasted Reserves',\n",
    "  'CF Y 11',\n",
    "  'Return of Maximum Offering Amount',\n",
    "  'Projected Terminal Cap Rate',\n",
    "  'Cash Flows']\n",
    "content='Retrieve the following values from the spreadsheet: Selling Costs, Disposition Fee, Net Operating Income, Loan Assumption/Payoff, Return of Forecasted Reserves, CF Y 11, Return of Maximum Offering Amount, Projected Terminal Cap Rate, Cash Flows (categories 1 through 9)\\nThen add Disposition Fee and Selling Cost together.'\n",
    "\n",
    "usr_msg = ChatMessage(\n",
    "    role=MessageRole.ASSISTANT,\n",
    "    content=content\n",
    ")\n",
    "response = agent1.chat(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content='Retrieve the following values from the spreadsheet: Selling Costs, Disposition Fee, Net Operating Income, Loan Assumption/Payoff, Return of Forecasted Reserves, CF Y 11, Return of Maximum Offering Amount, Projected Terminal Cap Rate, Cash Flows (categories 1 through 9)\\nThen add Disposition Fee and Selling Cost together.'\n",
    "\n",
    "messages = [\n",
    "        {\"role\": \"user\", \"content\": content},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Forward NOI Growth \t2.00%\n",
    "#  Selling Costs \t1.00%\n",
    "#  Disposition Fee \t2.50%\n",
    "\t\n",
    "# \tAssumes. 0-yr Hold\n",
    "# \tScenario A\n",
    "# Net Operating Income\t 4,644,391 \n",
    "# Projected Terminal Cap Rate\t5.25%\n",
    "# Projected Sales Price (95%)\t 88,464,592 \n",
    "# Loan Assumption/Payoff\t -   \n",
    "# Selling Costs\t (884,646)\n",
    "# Disposition Fee\t (2,211,615)\n",
    "# Return of Forecasted Reserves\t -   \n",
    "# Sale Proceeds\t 85,368,331 \n",
    "# Proceeds from Distributions\t 36,688,942 \n",
    "# Return of Maximum Offering Amount\t (77,670,567)\n",
    "# DST Total Gain / (Loss)\t 44,386,707 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import logging\n",
    "\n",
    "# create a tool\n",
    "def get_the_secret_fact() -> str:\n",
    "    \"\"\"Returns the secret fact.\"\"\"\n",
    "    return \"The secret fact is: A baby llama is called a 'Cria'.\"\n",
    "\n",
    "tool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n",
    "\n",
    "# Define an agent\n",
    "model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "llm = HuggingFaceLLM(model_name=model_name)\n",
    "worker = FunctionCallingAgentWorker.from_tools([tool], llm=llm)\n",
    "agent = worker.as_agent()\n",
    "\n",
    "# Create an agent service\n",
    "agent_service = AgentService(\n",
    "    agent=agent,\n",
    "    message_queue=message_queue,\n",
    "    description=\"General purpose assistant\",\n",
    "    service_name=\"assistant\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
