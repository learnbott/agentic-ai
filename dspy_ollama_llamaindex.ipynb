{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!cp '/workspace/repos/agentic-ai/PPM - MCG MADISON RIDGE DST.pdf' /workspace/data\n",
    "!cp '/workspace/repos/agentic-ai/MASTER - PYTHON - SCORING MODEL - MCG MADISON RIDGE DST - v2.0.xlsx' /workspace/data\n",
    "!pip3 install llama-index llama-parse llama-agents llama-index-llms-huggingface llama-index-embeddings-huggingface llama-index-llms-huggingface-api llama-index-llms-ollama llama-index-embeddings-ollama\n",
    "!pip3 install openpyxl sentencepiece protobuf evaluate rouge_score absl-py tensorboardX bitsandbytes peft accelerate python-dotenv dspy-ai\n",
    "!curl -L https://ollama.com/download/ollama-linux-amd64 -o /usr/bin/ollama\n",
    "!chmod +x /usr/bin/ollama\n",
    "!useradd -r -s /bin/false -m -d /usr/share/ollama ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip3 install InstructorEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install tmux, open new screen and run `ollama start`, and then `ollama pull qwen2:1.5b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "import dspy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from train_utils import get_csv_string, randomize_row_values, operators_dict, range_description_json, split_df_by_empty_columns, split_df_by_empty_rows, print_trainable_parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# model_name = \"mistral:latest\"\n",
    "model_name = \"llama3.1\"\n",
    "# model_name = \"qwen2:1.5b\"\n",
    "# model_name = \"gemma:1.5b\"\n",
    "llm = Ollama(model=model_name, url=\"http://127.0.0.1:11434\")\n",
    "llm.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /workspace/data && curl -X GET \"https://www.ecfr.gov/api/versioner/v1/full/2024-07-23/title-17.xml?chapter=II&part=200&subpart=A&section=200.1\" -H \"accept: application/xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /workspace/data && curl -X GET \"https://www.ecfr.gov/api/versioner/v1/full/2024-07-23/title-17.xml?chapter=II&part=200\" -H \"accept: application/xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Path to your XML file\n",
    "# xml_file_path = '/workspace/data/section2001.xml'\n",
    "xml_file_path = '/workspace/data/part200.xml'\n",
    "\n",
    "# Parse the XML file\n",
    "tree = ET.parse(xml_file_path)\n",
    "\n",
    "# Get the root element of the XML document\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out[\"['1PART 200—ORGANIZATION; CONDUCT AND ETHICS; AND\\\\nINFORMATION AND REQUESTS\\\\n', '2Subpart A—Organization and Program Management', '3§ 200.1 General statement and statutory authority.']\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_sections(s):\n",
    "    return s[2:-2].split(\"', '\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(['1PART 200—ORGANIZATION; CONDUCT AND ETHICS; AND\\\\nINFORMATION AND REQUESTS\\\\n',\n",
    "  '2Subpart A—Organization and Program Management',\n",
    "  '3§ 200.1 General statement and statutory authority.'],\n",
    " ['1PART 200—ORGANIZATION; CONDUCT AND ETHICS; AND\\\\nINFORMATION AND REQUESTS\\\\n',\n",
    "  '2Subpart A—Organization and Program Management',\n",
    "  '3§ 200.2 Statutory functions.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"['1PART 200—ORGANIZATION; CONDUCT AND ETHICS; AND\\\\nINFORMATION AND REQUESTS\\\\n', '2Subpart A—Organization and Program Management', '3§ 200.1 General statement and statutory authority.']\"\n",
    "test2 = \"['1PART 200—ORGANIZATION; CONDUCT AND ETHICS; AND\\\\nINFORMATION AND REQUESTS\\\\n', '2Subpart A—Organization and Program Management', '3§ 200.2 Statutory functions.']\"\n",
    "get_list_of_sections(test), get_list_of_sections(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"METADATA\")\n",
    "for i,t in enumerate(get_list_of_sections(test)):\n",
    "    if i==0:\n",
    "        print(\"[\\n\",t)\n",
    "    else:\n",
    "        print(t)\n",
    "print(\"]\")\n",
    "print()\n",
    "print(\"TEXT\\n\", out[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(out.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(element, depth=0, prev_depth=-1, lists=None, final_dict=None):\n",
    "    if lists is None:\n",
    "        lists = [[]]\n",
    "    if final_dict is None:\n",
    "        final_dict = {}\n",
    "\n",
    "    if element.tag == \"HEAD\":\n",
    "        # print(depth, prev_depth)\n",
    "        # Check if depth is increasing\n",
    "        if depth > prev_depth:\n",
    "            lists[-1].append(f\"{depth}{element.text}\")\n",
    "        else:\n",
    "            lists.append(lists[-1][:depth-1])\n",
    "            lists[-1].append(f\"{depth}{element.text}\")\n",
    "        \n",
    "    if element.tag == \"P\":\n",
    "        try:\n",
    "            final_dict[str(lists[-1])] += \"\\n\" + ''.join(element.itertext())\n",
    "        except:\n",
    "            final_dict[str(lists[-1])] = ''.join(element.itertext())\n",
    "\n",
    "    # Recursively call print_tree on each child, increasing the depth\n",
    "    for child in element:\n",
    "        print_tree(child, depth + 1, len(lists[-1]), lists, final_dict)\n",
    "\n",
    "    return final_dict\n",
    "\n",
    "# Example call to print_tree\n",
    "out = print_tree(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree2(element, depth=0):\n",
    "    # Print current element tag with indentation based on depth\n",
    "    # print('  ' * depth + f\"{depth}{element.tag}\")\n",
    "    if element.tag==\"P\": \n",
    "        ptext = ''.join(element.itertext())\n",
    "        print(ptext)\n",
    "    # print('  ' * depth + f\"{depth}{element.tag}\" +f\" {element.text}\")\n",
    "    # Recursively call print_tree on each child, increasing the depth\n",
    "    for child in element:\n",
    "        print_tree2(child, depth + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree2(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all <P> elements\n",
    "paragraphs = root.findall('.//HEAD')\n",
    "\n",
    "# Extract and print the text from each <P> element\n",
    "for p in paragraphs:\n",
    "    # If you want to include text from child elements (like <b>), use 'itertext()'\n",
    "    text = ''.join(p.itertext())\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"qwen2:1.5b\", \n",
    "# model_name = \"mistral:latest\", 32000\n",
    "model_name, num_ctx = \"llama3.1\", 128000\n",
    "llm = dspy.OllamaLocal(model=model_name, max_tokens=4000, num_ctx=num_ctx, model_type='chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/workspace/data/MASTER - PYTHON - SCORING MODEL - MCG MADISON RIDGE DST - v2.0.xlsx\"\n",
    "disposition_inputs = [\n",
    "  \"Selling Costs\",\n",
    "  \"Disposition Fee\",\n",
    "  \"Net Operating Income\",\n",
    "  \"Loan Assumption/Payoff\",\n",
    "  \"Return of Forecasted Reserves\",\n",
    "  \"CF Y 11\",\n",
    "  \"Return of Maximum Offering Amount\",\n",
    "  \"Projected Terminal Cap Rate\",\n",
    "  \"Cash Flows\"\n",
    "]\n",
    "dfs = pd.read_excel(filepath, sheet_name=\"5 - Disposition Analysis\", header=None)\n",
    "# Splitting the DataFrame by empty columns\n",
    "sub_dfs_by_columns = split_df_by_empty_columns(dfs)\n",
    "\n",
    "# Splitting each sub-DataFrame by empty rows\n",
    "final_split_dfs = []\n",
    "for sub_df in sub_dfs_by_columns:\n",
    "    split_sub_dfs = split_df_by_empty_rows(sub_df)\n",
    "    final_split_dfs.extend([get_csv_string(x) for x in split_sub_dfs if not x.empty])\n",
    "\n",
    "dfs.dropna(axis=0, how='all', inplace=True)\n",
    "dfs.dropna(axis=1, how='all', inplace=True)\n",
    "fee_columns = ['Disposition Fee', 'Selling Costs']\n",
    "cashflow_columns = [1,2,3,4,5,6,7,8,9]\n",
    "ground_truth = dfs[dfs[1].isin(disposition_inputs+cashflow_columns)].iloc[:, :2] # Get only the necessary columns\n",
    "ground_truth.drop(labels=[16, 17], axis=0, inplace=True) # drop the duplicate Selling and Disposition Costs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.embeddings.instructor import InstructorEmbedding\n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.readers.file import PandasExcelReader\n",
    "from llama_index.core import set_global_tokenizer, Document\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "from transformers import AutoTokenizer\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "access_token = os.getenv('HF_TOKEN')\n",
    "llama_api_key = os.getenv('LLAMA_API_KEY')\n",
    "\n",
    "\n",
    "# filepath = \"/workspace/data/MASTER - PYTHON - SCORING MODEL - MCG MADISON RIDGE DST - v2.0.xlsx\"\n",
    "# documents = PandasExcelReader(sheet_name=\"5 - Disposition Analysis\").load_data(filepath)\n",
    "documents = [Document(text=t) for t in final_split_dfs]\n",
    "\n",
    "Settings.llm = llm\n",
    "\n",
    "Settings.chunk_size = 100\n",
    "Settings.chunk_overlap = 25\n",
    "# embed_model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "embed_model_name = \"hkunlp/instructor-base\"\n",
    "# embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "embed_model = InstructorEmbedding(model_name=embed_model_name)\n",
    "vector_index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "# vector_index.storage_context.persist(persist_dir=\"/workspace/data/storage/alpha\")\n",
    "query_engine = vector_index.as_retriever(similarity_top_k=2)\n",
    "Settings.embed_model = embed_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.retrieve.llama_index_rm import LlamaIndexRM\n",
    "retriever = LlamaIndexRM(query_engine)\n",
    "dspy.settings.configure(lm=llm, rm=retriever)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spreadvars = ['Disposition Fee', 'Net Operating Income', 'Projected Terminal Cap Rate', 'Return of Forecasted Reserves', 'Return of Maximum Offering Amount']\n",
    "query_str = f\"For each of the variables return their value from the spreadsheet in this format 'variable: ' 'value'. If there are any duplicate variables then choose the first instance. Here are the variables: {spreadvars}.\"\n",
    "response = query_engine.query(query_str)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_collect = {}\n",
    "for row,col in ground_truth.iterrows():\n",
    "    if isinstance(col.values[0], int):\n",
    "        name = f\"Cashflows {col.values[0]}\"\n",
    "    else:\n",
    "        name = col.values[0]\n",
    "    value = col.values[1]\n",
    "    gt_collect[name] = str(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# dfs_str = get_csv_string(dfs)\n",
    "num_rounds = 2\n",
    "train_data = []\n",
    "for _ in range(num_rounds):\n",
    "    # TODO: gradually increase n_samples, random fill in of values in range\n",
    "    # dfs_aug = randomize_row_values(dfs, ground_truth=ground_truth, n_samples=15)\n",
    "    # dfs_str = get_csv_string(dfs_aug)\n",
    "    # dfs_str = get_csv_string(dfs)\n",
    "    \n",
    "    for value_to_extract in gt_collect:\n",
    "\n",
    "        question = f\"Extract the value for the variable name '{value_to_extract}'.\"\n",
    "        answer = f\"{value_to_extract}: {gt_collect[value_to_extract]}\"\n",
    "        train_data.append(dspy.Example(question=question, answer=answer).with_inputs('question'))\n",
    "        # train_data.append(dspy.Example(long_text=question, answer=answer).with_inputs('question'))\n",
    "    \n",
    "# random.shuffle(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train_utils import operators_dict, range_description_json\n",
    "from models_testing import SpreadSheetAnalyzer\n",
    "spreadsheeet_ananlyst = SpreadSheetAnalyzer(range_description_json, operators_dict, num_passages=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch, BootstrapFinetune, BootstrapFewShot, MIPRO, MIPROv2\n",
    "\n",
    "perc_train = 0.7\n",
    "num_train = int(len(train_data) * perc_train)\n",
    "def validate_answer(pred, example, trace=None):\n",
    "    return example.answer.lower() == pred.answer.lower()\n",
    "metric = dspy.evaluate.metrics.answer_exact_match\n",
    "# metric = dspy.evaluate.metrics.answer_passage_match\n",
    "# metric = validate_answer\n",
    "NUM_THREADS=1\n",
    "TRAIN_NUM=17\n",
    "\n",
    "\n",
    "#Compile program on BootstrapFinetune\n",
    "finetune_optimizer = MIPROv2(prompt_model=llm, task_model=llm, metric=metric, num_candidates=10, init_temperature=1.2, minibatch_size=1)\n",
    "kwargs = dict(num_threads=NUM_THREADS, display_progress=True, display_table=0)\n",
    "finetune_program = finetune_optimizer.compile(spreadsheeet_ananlyst, trainset=train_data[:TRAIN_NUM], \n",
    "                                                 num_batches=20, max_bootstrapped_demos=3, max_labeled_demos=5, \n",
    "                                                 eval_kwargs=kwargs, requires_permission_to_run=False)\n",
    "\n",
    "# finetune_optimizer = MIPRO(prompt_model=llm, task_model=llm, metric=metric, num_candidates=10, init_temperature=1.2)\n",
    "# kwargs = dict(num_threads=NUM_THREADS, display_progress=True, display_table=0)\n",
    "# compiled_prompt_opt = finetune_optimizer.compile(spreadsheeet_ananlyst, trainset=train_data[:TRAIN_NUM], num_trials=100, \n",
    "#                                                  max_bootstrapped_demos=3, max_labeled_demos=5, eval_kwargs=kwargs,\n",
    "#                                                  requires_permission_to_run=False)\n",
    "\n",
    "# config = dict(epochs=3, bf16=True, bsize=1, accumsteps=3, lr=7e-5) #path_prefix=None\n",
    "# finetune_optimizer = BootstrapFinetune(metric=metric)\n",
    "# finetune_program = finetune_optimizer.compile(spreadsheeet_ananlyst, trainset=train_data[:20], **config)\n",
    "\n",
    "# finetune_optimizer = BootstrapFewShot(metric=metric, max_bootstrapped_demos=8, max_labeled_demos=8)\n",
    "# finetune_program = finetune_optimizer.compile(spreadsheeet_ananlyst, trainset=train_data)\n",
    "\n",
    "# #Load program and activate model's parameters in program before evaluation\n",
    "# ckpt_path = \"saved_checkpoint_path_from_finetuning\"\n",
    "# LM = dspy.HFModel(checkpoint=ckpt_path, model=model_name)\n",
    "\n",
    "# for p in finetune_program.predictors():\n",
    "#     p.lm = LM\n",
    "#     p.activated = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_str = get_csv_string(dfs)\n",
    "collection = []\n",
    "for value_to_extract in gt_collect:\n",
    "    # if value_to_extract==\"Selling Costs\":\n",
    "        # continue\n",
    "    question = f\"Extract the value for the variable name '{value_to_extract}'?\"\n",
    "    print(question)\n",
    "    pred = finetune_program(question, verbose=True)\n",
    "    print(pred.answer)\n",
    "    collection.append((pred, f\"{value_to_extract}: {gt_collect[value_to_extract]}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train_utils import operators_dict, range_description_json\n",
    "from models_testing import SpreadSheetAnalyzer\n",
    "spreadsheeet_ananlyst = SpreadSheetAnalyzer(range_description_json, operators_dict, num_passages=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_str = get_csv_string(dfs)\n",
    "collection = []\n",
    "for value_to_extract in gt_collect:\n",
    "    # if value_to_extract==\"Selling Costs\":\n",
    "        # continue\n",
    "    question = f\"Extract the value for the variable name '{value_to_extract}'.\"\n",
    "    print(question)\n",
    "    pred = spreadsheeet_ananlyst(question, verbose=True)\n",
    "    print(pred.answer)\n",
    "    collection.append((pred, f\"{value_to_extract}: {gt_collect[value_to_extract]}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "query_engine_tools = QueryEngineTool(\n",
    "    query_engine=query_engine,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"value_retriever\",\n",
    "        description=\"provides useful information about a query.\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# query_engine_tools=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Literal\n",
    "from llama_index.core.bridge.pydantic import BaseModel, Field\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def adding_values(values: List[float]):\n",
    "    return sum(values)\n",
    "\n",
    "class AddingArgs(BaseModel):\n",
    "    values: List = Field(\n",
    "        description=\"A list of values to add together.\"\n",
    "    )\n",
    "\n",
    "adding_tool = FunctionTool.from_defaults(\n",
    "    fn=adding_values,\n",
    "    name=\"sum_values\",\n",
    "    description=\"Add values together\",\n",
    "    fn_schema=AddingArgs,\n",
    ")\n",
    "\n",
    "# query_engine_tools = [adding_tool]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents import (\n",
    "    AgentService,\n",
    "    ToolService,\n",
    "    LocalLauncher,\n",
    "    MetaServiceTool,\n",
    "    ControlPlaneServer,\n",
    "    SimpleMessageQueue,\n",
    "    AgentOrchestrator,\n",
    ")\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import ReActAgentWorker, ReActAgent\n",
    "\n",
    "# create our multi-agent framework components\n",
    "message_queue = SimpleMessageQueue()\n",
    "control_plane = ControlPlaneServer(\n",
    "    message_queue=message_queue,\n",
    "    orchestrator=AgentOrchestrator(llm=Settings.llm),\n",
    "    vector_store=vector_index.storage_context.vector_store,\n",
    "    # port=8001\n",
    ")\n",
    "\n",
    "# define Tool Service\n",
    "tool_service = ToolService(\n",
    "    message_queue=message_queue,\n",
    "    tools=[query_engine_tools],\n",
    "    running=True,\n",
    "    step_interval=0.5,\n",
    ")\n",
    "\n",
    "# define meta-tools here\n",
    "meta_tools = [\n",
    "    await MetaServiceTool.from_tool_service(\n",
    "        t.metadata.name,\n",
    "        message_queue=message_queue,\n",
    "        tool_service=tool_service,\n",
    "    )\n",
    "    for t in [query_engine_tools]\n",
    "]\n",
    "\n",
    "\n",
    "# define Agent and agent service\n",
    "worker1 = FunctionCallingAgent.from_tools(\n",
    "worker1 = ReActAgentWorker.from_tools(\n",
    "# worker1 = ReActAgent.from_tools(\n",
    "    # [query_engine_tools],\n",
    "    meta_tools,\n",
    "    # tool_service,\n",
    "    llm=hf_llm,\n",
    "    # max_iterations=15\n",
    ")\n",
    "agent1 = worker1.as_agent()\n",
    "# agent_server_1 = AgentService(\n",
    "#     agent=agent1,\n",
    "#     message_queue=message_queue,\n",
    "#     description=\"Used to retrieve values from a spreadsheet that has been converted to a string.\",\n",
    "#     service_name=\"spreadsheet_reader_agent\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker2 = ReActAgent.from_llm(llm=hf_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = worker2.query(\"What is the value for Selling Costs? It is a value between 0 and 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "launcher = LocalLauncher(\n",
    "    [agent_server_1, tool_service],\n",
    "    control_plane,\n",
    "    message_queue,\n",
    ")\n",
    "query_str = \"What is the Disposition Fee?\"\n",
    "result = launcher.launch_single(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.readers.file import PandasExcelReader\n",
    "from llama_index.core import set_global_tokenizer\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "from transformers import AutoTokenizer\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# set_global_tokenizer(\n",
    "#     AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\").encode\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "filepath = \"/workspace/data/MASTER - PYTHON - SCORING MODEL - MCG MADISON RIDGE DST - v2.0.xlsx\"\n",
    "documents = PandasExcelReader(sheet_name=\"5 - Disposition Analysis\").load_data(filepath)\n",
    "# documents[0].text = documents[0].text.split(\"\\n\")\n",
    "# import os\n",
    "# os.environ['HF_TOKEN']=access_token\n",
    "# model_name = \"jmars/trithemius-mistral-0.3-7b\"\n",
    "# model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "tokenizer_name = model_name\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "hf_llm = HuggingFaceLLM(model=model, tokenizer_name=tokenizer_name, is_chat_model=True)\n",
    "Settings.llm = hf_llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Literal\n",
    "from llama_index.core.bridge.pydantic import BaseModel, Field\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.base.llms.types import (\n",
    "    ChatMessage,\n",
    "    MessageRole,\n",
    ")\n",
    "\n",
    "\n",
    "def adding_values(values: List[float]):\n",
    "    return sum(values)\n",
    "\n",
    "\n",
    "class AddingArgs(BaseModel):\n",
    "    values: List = Field(\n",
    "        description=\"A list of values to add together.\"\n",
    "    )\n",
    "\n",
    "adding_tool = FunctionTool.from_defaults(\n",
    "    fn=adding_values,\n",
    "    name=\"sum_values\",\n",
    "    description=\"Add a list of values together\",\n",
    "    fn_schema=AddingArgs,\n",
    ")\n",
    "\n",
    "data=documents[0].text\n",
    "usr_msg = ChatMessage(\n",
    "    role=MessageRole.USER,\n",
    "    # content=f\"What is the sum of Disposition Fee percentage and Sales Cost percentage from this spreadsheet?\\n\\n##SPREADSHEET\\n{data}\",\n",
    "    content=f\"Extract the percentage values for 'Disposition Fee' and 'Sales Cost' from this spreadsheet?\\n\\n##SPREADSHEET\\n{data}\",\n",
    ")\n",
    "\n",
    "response = hf_llm.chat(\n",
    "    messages=[usr_msg],\n",
    "    tools=[\n",
    "        adding_tool\n",
    "    ],\n",
    "    tool_choice=\"add_values\",\n",
    ")\n",
    "\n",
    "print(response.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.readers.file import PandasExcelReader\n",
    "from llama_index.core import set_global_tokenizer\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "from transformers import AutoTokenizer\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "os.environ['HF_TOKEN']=access_token\n",
    "\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "tokenizer_name = model_name\n",
    "embed_model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "hf_llm = HuggingFaceLLM(model_name=model_name, tokenizer_name=tokenizer_name, is_chat_model=True, device_map='auto', max_new_tokens=2000, context_window=8000)\n",
    "set_global_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(tokenizer_name).encode  # pass in the HuggingFace model org + repo\n",
    ")\n",
    "# hf_llm = HuggingFaceLLM(model_name=model_name, tokenizer_name=tokenizer_name)\n",
    "# hf_llm = HuggingFaceInferenceAPI(model_name=model_name, tokenizer_name=tokenizer_name, is_chat_model=True, is_function_calling_model=True)\n",
    "embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "\n",
    "filepath = \"/workspace/data/MASTER - PYTHON - SCORING MODEL - MCG MADISON RIDGE DST - v2.0.xlsx\"\n",
    "documents = PandasExcelReader(sheet_name=\"5 - Disposition Analysis\").load_data(filepath)\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "# vector_index.storage_context.persist(persist_dir=\"/workspace/data/storage/alpha\")\n",
    "query_engine = vector_index.as_query_engine(llm=hf_llm, top_k=3)\n",
    "\n",
    "Settings.llm = hf_llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from typing import List, Literal\n",
    "from llama_index.core.bridge.pydantic import BaseModel, Field\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.base.llms.types import (\n",
    "    ChatMessage,\n",
    "    MessageRole,\n",
    ")\n",
    "\n",
    "\n",
    "query_engine_tools = QueryEngineTool(\n",
    "    query_engine=query_engine,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"spreadsheet_value_retriever\",\n",
    "        description=\"contains the information of a spreadsheet, and is useful for retrieving specific values from a spreadsheet\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "def adding_values(values: List[float]):\n",
    "    return sum(values)\n",
    "\n",
    "\n",
    "class AddingArgs(BaseModel):\n",
    "    values: List = Field(\n",
    "        description=\"A list of values to add together.\"\n",
    "    )\n",
    "\n",
    "adding_tool = FunctionTool.from_defaults(\n",
    "    fn=adding_values,\n",
    "    name=\"sum_values\",\n",
    "    description=\"Add a list of values together\",\n",
    "    fn_schema=AddingArgs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents import (\n",
    "    AgentService,\n",
    "    ToolService,\n",
    "    LocalLauncher,\n",
    "    MetaServiceTool,\n",
    "    ControlPlaneServer,\n",
    "    SimpleMessageQueue,\n",
    "    AgentOrchestrator,\n",
    ")\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import ReActAgentWorker, ReActAgent\n",
    "\n",
    "\n",
    "\n",
    "# create our multi-agent framework components\n",
    "message_queue = SimpleMessageQueue()\n",
    "control_plane = ControlPlaneServer(\n",
    "    message_queue=message_queue,\n",
    "    orchestrator=AgentOrchestrator(llm=hf_llm),\n",
    ")\n",
    "\n",
    "# define Tool Service\n",
    "tool_service = ToolService(\n",
    "    message_queue=message_queue,\n",
    "    tools=[query_engine_tools],#, adding_tool],\n",
    "    running=True,\n",
    "    step_interval=0.5,\n",
    ")\n",
    "\n",
    "# define meta-tools here\n",
    "meta_tools = [\n",
    "    await MetaServiceTool.from_tool_service(\n",
    "        t.metadata.name,\n",
    "        message_queue=message_queue,\n",
    "        tool_service=tool_service,\n",
    "    )\n",
    "    for t in [query_engine_tools]#, adding_tool]\n",
    "]\n",
    "\n",
    "\n",
    "# define Agent and agent service\n",
    "# worker1 = FunctionCallingAgentWorker.from_tools(\n",
    "worker1 = ReActAgentWorker.from_tools(\n",
    "    meta_tools,\n",
    "    llm=hf_llm,\n",
    ")\n",
    "agent1 = worker1.as_agent()\n",
    "agent_server_1 = AgentService(\n",
    "    agent=agent1,\n",
    "    message_queue=message_queue,\n",
    "    description=\"Used to answer questions over Uber and Lyft 10K documents\",\n",
    "    service_name=\"uber_lyft_10k_analyst_agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "launcher = LocalLauncher(\n",
    "    [agent_server_1, tool_service],\n",
    "    control_plane,\n",
    "    message_queue,\n",
    ")\n",
    "query_str = \"What is the Disposition Fee?\"\n",
    "result = launcher.launch_single(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgentWorker, ReActAgent\n",
    "agent = ReActAgent.from_tools(\n",
    "    [query_engine_tools, adding_tool],\n",
    "    llm=hf_llm,\n",
    "    verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = ['Selling Costs',\n",
    "  'Disposition Fee',\n",
    "  'Net Operating Income',\n",
    "  'Loan Assumption/Payoff',\n",
    "  'Return of Forecasted Reserves',\n",
    "  'CF Y 11',\n",
    "  'Return of Maximum Offering Amount',\n",
    "  'Projected Terminal Cap Rate',\n",
    "  'Cash Flows']\n",
    "content='Retrieve the following values from the spreadsheet: Selling Costs, Disposition Fee, Net Operating Income, Loan Assumption/Payoff, Return of Forecasted Reserves, CF Y 11, Return of Maximum Offering Amount, Projected Terminal Cap Rate, Cash Flows (categories 1 through 9)\\nThen add Disposition Fee and Selling Cost together.'\n",
    "\n",
    "usr_msg = ChatMessage(\n",
    "    role=MessageRole.ASSISTANT,\n",
    "    content=content\n",
    ")\n",
    "response = agent1.chat(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content='Retrieve the following values from the spreadsheet: Selling Costs, Disposition Fee, Net Operating Income, Loan Assumption/Payoff, Return of Forecasted Reserves, CF Y 11, Return of Maximum Offering Amount, Projected Terminal Cap Rate, Cash Flows (categories 1 through 9)\\nThen add Disposition Fee and Selling Cost together.'\n",
    "\n",
    "messages = [\n",
    "        {\"role\": \"user\", \"content\": content},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Forward NOI Growth \t2.00%\n",
    "#  Selling Costs \t1.00%\n",
    "#  Disposition Fee \t2.50%\n",
    "\t\n",
    "# \tAssumes. 0-yr Hold\n",
    "# \tScenario A\n",
    "# Net Operating Income\t 4,644,391 \n",
    "# Projected Terminal Cap Rate\t5.25%\n",
    "# Projected Sales Price (95%)\t 88,464,592 \n",
    "# Loan Assumption/Payoff\t -   \n",
    "# Selling Costs\t (884,646)\n",
    "# Disposition Fee\t (2,211,615)\n",
    "# Return of Forecasted Reserves\t -   \n",
    "# Sale Proceeds\t 85,368,331 \n",
    "# Proceeds from Distributions\t 36,688,942 \n",
    "# Return of Maximum Offering Amount\t (77,670,567)\n",
    "# DST Total Gain / (Loss)\t 44,386,707 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import logging\n",
    "\n",
    "# create a tool\n",
    "def get_the_secret_fact() -> str:\n",
    "    \"\"\"Returns the secret fact.\"\"\"\n",
    "    return \"The secret fact is: A baby llama is called a 'Cria'.\"\n",
    "\n",
    "tool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n",
    "\n",
    "# Define an agent\n",
    "model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "llm = HuggingFaceLLM(model_name=model_name)\n",
    "worker = FunctionCallingAgentWorker.from_tools([tool], llm=llm)\n",
    "agent = worker.as_agent()\n",
    "\n",
    "# Create an agent service\n",
    "agent_service = AgentService(\n",
    "    agent=agent,\n",
    "    message_queue=message_queue,\n",
    "    description=\"General purpose assistant\",\n",
    "    service_name=\"assistant\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
