{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip3 install llama-index llama-parse llama-index-embeddings-huggingface accelerate dspy-ai openpyxl langchain chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip3 install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import dspy\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.datasets.hotpotqa import HotPotQA\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "# from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "# from llama_index.readers.file import PandasExcelReader\n",
    "# from llama_index.core.embeddings import resolve_embed_model\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "from typing import List, Any, Callable, Optional\n",
    "from pydantic import BaseModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class SentimentSummary(dspy.Signature):\n",
    "#     \"\"\"Given the sentiment label craft a sentiment summary of the given text so that the summary justifies the sentiment label.\"\"\"\n",
    "#     text = dspy.InputField(desc=\"Sentiment text\")\n",
    "#     sentiment_label = dspy.InputField(desc=\"Sentiment label\")\n",
    "#     answer = dspy.OutputField(desc=\"Sentiment label: {sentiment_label}\\n\\nSummary: {sentiment summary}\")\n",
    "\n",
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Parses through spreadsheet information and returns requested values.\"\"\"\n",
    "\n",
    "    context_str = dspy.InputField(desc=\"data in spreadsheet format.\")\n",
    "    query_str = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"the float value from the spreadsheet.\")\n",
    "\n",
    "\n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tool(BaseModel):\n",
    "    name: str\n",
    "    desc: str\n",
    "    input_variable: str\n",
    "    func: Callable\n",
    "    \n",
    "# class GiveTime:\n",
    "#     name = \"GiveTime\"\n",
    "#     input_variable = \"empty\"\n",
    "#     desc = \"takes an empty string and returns the current local time\"\n",
    "\n",
    "#     def __init__(self, k=3):\n",
    "#         pass\n",
    "   \n",
    "#     def __call__(self, *args, **kwargs):\n",
    "#         return datetime.datetime.now().strftime(\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_values(input_variable: List[float])->float:\n",
    "    return sum(input_variable)\n",
    "\n",
    "adding_tool = Tool(name=\"addition\", desc=\"add a list of floats together\", input_variable=\"input_variable\", func=add_values)\n",
    "\n",
    "# test_tools = [\n",
    "    # Tool(name=\"addition\", description=\"add a list of floats together\", requires=\"values\", func=add_values)\n",
    "    # Tool(name=\"local business lookup\", description=\"Look up businesses by category\", requires=\"business category\", func=lambda x: \"Bills landscaping: 415-555-5555\")\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first model load...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting model...\n",
      "reloading model...\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from dspy.retrieve.chromadb_rm import ChromadbRM\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from llama_index.readers.file import PandasExcelReader\n",
    "CHROMA_COLLECTION_NAME = \"blockchain_and_ai\"\n",
    "CHROMADB_DIR = \"/workspace/data/db/\"\n",
    "\n",
    "access_token = \"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\"\n",
    "# model_name = \"EleutherAI/gpt-neo-125m\"\n",
    "# model_name = \"clibrain/mamba-2.8b-instruct-openhermes\"\n",
    "# model_name = \"microsoft/Phi-3-mini-128k-instruct\" # 128K context window\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\" # 8K context window\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.3\" # 32K context window\n",
    "# model_name = \"clibrain/mamba-2.8b-instruct-openhermes\" # 8K context window\n",
    "# embed_model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "print('first model load...')\n",
    "model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "llm = dspy.HFModel(model=model_name, hf_device_map='auto', token=access_token)\n",
    "llm.kwargs['max_new_tokens']=1000\n",
    "llm.kwargs['do_sample']=True\n",
    "# llm.kwargs['typical_p']=0.9\n",
    "# llm.kwargs['temperature']=0.0\n",
    "\n",
    "\n",
    "print('deleting model...')\n",
    "llm.model=None\n",
    "gc.collect\n",
    "print('reloading model...')\n",
    "llm.model=AutoModelForCausalLM.from_pretrained(model_name, quantization_config=None, \n",
    "                                               trust_remote_code=True, device_map=\"auto\", \n",
    "                                               attn_implementation=\"flash_attention_2\",  \n",
    "                                               torch_dtype=torch.float16)\n",
    "\n",
    "# llm.model.generation_config.pad_token_id = llm.tokenizer.eos_token_id\n",
    "# llm.tokenizer.pad_token_id = llm.tokenizer.eos_token_id\n",
    "\n",
    "# retriever = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "# filepath = \"/workspace/data/MASTER - PYTHON - SCORING MODEL - MCG MADISON RIDGE DST - v2.0.xlsx\"\n",
    "# docs = PandasExcelReader(sheet_name=\"5 - Disposition Analysis\").load_data(filepath)\n",
    "# # documents = PandasExcelReader(sheet_name=\"5 - Disposition Analysis\").load_data(filepath)\n",
    "# # embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "# # vector_index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "# # vector_index.storage_context.persist(persist_dir=\"/workspace/data/storage/alpha\")\n",
    "# # retriever = vector_index.as_retriever(top_k=2)\n",
    "\n",
    "# chroma_client = chromadb.PersistentClient(path=CHROMADB_DIR)\n",
    "# collection = chroma_client.get_or_create_collection(name=CHROMA_COLLECTION_NAME)\n",
    "# text_splitter = SentenceTransformersTokenTextSplitter()\n",
    "\n",
    "# ids = []\n",
    "# documents = []\n",
    "# metadatas = []\n",
    "# chunks = text_splitter.create_documents([docs[0].text], )\n",
    "# for chunk_no, chunk in enumerate(chunks):\n",
    "#     ids.append(f\"{chunk_no}\")\n",
    "#     documents.append(chunk.page_content)\n",
    "#     # metadatas.append({\"title\":})\n",
    "# if ids:\n",
    "#     collection.upsert(ids=ids, documents=documents)#, metadatas=metadatas)\n",
    "\n",
    "# # collection.upsert(documents=[docs[0].text])\n",
    "\n",
    "\n",
    "# # default_ef = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "# default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "# retriever = ChromadbRM(CHROMA_COLLECTION_NAME, CHROMADB_DIR, default_ef, k=3)\n",
    "\n",
    "# dspy.settings.configure(lm=llm, rm=retriever)\n",
    "dspy.settings.configure(lm=llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uncompiled_rag = RAG()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "question = \"What is the Disposition Fee?\"\n",
    "answer = uncompiled_rag(question).answer\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine tune spreadsheet model\n",
    "# dspy module class for multiagent stuff with tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/workspace/data/MASTER - PYTHON - SCORING MODEL - MCG MADISON RIDGE DST - v2.0.xlsx\"\n",
    "docs = PandasExcelReader(sheet_name=\"5 - Disposition Analysis\", pandas_config={'keep_default_na':False}).load_data(filepath)\n",
    "# docs2 = PandasExcelReader(sheet_name=\"5 - Disposition Analysis\").load_data(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpreadsheetValues(dspy.Signature):\n",
    "    \"\"\"Parses through spreadsheet information and returns requested values.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"data in spreadsheet format.\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"the float value or values from the spreadsheet.\")\n",
    "\n",
    "# class GoogleSearch(dspy.Signature):\n",
    "#     \"\"\"Searches Google for a query and returns the top results.\"\"\"\n",
    "\n",
    "#     query = dspy.InputField()\n",
    "#     results = dspy.OutputField(desc=\"the top search results from Google.\")\n",
    "\n",
    "\n",
    "\n",
    "agent = dspy.ReAct(SpreadsheetValues) #dspy.Retrieve(k=1) for a tool , tools=[adding_tool] \n",
    "# classify = dspy.Predict(SpreadsheetValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:562: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "question='Retrieve the following values from the spreadsheet: Selling Costs, Disposition Fee, Net Operating Income, Loan Assumption/Payoff, Return of Forecasted Reserves, CF Y 11, Return of Maximum Offering Amount, Projected Terminal Cap Rate.\\nThen add Disposition Fee and Selling Cost together.'\n",
    "\n",
    "# question=\"What is the Disposition Fee and Selling Cost? There may be duplicate fields. Use the one that returns a value between 0 and 1.\"\n",
    "# context=text_representation\n",
    "context=docs[0].text\n",
    "result = agent(question=question, context=context, verbose=True)\n",
    "# result = classify(question=question, context=context)\n",
    "print(f\"Final Predicted Answer:\\n\\n{result.answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/workspace/data/MASTER - PYTHON - SCORING MODEL - MCG MADISON RIDGE DST - v2.0.xlsx\"\n",
    "data = pd.read_excel(filepath, sheet_name=\"5 - Disposition Analysis\", header=None)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(axis=1, how='all', inplace=True)\n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(axis=0, how='all', inplace=True)\n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_representation = \"Spreadsheet Data:\\n\\n\"\n",
    "for index, row in data.iterrows():\n",
    "    row_text = ''#f\"Record {index + 1}: \"\n",
    "    row_items = []\n",
    "    for col_name, value in row.items():\n",
    "        # Handle NaN values explicitly if needed\n",
    "        value_text = 'None' if pd.isna(value) else str(value)\n",
    "        row_items.append(f\"{col_name}: {value_text}\")\n",
    "    row_text += \", \".join(row_items)\n",
    "    text_representation += row_text + \"\\n\"\n",
    "\n",
    "# print(text_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from llama_index.core.query_pipeline import QueryPipeline as QP, InputComponent, FnComponent\n",
    "from dspy.predict.llamaindex import DSPyComponent, LlamaIndexModule\n",
    "\n",
    "dspy_component = DSPyComponent(\n",
    "    dspy.ChainOfThought(GenerateAnswer),\n",
    "    # dspy.ReAct(\"question -> answer\", tools=[dspy.Retrieve(k=1), adding_tool] )\n",
    ")\n",
    "\n",
    "retriever_post = FnComponent(\n",
    "    lambda contexts: \"\\n\\n\".join([n.get_content() for n in contexts])\n",
    ")\n",
    "\n",
    "\n",
    "p = QP(verbose=True)\n",
    "p.add_modules(\n",
    "    {\n",
    "        \"input\": InputComponent(),\n",
    "        \"retriever\": retriever,\n",
    "        \"retriever_post\": retriever_post,\n",
    "        \"synthesizer\": dspy_component,\n",
    "    }\n",
    ")\n",
    "p.add_link(\"input\", \"retriever\")\n",
    "p.add_link(\"retriever\", \"retriever_post\")\n",
    "p.add_link(\"input\", \"synthesizer\", dest_key=\"query_str\")\n",
    "p.add_link(\"retriever_post\", \"synthesizer\", dest_key=\"context_str\")\n",
    "\n",
    "\n",
    "dspy_qp = LlamaIndexModule(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out = dspy_qp(query_str=\"What is the Disposition Fee and Selling Cost? Add them together.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(out.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = dspy.ReAct(\"question -> answer\", tools=[dspy.Retrieve(k=1), adding_tool] )\n",
    "agent(question=\"What is the Disposition Fee and Selling Cost?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any, Callable, Optional\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plan(dspy.Signature):\n",
    "    \"\"\"Produce a step by step plan to perform the task. \n",
    "The plan needs to be in markdown format and should be broken down into big steps (with ## headings) and sub-steps beneath those.\n",
    "When thinking about your plan, be sure to think about the tools at your disposal and include them in your plan.\n",
    "    \"\"\"\n",
    "    task = dspy.InputField(prefix=\"Task\", desc=\"The task\")\n",
    "    context = dspy.InputField(format=str, desc=\"The context around the plan\")\n",
    "    proposed_plan = dspy.OutputField(desc=\"The proposed, step by step execution plan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker(dspy.Module):\n",
    "    def __init__(self, role:str, tools:List):\n",
    "        self.role = role\n",
    "        self.tools = tools\n",
    "        self.tool_descriptions = \"\\n\".join([f\"- {t.name}: {t.description}. To use this tool please provide: `{t.requires}`\" for t in tools])\n",
    "        self.plan = dspy.ChainOfThought(Plan)\n",
    "    def forward(self, task:str):\n",
    "        context = f\"{self.role}\\n{self.tool_descriptions}\"\n",
    "        input_args = dict(\n",
    "            context = context,\n",
    "            task = task\n",
    "        ) # just did args for printing for debugging\n",
    "        result = self.plan(**input_args)\n",
    "        print(result.proposed_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tool(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    requires: str\n",
    "    func: Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tools = [\n",
    "    Tool(name=\"phone\", description=\"a way of making phone calls\", requires=\"phone_number\", func=lambda x: \"they've got time\"),\n",
    "    Tool(name=\"local business lookup\", description=\"Look up businesses by category\", requires=\"business category\", func=lambda x: \"Bills landscaping: 415-555-5555\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dspy.context(lm=wrkr):\n",
    "    Worker(\"assistant\", test_tools).forward(\"get this yard cleaned up.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker2(dspy.Module):\n",
    "    def __init__(self, role:str, tools:List):\n",
    "        self.role = role\n",
    "        self.tools = dict([(t.name, t) for t in tools])\n",
    "        self.tool_descriptions = \"\\n\".join([f\"- {t.name}: {t.description}. To use this tool please provide: `{t.requires}`\" for t in tools])\n",
    "        self._plan = dspy.ChainOfThought(Plan)\n",
    "        self._tool = dspy.ChainOfThought(\"task, context -> tool_name, tool_argument\")\n",
    "        \n",
    "        print(self.tool_descriptions)\n",
    "    def plan(self, task:str, feedback:Optional[str]=None):\n",
    "        context = f\"Your role:{self.role}\\n Tools at your disposal:\\n{self.tool_descriptions}\"\n",
    "        if feedback:\n",
    "            context += f\"\\nPrevious feedback on your prior plan {feedback}\"\n",
    "        input_args = dict(\n",
    "            task=task,\n",
    "            context=context\n",
    "        )    \n",
    "        result = self._plan(**input_args)\n",
    "        return result.proposed_plan\n",
    "    def execute(self, task:str, use_tool:bool):\n",
    "        print(f\"executing {task}\")\n",
    "        if not use_tool:\n",
    "            return f\"{task} completed successfully\"\n",
    "            \n",
    "        res = self._tool(task=task, context=self.tool_descriptions)\n",
    "        t = res.tool_name\n",
    "        arg = res.tool_argument\n",
    "        if t in self.tools:\n",
    "            complete = self.tools[t].func(arg)\n",
    "            return complete\n",
    "        return \"Not done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsp.utils import flatten, deduplicate\n",
    "\n",
    "AGENTS = [x[-1] for x in optimized_react.candidate_programs[:5]]\n",
    "\n",
    "class Aggregator(dspy.Module):\n",
    "\tdef __init__(self, temperature=0.0):\n",
    "\t\tself.aggregate = dspy.ChainOfThought('context, question -> answer')\n",
    "\t\tself.temperature = temperature\n",
    "\n",
    "\tdef forward(self, question):\n",
    "\t\t# Run all five agents with high temperature, then extract and deduplicate their observed contexts\n",
    "\t\twith dspy.context(lm=gpt3.copy(temperature=self.temperature)):\n",
    "\t\t\tpreds = [agent(question=question) for agent in AGENTS]\n",
    "\t\t\tcontext = deduplicate(flatten([flatten(p.observations) for p in preds]))\n",
    "\n",
    "\t\t# Run the aggregation step to produce a final answer\n",
    "\t\treturn self.aggregate(context=context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from typing import List, Literal\n",
    "from llama_index.core.bridge.pydantic import BaseModel, Field\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.base.llms.types import (\n",
    "    ChatMessage,\n",
    "    MessageRole,\n",
    ")\n",
    "\n",
    "\n",
    "query_engine_tools = QueryEngineTool(\n",
    "    query_engine=query_engine,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"spreadsheet_value_retriever\",\n",
    "        description=\"contains the information of a spreadsheet, and is useful for retrieving specific values from a spreadsheet\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "def adding_values(values: List[float]):\n",
    "    return sum(values)\n",
    "\n",
    "\n",
    "class AddingArgs(BaseModel):\n",
    "    values: List = Field(\n",
    "        description=\"A list of values to add together.\"\n",
    "    )\n",
    "\n",
    "adding_tool = FunctionTool.from_defaults(\n",
    "    fn=adding_values,\n",
    "    name=\"sum_values\",\n",
    "    description=\"Add a list of values together\",\n",
    "    fn_schema=AddingArgs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = ['Selling Costs',\n",
    "  'Disposition Fee',\n",
    "  'Net Operating Income',\n",
    "  'Loan Assumption/Payoff',\n",
    "  'Return of Forecasted Reserves',\n",
    "  'CF Y 11',\n",
    "  'Return of Maximum Offering Amount',\n",
    "  'Projected Terminal Cap Rate',\n",
    "  'Cash Flows']\n",
    "content='Retrieve the following values from the spreadsheet: Selling Costs, Disposition Fee, Net Operating Income, Loan Assumption/Payoff, Return of Forecasted Reserves, CF Y 11, Return of Maximum Offering Amount, Projected Terminal Cap Rate, Cash Flows (categories 1 through 9)\\nThen add Disposition Fee and Selling Cost together.'\n",
    "\n",
    "usr_msg = ChatMessage(\n",
    "    role=MessageRole.ASSISTANT,\n",
    "    content=content\n",
    ")\n",
    "response = agent1.chat(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content='Retrieve the following values from the spreadsheet: Selling Costs, Disposition Fee, Net Operating Income, Loan Assumption/Payoff, Return of Forecasted Reserves, CF Y 11, Return of Maximum Offering Amount, Projected Terminal Cap Rate, Cash Flows (categories 1 through 9)\\nThen add Disposition Fee and Selling Cost together.'\n",
    "\n",
    "messages = [\n",
    "        {\"role\": \"user\", \"content\": content},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
