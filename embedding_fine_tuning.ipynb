{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import hashlib, uuid\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter, SemanticSplitterNodeParser\n",
    "\n",
    "class SyntheticDataGenerator:\n",
    "    \"\"\"\n",
    "    This class generates synthetic data from a list of text documents and query prompts.\n",
    "    The synthetic data is generated by querying a language model (LLM) with a prompt that includes the text from the document.\n",
    "    The LLM generates a response, which is then parsed to extract the synthetic data.\n",
    "    The synthetic data is saved in a json file.\n",
    "    \"\"\"\n",
    "\n",
    "    #TODO: add splitter_arguments and possibly option for sentence_splitter\n",
    "    def __init__(self, embed_model, llm, query_list, max_gen_attempts=2, output_dir=\"./synth_dataset\"):\n",
    "        self.embed_model = embed_model\n",
    "        self.llm = llm\n",
    "        self.query_list = query_list # different types of queries you want to generate for each text chunk\n",
    "        self.output_dir = output_dir\n",
    "        self.parser = SentenceSplitter(chunk_size=500, chunk_overlap=50) if embed_model is None else SemanticSplitterNodeParser(buffer_size=1, embed_model=embed_model)\n",
    "        self._ensure_output_dir_exists()\n",
    "        self.max_gen_attempts = max_gen_attempts # number of times to try generating and successfully parsing a response for a text chunk\n",
    "        self.parse_attempt_counters = {} # keeps track of the number of times we've tried to parse a response for a text chunk\n",
    "\n",
    "    def _ensure_output_dir_exists(self):\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    def generate_random_hash(self):\n",
    "        random_uuid = uuid.uuid4()\n",
    "        hash_object = hashlib.sha256(random_uuid.bytes)\n",
    "        return hash_object.hexdigest()\n",
    "\n",
    "    def generate_synthetic_data(self, documents, num_generate=1, file_name=\"nodes.pkl\"):\n",
    "        \"\"\"\n",
    "        documents is a list of text. Each text is a document.\n",
    "        num_generate is the number of synthetic examples to generate for text chunk.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Parsing documents...\")\n",
    "        # the corpus will be the same for all tasks, only need to save it once\n",
    "        if os.path.exists(os.path.join(self.output_dir, file_name)):\n",
    "            with open(os.path.join(self.output_dir, file_name), 'rb') as f:\n",
    "                nodes = pickle.load(f)\n",
    "        else:\n",
    "            documents = [Document(doc_id=i, text=doc) for i, doc in enumerate(documents)]\n",
    "            nodes = self.parser.get_nodes_from_documents(documents, show_progress=False)\n",
    "            with open(os.path.join(self.output_dir, file_name), 'wb') as f:\n",
    "                pickle.dump(nodes, f)\n",
    "        \n",
    "        # corpus.json is the first of two files needed for embedding fine tuning\n",
    "        if not os.path.exists(os.path.join(self.output_dir, f\"corpus.json\")):\n",
    "            # node.node_id is the connection between the node and the generated data\n",
    "            corpus = {node.node_id: node.text for node in nodes}\n",
    "            with open(os.path.join(self.output_dir, f\"corpus.json\"), 'w', encoding='utf-8') as f:\n",
    "                json.dump(corpus, f, ensure_ascii=False, indent=2)\n",
    "        else:\n",
    "            with open(os.path.join(self.output_dir, f\"corpus.json\"), 'r', encoding='utf-8') as f:\n",
    "                corpus = json.load(f)\n",
    "        print(f\"Number of nodes: {len(nodes)}\")\n",
    "\n",
    "        # for each task in the query list, generate synthetic data from each node\n",
    "        # a task is a specific type of question or instruction, e.g. \"Generate a query from the text in the form of a socratic question\"\n",
    "        # the number of tasks depends on your final goal for the LLM\n",
    "        for task in self.query_list:\n",
    "            self._process_task(task, nodes, num_generate, corpus)\n",
    "\n",
    "    def _process_task(self, task, nodes, num_generate, corpus):\n",
    "        print(f\"  Processing {task}...\")\n",
    "        collection = {}\n",
    "        # this is the second type of two files needed for embedding training\n",
    "        # you will get a json file for each task\n",
    "        dataset_output_path = os.path.join(self.output_dir, f\"dataset_{task}.json\")\n",
    "        \n",
    "        # process each node for one task\n",
    "        for node_counter, node in enumerate(nodes):\n",
    "            self._process_one_node(task, node, num_generate, corpus, collection)\n",
    "            # periodically save the generated data to file, and the last batch\n",
    "            if node_counter % max(5, int(len(nodes) / 10)) == 0 or node_counter == len(nodes) - 1:\n",
    "                self._save_to_file(dataset_output_path, collection)\n",
    "                # print(f\"    Processed {node_counter+1}/{len(nodes)} nodes...\")\n",
    "                # reset collection for next batch to save memory\n",
    "                collection = {}\n",
    "\n",
    "    def task_parser(self, response):\n",
    "        # this method parses the response from the llm, and returns a list of strings\n",
    "        # the len of the list should be equal to the number of examples you want to generate per text chunk\n",
    "        raise NotImplementedError(\"task_parser method must be implemented in a subclass\")\n",
    "\n",
    "    def _process_one_node(self, task, node, num_generate, corpus, collection):\n",
    "        # Reset counter for new node\n",
    "        node_key = f\"{task}_{node.node_id}\"\n",
    "        self.parse_attempt_counters[node_key] = 0\n",
    "        \n",
    "        instruct_prompt = task\n",
    "        # num_generate is a parameter in the prompt to tell the LLM how many examples from the text chunk to generate\n",
    "        # This is so that the LLM doesn't generate the same response num_generate times for each text chunk\n",
    "        prompt = instruct_prompt.format(text=node.text, num_generate=num_generate)\n",
    "\n",
    "        # Loop until we get a successful parsing of the response or reach max attempts\n",
    "        # this is a safe guard agains the LLM not generating a response that can be parsed\n",
    "        while True:\n",
    "            response = self.llm.complete(prompt)\n",
    "            clean_response = self.task_parser(response.text)\n",
    "            if clean_response == []:\n",
    "                if self._handle_failed_parse(task, node):\n",
    "                    break\n",
    "                continue\n",
    "            else:\n",
    "                self._handle_successful_parse(clean_response, node, task, corpus, collection)\n",
    "                break\n",
    "\n",
    "    def _handle_failed_parse(self, task, node):\n",
    "        node_key = f\"{task}_{node.node_id}\"\n",
    "        self.parse_attempt_counters[node_key] += 1\n",
    "        \n",
    "        if self.parse_attempt_counters[node_key] == self.max_gen_attempts:\n",
    "            print(f\"    Failed to parse response for node id {node.node_id}...\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _handle_successful_parse(self, clean_response, node, task, corpus, collection):\n",
    "        if isinstance(clean_response, str):\n",
    "            clean_response = [clean_response]\n",
    "\n",
    "        # each generated example will have a unique hash\n",
    "        hashes = [self.generate_random_hash() for _ in range(len(clean_response))]\n",
    "        \n",
    "        corpus[node.node_id] = node.text\n",
    "        for i in range(len(hashes)):\n",
    "            collection[hashes[i]] = {\n",
    "                'response': clean_response[i],\n",
    "                'hash_id': hashes[i],\n",
    "                'relevant_doc': node.node_id, # this is the connection between the generated data and the corpus.json\n",
    "                # 'task': task # this may not be important\n",
    "            }\n",
    "\n",
    "    def _save_to_file(self, file_path, data):\n",
    "        # Load existing data if file exists\n",
    "        existing_data = {}\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "        \n",
    "        # append new data to existing data\n",
    "        merged_data = {**existing_data, **data}\n",
    "        \n",
    "        # Save merged data back to file\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(merged_data, f, ensure_ascii=False, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
