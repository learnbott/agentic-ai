{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and install OLLAMA\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter docker container in terminal\n",
    "# docker exec -it <DOCKER_CONTAINER_ID> /bin/bash\n",
    "\n",
    "# In a separate terminal (or tmux screen) run the following command to start the Ollama server:\n",
    "# ollama start\n",
    "# ---or pull and run the model at the same time in the terminal---\n",
    "# ollama run <MODEL_NAME>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ollama.com/library?sort=popular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/spaces/open-llm-leaderboard/comparator\n",
    "- compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull qwen2.5:3b-instruct-q8_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "# model_name, ctx_len = \"gpt-4o-2024-08-06\", 128000\n",
    "# model_name, ctx_len = \"llama3.2:1b\", 128000\n",
    "# model_name, ctx_len = \"bespoke-minicheck\", 32000\n",
    "model_name, ctx_len = \"qwen2.5:3b-instruct-q8_0\", 32000\n",
    "\n",
    "if \"gpt-4o\" in model_name:\n",
    "    openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "    \n",
    "    print(f\"Using OpenAI {model_name}...\")\n",
    "    llm = OpenAI(model=model_name, max_tokens=8000)\n",
    "    \n",
    "addtion_kwargs = {\"max_new_tokens\": 8000}\n",
    "llm = Ollama(model=model_name, url=\"http://127.0.0.1:11434\", context_window=ctx_len, model_type=\"chat\", is_function_calling_model=False, \n",
    "                request_timeout=4000.0, additional_kwargs=addtion_kwargs)\n",
    "print(llm.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.complete(\"What is the meaning of life?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert any model to GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For faster compilation, use ccache (only if you will recompile llama.cpp)\n",
    "\n",
    "# import os\n",
    "\n",
    "# # Install package\n",
    "# !apt update && apt install -y ccache\n",
    "\n",
    "# # Update symlinks\n",
    "# !/usr/sbin/update-ccache-symlinks\n",
    "\n",
    "# # Prepend ccache into the PATH\n",
    "# os.environ['PATH'] = \"/usr/lib/ccache:\" + os.environ['PATH']\n",
    "\n",
    "# !echo $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and install LLAMA.CPP (15-20 mins)\n",
    "!cd /workspace/repos/ && git clone https://github.com/ggerganov/llama.cpp\n",
    "!cd /workspace/repos/llama.cpp && git pull && make clean && LLAMA_CUDA=0 make\n",
    "!chmod 755 /workspace/repos/llama.cpp/requirements.txt && pip3 install -r /workspace/repos/llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "\n",
    "save_dir = \"/workspace/data/new_embed_model\"\n",
    "\n",
    "# tokenizer_name = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "# model_name = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "tokenizer_name = \"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "model_name = \"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "model = AutoModel.from_pretrained(model_name,\n",
    "                                return_dict=True,\n",
    "                                torch_dtype=torch.float16,\n",
    "                                device_map=\"auto\",\n",
    "                                trust_remote_code=True\n",
    "                                )\n",
    "# fine tune the model\n",
    "# train a new model \n",
    "# peft the model\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "\n",
    "print('Saving model')\n",
    "model.save_pretrained(save_dir)\n",
    "print('Saving tokenizer')\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert model to gguf format\n",
    "!python3 /workspace/repos/llama.cpp/convert_hf_to_gguf.py /workspace/data/new_embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SmolLM-135M-Instruct-F16.gguf is the resulting name in save_dir from the gguf conversion (see inside new_embed_model directory)\n",
    "# make a Modelfile for the new_embed_model\n",
    "!cd /workspace/data/new_embed_model && echo 'FROM \"/workspace/data/new_embed_model/mxbai-embed-large-v1-F16.gguf\"' >> Modelfile\n",
    "# add new_embed_model to the Ollama registry\n",
    "!ollama create new_embed_model -f /workspace/data/new_embed_model/Modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Army of specialized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple Ollama models \n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# The default Ollama server runs at http://127.0.0.1:11434, so we arbitrarily increment the port number by 1\n",
    "# From a second tmux screen start a new ollama server with the model\n",
    "# OLLAMA_HOST=\"http://127.0.0.1:11435\" ollama start \n",
    "\n",
    "embed_model = OllamaEmbedding(\n",
    "                              model_name=\"new_embed_model:latest\",\n",
    "                              base_url=\"http://127.0.0.1:11435\",\n",
    "                              ollama_additional_kwargs={\"mirostat\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: notify if ollama server is running with model loaded\n",
    "import subprocess, os\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.openai import OpenAI as LOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "# model_name, ctx_len = \"gpt-4o-2024-08-06\", 128000\n",
    "# model_name, ctx_len = \"llama3.1:8b-instruct-q8_0\", 128000\n",
    "# model_name, ctx_len = \"bespoke-minicheck\", 32000\n",
    "model_name, ctx_len = \"qwen2.5:3b-instruct-q8_0\", 128000\n",
    "\n",
    "if \"gpt-4o\" in model_name:\n",
    "    openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "    \n",
    "    print(f\"Using OpenAI {model_name}...\")\n",
    "    llm = LOpenAI(model=model_name, max_tokens=8000)\n",
    "else:\n",
    "    subout = subprocess.run(['ollama', 'list'], capture_output=True, text=True)\n",
    "    if model_name in subout.stdout:\n",
    "        print('Model loaded...')\n",
    "    else:\n",
    "        try: \n",
    "            print(\"Pulling Ollama model...\")\n",
    "            sub_out = subprocess.run(['ollama', 'pull', model_name], capture_output=True, text=True)\n",
    "        except Exception as e: \n",
    "            print(f\"Error pulling model: Is the Ollama server running?\\n{e}\")\n",
    "    \n",
    "    addtion_kwargs = {\"max_new_tokens\": 8000}\n",
    "    llm = Ollama(model=model_name, url=\"http://127.0.0.1:11434\", context_window=ctx_len, model_type=\"chat\", is_function_calling_model=False, \n",
    "                 request_timeout=4000.0, additional_kwargs=addtion_kwargs)\n",
    "    print(llm.metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Know Your Rule Proposal (KYRP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from federal_register.client import FederalRegister\n",
    "import requests\n",
    "\n",
    "# Initialize the client.\n",
    "federal_register_client = FederalRegister()\n",
    "\n",
    "# Grab a specific document.\n",
    "federal_document = federal_register_client.document_by_id(\n",
    "    document_id='2024-10738',\n",
    "    fields='all'\n",
    ")\n",
    "\n",
    "# Print it out.\n",
    "raw_text_url = federal_document['raw_text_url']\n",
    "title = federal_document['title']\n",
    "headers = {\"accept\": \"*/*\"}\n",
    "response = requests.get(raw_text_url, headers=headers)\n",
    "print(\"TITLE: \",title)\n",
    "print()\n",
    "print(response.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html_tags(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "rule_proposal = remove_html_tags(response.text)\n",
    "print(rule_proposal[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excerpt = \"\"\"Subpart A--General\n",
    "\n",
    "Sec.  1032.100  Definitions.\n",
    "\n",
    "    Refer to Sec.  1010.100 of this chapter for general definitions not \n",
    "noted herein. To the extent there is a differing definition in Sec.  \n",
    "1010.100, the definition in this section is what applies to part 1032. \n",
    "Unless otherwise indicated, for purposes of this part:\n",
    "    (a) Account. For purposes of Sec.  1032.220:\n",
    "    (1) Account means any contractual or other business relationship \n",
    "between a person and an investment adviser under which the investment \n",
    "adviser provides investment advisory services.\n",
    "    (2) Account does not include:\n",
    "    (i) An account that the investment adviser acquires through any \n",
    "acquisition, merger, purchase of assets, or assumption of liabilities.\n",
    "    (ii) [Reserved]\n",
    "    (b) Commission means the United States Securities and Exchange \n",
    "Commission.\n",
    "    (c) Customer. For purposes of Sec.  1032.220:\n",
    "    (1) Customer means:\n",
    "    (i) A person that opens a new account; and\n",
    "    (ii) An individual who opens a new account for:\n",
    "    (A) An individual who lacks legal capacity, such as a minor; or\n",
    "    (B) An entity that is not a legal person, such as a civic club.\n",
    "    (2) Customer does not include:\n",
    "    (i) A financial institution regulated by a Federal functional \n",
    "regulator or a bank regulated by a State bank regulator;\n",
    "    (ii) A person described in Sec.  1020.315(b)(2) through (4) of this \n",
    "chapter; or\n",
    "    (iii) A person that has an existing account with the investment \n",
    "adviser, provided the investment adviser has a reasonable belief that \n",
    "it knows the true identity of the person.\n",
    "    (d) Financial institution is defined at 31 U.S.C. 5312(a)(2) and \n",
    "(c)(1) and its implementing regulation in Chapter X of Title 31.\n",
    "    (e) Investment adviser. Any person who is registered or required to \n",
    "register with the Commission under section 203 of the Investment \n",
    "Advisers Act of 1940 (15 U.S.C. 80b-3(a)), or any person that is exempt \n",
    "from Commission registration under sections 203(l) or 203(m) of the \n",
    "Investment Advisers Act of 1940 (15 U.S.C. 80b-3(l), (m)).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import Document\n",
    "\n",
    "documents = [Document(text=excerpt, metadata={\"title\": \"CFR Rule Proposal\"})]\n",
    "parser = SentenceSplitter(chunk_size=200, chunk_overlap=20, tokenizer=None)\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the LLM is a closed model, then this becomes distillation\n",
    "result_collection = []\n",
    "for node in nodes:\n",
    "    result = llm.complete(f\"\"\"Create 5 questions with answers based on the text below.\n",
    "                              Answers should vary in length and complexity.\n",
    "                              Include your reasoning for each answer.\n",
    "                              Return the questions in the following format:\n",
    "                              Q1: ...? \n",
    "                              A1: ... \n",
    "                              Reasoning: ...\n",
    "                        \n",
    "                              Q2: ...? \n",
    "                              A2: ...\n",
    "                              Reasoning: ...\n",
    "                              \n",
    "                              Text:\n",
    "                              {node.text}\"\"\")\n",
    "    result_collection.append(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_collection[0])\n",
    "\n",
    "# Fine tune model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaIndex RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/spaces/mteb/leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "\n",
    "save_dir = \"/workspace/data/new_embed_model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "embed_model = AutoModel.from_pretrained(save_dir,\n",
    "                                             return_dict=True,\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=True\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple Ollama models \n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# The default Ollama server runs at http://127.0.0.1:11434, so we arbitrarily increment the port number by 1\n",
    "# From a second tmux screen start a new ollama server with the model\n",
    "# OLLAMA_HOST=\"http://127.0.0.1:11435\" ollama start \n",
    "\n",
    "embed_model = OllamaEmbedding(\n",
    "                              model_name=\"new_embed_model:latest\",\n",
    "                              base_url=\"http://127.0.0.1:11435\",\n",
    "                              ollama_additional_kwargs={\"mirostat\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Database RAG\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes=nodes, \n",
    "                                embed_model=embed_model, \n",
    "                                show_progress=True,\n",
    "                            )\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    llm=llm,\n",
    "    similarity_top_k=5,\n",
    "    node_postprocessors=[\n",
    "        LLMRerank(\n",
    "            llm=llm,\n",
    "            choice_batch_size=5,\n",
    "            top_n=2,\n",
    "        )\n",
    "    ],\n",
    "    # see https://github.com/run-llama/llama_index/blob/f7c5ee5efbb6172e819f26d1705fcdf6114b11a3/llama-index-core/llama_index/core/response_synthesizers/type.py#L4\n",
    "    response_mode=\"tree_summarize\", # \"accumulate\", \"compact_accumulate\", \"compact\", \"simple_summarize\", \"tree_summarize\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"What is the definition of 'Account'?\",\n",
    ")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in response.source_nodes:\n",
    "    \n",
    "    print('TEXT:\\n',source.text)\n",
    "    print()\n",
    "    print('SCORE:',source.score)\n",
    "    print('ID:',source.node_id)\n",
    "    print('-'*100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaIndex Workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import subprocess, os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.openai import OpenAI as LOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "from utils import parse_list_from_output_string, extract_list_from_string\n",
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    ")\n",
    "\n",
    "from llama_index.core.agent.react import ReActAgent\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import ToolSelection, ToolOutput\n",
    "\n",
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.openai import OpenAI as LOpenAI\n",
    "from rag_utils import create_llama_vector_index_rag\n",
    "\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from sec_utils import get_tree_data, get_metadata\n",
    "from llamaindex_data_utils import extract_text_from_pdf\n",
    "\n",
    "import os, re\n",
    "from llama_index.readers.file import PDFReader\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class QueryQualityEvent(Event):\n",
    "    result: str\n",
    "\n",
    "class InitializationEvent(Event):\n",
    "    pass\n",
    "\n",
    "class InitializationCleanupEvent(Event):\n",
    "    result: str\n",
    "\n",
    "class RegulationsExtractionEvent(Event):\n",
    "    pass\n",
    "\n",
    "class FormatCorrectionEvent(Event):\n",
    "    result: list\n",
    "\n",
    "class SummarizationEvent(Event):\n",
    "    result: list\n",
    "\n",
    "class SummarizationNumericalValidationEvent(Event):\n",
    "    result: list\n",
    "    summaries: list\n",
    "\n",
    "class RuleSummarizationFlow(Workflow):\n",
    "\n",
    "    # Similar to __init__, but for workflows\n",
    "    @step\n",
    "    async def initialize(self, ctx: Context, ev: StartEvent) -> RegulationsExtractionEvent:\n",
    "        \n",
    "        # Open source work horse model\n",
    "        model_name, ctx_len = \"llama3.1:8b-instruct-q8_0\", 128000\n",
    "        addtion_kwargs = {\"max_new_tokens\": 8000}\n",
    "        ctx.data[\"llm\"] = Ollama(model=model_name, \n",
    "                                 url=\"http://127.0.0.1:11434\", \n",
    "                                 context_window=ctx_len, \n",
    "                                 model_type=\"chat\", \n",
    "                                 is_function_calling_model=True,\n",
    "                                 request_timeout=4000.0, \n",
    "                                 additional_kwargs=addtion_kwargs)\n",
    "        \n",
    "        # Expert closed model\n",
    "        ctx.data[\"expert_llm\"] = ctx.data[\"llm\"] # OpenAI(model=\"gpt-4o\",temperature=0.1)\n",
    "        \n",
    "        # Embedding model\n",
    "        ctx.data[\"embed_model\"] = OllamaEmbedding(model_name, base_url=\"http://localhost:11435\")\n",
    "        \n",
    "        # Load the document\n",
    "        documents_proposal = [Document(text=t, text_template='{metadata_str}\\n\\n{content}') for t in sections]\n",
    "\n",
    "        # add metadata to the documents_proposal\n",
    "        for i in range(len(documents_proposal)):\n",
    "            documents_proposal[i].metadata[\"section\"] = documents_proposal[i].text.split(\"\\n\")[0].strip()\n",
    "            \n",
    "        # Global state context\n",
    "        ctx.data[\"chat_llm\"] = SimpleChatEngine.from_defaults(llm=ctx.data[\"expert_llm\"])\n",
    "        ctx.data[\"summarizer_llm\"] = TreeSummarize(llm=ctx.data[\"expert_llm\"], verbose=False)\n",
    "        ctx.data[\"new_rule_documents\"] = documents_proposal\n",
    "        \n",
    "        ctx.data[\"initialized\"] = True\n",
    "        return RegulationsExtractionEvent()\n",
    "\n",
    "    # steps only take in one Event and return one Event, but can have multiple types of return Events\n",
    "    @step\n",
    "    async def extract_regulations(self, ctx: Context, ev: RegulationsExtractionEvent) -> FormatCorrectionEvent:\n",
    "        assert ctx.data[\"initialized\"], \"Workflow not initialized.\"\n",
    "        \n",
    "        regs_extraction=[]\n",
    "        for i,sec in enumerate(ctx.data[\"new_rule_documents\"]):\n",
    "\n",
    "            rule_prompt = f\"\"\"\n",
    "            Extract all mentions of any regulatory sections, rules, and acts in the following text.\n",
    "            Compile all extracted items into a single Python List of Strings object.\n",
    "            For example, [\"Item 1\", \"Item 2\", \"Item 3\"].\n",
    "            Only return the Python List of Strings.\n",
    "\n",
    "            Here is the text:\n",
    "            {sec}\n",
    "            \"\"\"\n",
    "            response = ctx.data[\"chat\"].chat(rule_prompt)\n",
    "            regs_extraction.append(response)\n",
    "        \n",
    "        return FormatCorrectionEvent(result=regs_extraction)\n",
    "    \n",
    "    @step\n",
    "    async def correct_format(self, ctx: Context, ev: FormatCorrectionEvent) -> SummarizationEvent:\n",
    "        \n",
    "        regs_list = []\n",
    "        for response in ev.result:\n",
    "            # List checker\n",
    "            rewrite_counter=0\n",
    "            while True:\n",
    "                try: \n",
    "                    \n",
    "                    # extracted_reg = parse_list_from_output_string(response)\n",
    "                    extracted_reg = extract_list_from_string(response)\n",
    "                    break\n",
    "                except:\n",
    "                    print(\"Correcting list format...\")\n",
    "                    response = ctx.data[\"chat\"].chat(f\"\"\"The following text does not contain a valid Python List of Strings? \n",
    "                                                Rewrite the text so that the List is in a valid Python format.\n",
    "                                                For example, [\"Item 1\", \"Item 2\", \"Item 3\"].\\nText:\\n\\n{response}\n",
    "                                            \"\"\")\n",
    "                    rewrite_counter+=1\n",
    "                    print(f\"   Rewrote list {rewrite_counter} times.\")\n",
    "                    if rewrite_counter>5:\n",
    "                        raise ValueError(\"Could not correct list format.\")\n",
    "            regs_list.append(extracted_reg)\n",
    "\n",
    "        return SummarizationEvent(result=regs_list)\n",
    "\n",
    "\n",
    "    @step\n",
    "    async def summarize_sections(self, ctx: Context, ev: SummarizationEvent) -> SummarizationNumericalValidationEvent | StopEvent: #SummarizationValidationEvent:\n",
    "        \n",
    "        if len(ctx.data[\"bad_summaries\"])==0:\n",
    "            return StopEvent(result=ctx.data[\"section_summaries\"])\n",
    "        elif \"section_summaries\" in ctx.data:\n",
    "            print(\"   Bad summaries found. Correcting...\")\n",
    "            section_summaries = ctx.data[\"section_summaries\"]\n",
    "            summary_count = ctx.data[\"bad_summaries\"]\n",
    "            prompt_suffix = f\"\"\"\\nThe first attempt at summarizing this section had numerical copy mistakes. Copy numbers exactly as they appear in the text.\"\"\"\n",
    "        else:\n",
    "            section_summaries = [None]*ctx.data[\"num_sections\"]\n",
    "            summary_count = range(ctx.data[\"num_sections\"])\n",
    "            prompt_suffix = \"\"\n",
    "        \n",
    "        for i in summary_count:\n",
    "            extracted_reg = ev.result[i]\n",
    "            prompt_summary = f\"\"\"\n",
    "            Summarize the content of the following section from a new SEC rule proposal or amendment. Ensure that the summary:\n",
    "\n",
    "            1. Includes every reference to any specific regulatory section, rule, or act mentioned in the text (e.g., 12 U.S.C. 1843(k)(4)(C), 240.13a-15, Unfunded Mandates Reform Act (section 202(a)), Investment Company Act of 1940).\n",
    "            2. Stays strictly within the information presented in the text, without incorporating any outside or prior knowledge.\n",
    "            3. Is detailed, thorough, and specific in its coverage of all key points, definitions and exclusions.\n",
    "            4. Avoids adding any introductory or concluding remarks outside the scope of the summary itself.\n",
    "\n",
    "            Here are some of the regulatory sections, rules, and acts:\n",
    "            {extracted_reg} \n",
    "            \"\"\"\n",
    "            print(f\"   Summarizing section {i+1}/{ctx.data['num_sections']}...\")\n",
    "            parser = SentenceSplitter(chunk_size=500, chunk_overlap=20)\n",
    "            nodes = parser.get_nodes_from_documents(ctx.data[\"new_rule_documents\"], show_progress=True)\n",
    "            response = ctx.data[\"summarizer\"].get_response(prompt_summary+prompt_suffix, [doc.text for doc in nodes])\n",
    "            section_summaries[i] = response\n",
    "        \n",
    "        ctx.data[\"section_summaries\"] = section_summaries\n",
    "        return SummarizationNumericalValidationEvent(result=ev.result, summaries=section_summaries)\n",
    "    \n",
    "    @step\n",
    "    async def validate_summaries(self, ctx: Context, ev: SummarizationNumericalValidationEvent) -> SummarizationEvent:\n",
    "        \n",
    "        bad_summaries = []\n",
    "        sections = [x.text for x in ctx.data[\"new_rule_documents\"]]\n",
    "        for i in ctx.data[\"bad_summaries\"]:\n",
    "            goods=0\n",
    "            bads=0\n",
    "            numbers = re.findall(r'(?<!\\w)(\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?|\\d+\\.\\d+|\\d+)(?!\\w)', ev.summaries[i])\n",
    "            numbers = [x for x in numbers if len(x)>1]\n",
    "            for number in numbers:\n",
    "                if number not in sections[i]:\n",
    "                    bads+=1\n",
    "                    # print(f\"{number} not in section {i}!\")\n",
    "                else:\n",
    "                    goods+=1\n",
    "        \n",
    "            # TODO: this is a manual param. put in ctx.data\n",
    "            if bads>7:\n",
    "                bad_summaries.append(i)\n",
    "            print(f\"   Section {i} Goods: {goods}, Bads: {bads}\")\n",
    "\n",
    "        ctx.data[\"bad_summaries\"] = bad_summaries            \n",
    "        return SummarizationEvent(result=ev.result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
