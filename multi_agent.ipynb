{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent use case #1 - Financial data collection\n",
    "\n",
    "* Build dataset (possibly revisit information compression)\n",
    "  * Design targets using price data\n",
    "  * Build dataset justifying the target\n",
    "\n",
    "* Collect time series data\n",
    "  * preprocess \n",
    "  * Create features\n",
    "* Collect news\n",
    "  * Preprocess\n",
    "  * Extract trading info\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -U llama-index --upgrade --no-cache-dir --force-reinstall\n",
    "!pip3 install llama-parse llama-agents llama-index-llms-huggingface llama-index-llms-huggingface-api llama-index\n",
    "!pip3 install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, PromptTemplate\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from pydantic import BaseModel\n",
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "import os\n",
    "import ast\n",
    "\n",
    "\n",
    "from llama_parse import LlamaParse\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"Purpose: The primary role of this agent is to assist users by analyzing code. It should\n",
    "            be able to generate code and answer questions about code provided. \"\"\"\n",
    "\n",
    "code_parser_template = \"\"\"Parse the response from a previous LLM into a description and a string of valid code, \n",
    "                            also come up with a valid filename this could be saved as that doesnt contain special characters. \n",
    "                            Here is the response: {response}. You should parse this in the following JSON Format: \"\"\"\n",
    "\n",
    "from llama_index.core.tools import FunctionTool\n",
    "import os\n",
    "\n",
    "\n",
    "def code_reader_func(file_name):\n",
    "    path = os.path.join(\"data\", file_name)\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            content = f.read()\n",
    "            return {\"file_content\": content}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "code_reader = FunctionTool.from_defaults(\n",
    "    fn=code_reader_func,\n",
    "    name=\"code_reader\",\n",
    "    description=\"\"\"this tool can read the contents of code files and return \n",
    "    their results. Use this when you need to read the contents of a file\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "access_token = \"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\"\n",
    "LLAMA_API_KEY = 'llx-q5yu4FxUqUChxv4dpGKjSvgN7nSwb8rWPqWoxhkP0NqmQKmv'\n",
    "parser = LlamaParse(api_key=LLAMA_API_KEY, result_type=\"markdown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, PromptTemplate\n",
    "from llama_index.readers.file import PandasExcelReader\n",
    "from llama_index.core import set_global_tokenizer\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "set_global_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\").encode\n",
    ")\n",
    "\n",
    "filepath = \"/workspace/repos/stax_prod/range_scoring/MASTER - PYTHON - SCORING MODEL - MCG MADISON RIDGE DST - v2.0.xlsx\"\n",
    "documents = PandasExcelReader(sheet_name=\"5 - Disposition Analysis\").load_data(filepath)\n",
    "\n",
    "# mistralai/Mistral-7B-Instruct-v0.3\n",
    "model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "tokenizer_name = model_name\n",
    "llm = HuggingFaceLLM(model_name=model_name, tokenizer_name=tokenizer_name)\n",
    "\n",
    "# embed_model = resolve_embed_model(\"local:BAAI/bge-m3\")\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "vector_index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "query_engine = vector_index.as_query_engine(llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "from llama_index.core.prompts.base import ChatPromptTemplate\n",
    "\n",
    "query_str = \"What are the disposition fees? Only return the value.\"\n",
    "response = query_engine.query(query_str)\n",
    "print(str(response))\n",
    "# base_response = base_query_engine.query(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Forward NOI Growth \t2.00%\n",
    "#  Selling Costs \t1.00%\n",
    "#  Disposition Fee \t2.50%\n",
    "\t\n",
    "# \tAssumes. 0-yr Hold\n",
    "# \tScenario A\n",
    "# Net Operating Income\t 4,644,391 \n",
    "# Projected Terminal Cap Rate\t5.25%\n",
    "# Projected Sales Price (95%)\t 88,464,592 \n",
    "# Loan Assumption/Payoff\t -   \n",
    "# Selling Costs\t (884,646)\n",
    "# Disposition Fee\t (2,211,615)\n",
    "# Return of Forecasted Reserves\t -   \n",
    "# Sale Proceeds\t 85,368,331 \n",
    "# Proceeds from Distributions\t 36,688,942 \n",
    "# Return of Maximum Offering Amount\t (77,670,567)\n",
    "# DST Total Gain / (Loss)\t 44,386,707 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import logging\n",
    "\n",
    "# create a tool\n",
    "def get_the_secret_fact() -> str:\n",
    "    \"\"\"Returns the secret fact.\"\"\"\n",
    "    return \"The secret fact is: A baby llama is called a 'Cria'.\"\n",
    "\n",
    "tool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n",
    "\n",
    "# Define an agent\n",
    "model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "llm = HuggingFaceLLM(model_name=model_name)\n",
    "worker = FunctionCallingAgentWorker.from_tools([tool], llm=llm)\n",
    "agent = worker.as_agent()\n",
    "\n",
    "# Create an agent service\n",
    "agent_service = AgentService(\n",
    "    agent=agent,\n",
    "    message_queue=message_queue,\n",
    "    description=\"General purpose assistant\",\n",
    "    service_name=\"assistant\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
