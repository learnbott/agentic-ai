{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib, requests, random, time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from fp.fp import FreeProxy\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "\n",
    "# def extract_proxies(html):\n",
    "#     soup = BeautifulSoup(html, 'html.parser')\n",
    "#     text = soup.get_text()\n",
    "    \n",
    "#     # Regular expression to match IP addresses and ports\n",
    "#     proxy_pattern = re.compile(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}:[0-9]{1,5}\\b')\n",
    "#     proxies = proxy_pattern.findall(text)\n",
    "    \n",
    "#     return proxies\n",
    "\n",
    "# def fetch_proxies():\n",
    "#     url = \"https://free-proxy-list.net/\"\n",
    "#     response = requests.get(url)\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#     ips_ports = extract_proxies(soup.text)\n",
    "#     proxies = []\n",
    "\n",
    "#     for ip_port in ips_ports:\n",
    "#         proxy = f\"http://{ip_port}\"\n",
    "#         proxies.append(proxy)\n",
    "    \n",
    "#     return proxies\n",
    "\n",
    "\n",
    "class GoogleSearch:\n",
    "    def __init__(self, query: str) -> None:\n",
    "        self.query = query\n",
    "        escaped_query = urllib.parse.quote_plus(query)\n",
    "        self.URL = f\"https://www.google.com/search?q={escaped_query}\"\n",
    "\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3538.102 Safari/537.36\"\n",
    "        }\n",
    "        self.ua = UserAgent()\n",
    "        self.proxy = None\n",
    "        self.links = self.get_initial_links()\n",
    "        self.all_page_data = self.all_pages()\n",
    "\n",
    "    def get_random_user_agent(self):\n",
    "        self.headers[\"User-Agent\"] = self.ua.random\n",
    "        return None\n",
    "    \n",
    "    def get_random_proxy(self):\n",
    "        # return FreeProxy(https=True, google=True, timeout=1.0).get(repeat=False) country_id=['US', 'GB', 'CA', 'AU'], \n",
    "        return FreeProxy(google=True, https=True, timeout=20.0).get()\n",
    "\n",
    "    def clean_urls(self, anchors: list[str]) -> list[str]:\n",
    "\n",
    "        links: list[str] = []\n",
    "        for a in anchors:\n",
    "            links.append(\n",
    "                list(filter(lambda l: l.startswith(\"url=http\"), a[\"href\"].split(\"&\")))\n",
    "            )\n",
    "\n",
    "        links = [\n",
    "            link.split(\"url=\")[-1]\n",
    "            for sublist in links\n",
    "            for link in sublist\n",
    "            if len(link) > 0\n",
    "        ]\n",
    "\n",
    "        return links\n",
    "\n",
    "    def read_url_page(self, url: str) -> str:\n",
    "        self.get_random_user_agent()\n",
    "        # response = requests.get(url, headers=self.headers)\n",
    "        response = requests.get(url, headers=self.headers, proxies={\"https\": self.proxy, \"http\": self.proxy})\n",
    "        print(\"read page\", response.status_code)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        return soup.get_text(strip=True)\n",
    "\n",
    "    def get_initial_links(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        scrape google for the query with keyword based search\n",
    "        \"\"\"\n",
    "        # print(\"Searching Google...\")\n",
    "        self.proxy = self.get_random_proxy()\n",
    "        self.get_random_user_agent()\n",
    "        response = requests.get(self.URL, headers=self.headers, proxies={\"https\": self.proxy, \"http\": self.proxy}) #\"https\": proxy, \n",
    "        print(\"inital links\", response.status_code)\n",
    "        # response = requests.get(self.URL, headers=self.headers)\n",
    "        # Print the response status and content for debugging\n",
    "        code429 = 0\n",
    "        while True:\n",
    "            if response.status_code in [429, 400]:\n",
    "                print(f\"Got a {response.status_code}. Retrying.\")\n",
    "                retry_after = int(response.headers.get(\"Retry-After\", 4))  # Default to 60 seconds if not provided\n",
    "                print(f\"Rate limited. Retrying after {retry_after} seconds.\")\n",
    "                time.sleep(retry_after)\n",
    "                proxy = self.get_random_proxy()\n",
    "                print(proxy)\n",
    "                self.get_random_user_agent()\n",
    "                response = requests.get(self.URL, headers=self.headers, proxies={\"https\": proxy, \"http\": proxy}) #\"https\": proxy, \n",
    "                code429 += 1\n",
    "                if code429 > 5:\n",
    "                    print(\"Too many 429s. Exiting.\", response.status_code)\n",
    "                    break\n",
    "            else:\n",
    "                print(\"Finally got through\", response.status_code)\n",
    "                break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        anchors = soup.find_all(\"a\", href=True)\n",
    "        return self.clean_urls(anchors)\n",
    "\n",
    "    def all_pages(self) -> list[tuple[str, str]]:\n",
    "        print(self.links)\n",
    "        data: list[tuple[str, str]] = []\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "\n",
    "            future_to_url = {\n",
    "                executor.submit(self.read_url_page, url): url for url in self.links[:3]\n",
    "            }\n",
    "            for future in as_completed(future_to_url):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    output = future.result()\n",
    "                    data.append((url, output))\n",
    "\n",
    "                except requests.exceptions.HTTPError as e:\n",
    "                    print(e)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crawl4ai import AsyncWebCrawler\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from fake_useragent import UserAgent\n",
    "# https://pypi.org/project/googlesearch-python/\n",
    "from googlesearch import search\n",
    "import csv\n",
    "import random\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "verbose=False\n",
    "\n",
    "async def main(urls):\n",
    "    content = []\n",
    "    ua = UserAgent()\n",
    "    # Can add proxy in AsyncWebCrawler(verbose=False, proxy=\"http://127.0.0.1:7890\")\n",
    "    async with AsyncWebCrawler(verbose=verbose) as crawler:\n",
    "        for url in urls:\n",
    "            result = await crawler.arun(url=url, word_count_threshold=2000, user_agent=ua.random, verbose=verbose)\n",
    "            content.append(result)\n",
    "    return content\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    content = asyncio.run(main())\n",
    "\n",
    "\n",
    "def get_url_info(queries, num_results=3, num_words=2000, description_only=True):\n",
    "    urls = []\n",
    "    if isinstance(queries, str):\n",
    "        queries = [queries]\n",
    "    for query in queries:\n",
    "        for url in search(query, \n",
    "                          sleep_interval=random.randint(2, 5), \n",
    "                          num_results=num_results,\n",
    "                          timeout=1000,\n",
    "                          advanced=True):\n",
    "            if description_only:\n",
    "                urls.append(url.description)\n",
    "            else:\n",
    "                urls.append(url.url)\n",
    "\n",
    "    context = \"\"\n",
    "\n",
    "    if description_only:\n",
    "        for con in urls:\n",
    "            if con is not None:\n",
    "                context += \"\\n\" + con\n",
    "            else:\n",
    "                continue\n",
    "    else:\n",
    "        content = asyncio.run(main(urls))\n",
    "        for con in content:\n",
    "            if con.markdown is not None:\n",
    "                truncated_content = \" \".join(con.markdown.split()[:num_words])\n",
    "                context += \"\\n\" + truncated_content\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    return context\n",
    "\n",
    "def make_csv(results, filename, header=None, verbose=True):\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    # Open the file in append mode if it exists, otherwise write mode\n",
    "    with open(filename, 'a' if file_exists else 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write the header only if the file does not exist\n",
    "        if not file_exists and header is not None:\n",
    "            writer.writerow(header)\n",
    "        \n",
    "        # Write each row to the CSV file\n",
    "        for row in results:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    if verbose: \n",
    "        print(f\"Data has been {'appended to' if file_exists else 'written to'} {filename}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install scrapingbee fake_useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os\n",
    "import requests\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "scrapingbee_key = os.getenv(\"SCRAPINGBEE_API_KEY\")\n",
    "\n",
    "class ProxyWebScraper:\n",
    "    def __init__(self, dest_url, api_key, verbose=False):\n",
    "        self.verbose = verbose\n",
    "        self.api_key = api_key\n",
    "        self.dest_url = dest_url\n",
    "        self.ua = UserAgent()\n",
    "        self.session = requests.Session()\n",
    "        self.headers = {\"User-Agent\": self.ua.random}\n",
    "\n",
    "    # def get_random_user_agent(self):\n",
    "    #     self.headers[\"User-Agent\"] = self.ua.random\n",
    "    #     return None\n",
    "    \n",
    "    def send_request(self):\n",
    "        try:\n",
    "            # self.get_random_user_agent()\n",
    "            response = self.session.get(\n",
    "                url=\"https://app.scrapingbee.com/api/v1\",\n",
    "                params={\n",
    "                    \"url\": self.dest_url,\n",
    "                    \"api_key\": self.api_key,\n",
    "                    \"render_js\": False,\n",
    "                },\n",
    "                headers=self.headers\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            if self.verbose:\n",
    "                print(f'Response HTTP Status Code: {response.status_code}')\n",
    "                print(f'Response HTTP Response Body: {response.content}')\n",
    "            return response\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f'HTTP Request failed: {e}')\n",
    "            return None\n",
    "\n",
    "    def get_links(self, response):\n",
    "        if response is None:\n",
    "            return []\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup.find_all('a', href=True)\n",
    "\n",
    "    def get_sub_links(self, links):\n",
    "        domain_name = urllib.parse.urlsplit(self.dest_url).netloc\n",
    "        return [link['href'] for link in links if domain_name in link['href']]\n",
    "    \n",
    "    def update_url(self, new_url):\n",
    "        self.dest_url = new_url\n",
    "        return None\n",
    "    \n",
    "\n",
    "counter = 0\n",
    "start_time = time.time()\n",
    "hits = 200\n",
    "for i in range(hits):\n",
    "    scraper = ProxyWebScraper(dest_url=\"https://praxissolutions.com\", api_key=scrapingbee_key, verbose=False)\n",
    "    response = scraper.send_request()\n",
    "    links = scraper.get_links(response)\n",
    "    sub_links = scraper.get_sub_links(links)\n",
    "    counter += 1\n",
    "    time.sleep(random.randint(2, 20))\n",
    "    # While loop to simulate user behavior?\n",
    "    if np.random.choice([0, 1], p=[0.7, 0.3])==1:\n",
    "        sub_link = random.choice(sub_links)\n",
    "        full_url = urllib.parse.urljoin(response.url, sub_link)\n",
    "        scraper.update_url(full_url)\n",
    "        sub_response = scraper.send_request()\n",
    "        counter += 1\n",
    "    if i%20==0:\n",
    "        end_time = time.time()\n",
    "        print(f\"Round {i} - {counter} credits used.\\n   Time elapsed: {np.round((end_time-start_time)/60, 2)}.\\n   Time remaining: {np.round((end_time-start_time)/60/counter*(hits-i), 2)}\")\n",
    "print(f\"{counter} total credits used.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os, json\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "scrapingbee_key = os.getenv(\"SCRAPINGBEE_API_KEY\")\n",
    "\n",
    "class ProxyWebScraper:\n",
    "    def __init__(self, dest_url, api_key, verbose=False):\n",
    "        self.verbose = verbose\n",
    "        self.api_key = api_key\n",
    "        self.dest_url = dest_url\n",
    "        self.ua = UserAgent()\n",
    "        self.headers = {\"User-Agent\": self.ua.random}\n",
    "\n",
    "    async def send_request(self, session):\n",
    "        try:\n",
    "            async with session.get(\n",
    "                url=\"https://app.scrapingbee.com/api/v1\",\n",
    "                params={\n",
    "                    \"url\": self.dest_url,\n",
    "                    \"api_key\": self.api_key,\n",
    "                    \"render_js\": 'false',\n",
    "                },\n",
    "                headers=self.headers\n",
    "            ) as response:\n",
    "                response.raise_for_status()\n",
    "                if self.verbose:\n",
    "                    print(f'Response HTTP Status Code: {response.status}')\n",
    "                    print(f'Response HTTP Response Body: {await response.text()}')\n",
    "                return await response.text()\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f'HTTP Request failed: {e}')\n",
    "            return None\n",
    "\n",
    "    def get_links(self, response_text):\n",
    "        if response_text is None:\n",
    "            return []\n",
    "        soup = BeautifulSoup(response_text, 'html.parser')\n",
    "        return soup.find_all('a', href=True)\n",
    "\n",
    "    def get_sub_links(self, links):\n",
    "        domain_name = urllib.parse.urlsplit(self.dest_url).netloc\n",
    "        return [link['href'] for link in links if domain_name in link['href']]\n",
    "    \n",
    "    def update_url(self, new_url):\n",
    "        self.dest_url = new_url\n",
    "        return None\n",
    "    \n",
    "def send_request():\n",
    "    response = requests.get(\n",
    "        url=\"https://app.scrapingbee.com/api/v1/usage\",\n",
    "        params={\n",
    "            \"api_key\": \"6D5M0NNTHGA1A7PU7O5T4I5N62BHYV2PH3T7NIFZD6YENV69N55KTPCCYJR5AAKDOPCIBNFIQERIZE75\",\n",
    "        },\n",
    "\n",
    "    )\n",
    "    # print('   Response HTTP Status Code: ', response.status_code)\n",
    "    data_dict = json.loads(response.content)\n",
    "    print(f'   API credits: {data_dict[\"used_api_credit\"]}/{data_dict[\"max_api_credit\"]}')\n",
    "    print(f'   Concurrency: {data_dict[\"current_concurrency\"]}/{data_dict[\"max_concurrency\"]}')\n",
    "\n",
    "async def main(concurrent_requests=4):\n",
    "    counter = 0\n",
    "    start_time = time.time()\n",
    "    hits = 20\n",
    "    connector = aiohttp.TCPConnector(limit=concurrent_requests)\n",
    "    async with aiohttp.ClientSession(connector=connector) as session:\n",
    "        for i in range(hits):\n",
    "            scraper = ProxyWebScraper(dest_url=\"https://praxissolutions.com\", api_key=scrapingbee_key, verbose=False)\n",
    "            response_text = await scraper.send_request(session)\n",
    "            links = scraper.get_links(response_text)\n",
    "            sub_links = scraper.get_sub_links(links)\n",
    "            counter += 1\n",
    "            await asyncio.sleep(random.randint(2, 5))  # Sleep after the main request\n",
    "            if np.random.choice([0, 1], p=[0.7, 0.3]) == 1:\n",
    "                sub_link = random.choice(sub_links)\n",
    "                full_url = urllib.parse.urljoin(scraper.dest_url, sub_link)\n",
    "                scraper.update_url(full_url)\n",
    "                await asyncio.sleep(random.randint(1, 3))  # Sleep before the sub-request\n",
    "                sub_response_text = await scraper.send_request(session)\n",
    "                counter += 1\n",
    "            if i % 10 == 0:\n",
    "                end_time = time.time()\n",
    "                print(f\"Round {i} -- {counter} credits used.\\n   Time elapsed: {np.round((end_time-start_time)/60, 2)}.\\n   Time remaining: {np.round((end_time-start_time)/60/counter*(hits-i), 2)}\")\n",
    "                send_request()\n",
    "\n",
    "    print(f\"{counter} total credits used.\")\n",
    "    print(f\"Total Time elapsed: {np.round((time.time()-start_time)/60, 2)}\")\n",
    "\n",
    "# Run the main function\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0\n",
      "   Time elapsed: 0.16.\n",
      "   Time remaining: 1.61\n",
      "   API credits: 385/1000\n",
      "   Concurrency: 0/5\n",
      "\n",
      "Round 10\n",
      "   Time elapsed: 1.23.\n",
      "   Time remaining: 0.77\n",
      "   API credits: 396/1000\n",
      "   Concurrency: 0/5\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Ending stats:\n",
      "  Concurrency: 0/5\n",
      "  API credits: 409/1000\n",
      "  Total credits used: 24\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'start_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 117\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  API credits: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfin[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused_api_credit\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfin[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_api_credit\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Total credits used: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfin[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mused_api_credit\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m-\u001b[39mbegin[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mused_api_credit\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Total Time elapsed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mround((time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39m\u001b[43mstart_time\u001b[49m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'start_time' is not defined"
     ]
    }
   ],
   "source": [
    "import time, os, json\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import numpy as np\n",
    "import random\n",
    "import requests\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "scrapingbee_key = os.getenv(\"SCRAPINGBEE_API_KEY\")\n",
    "\n",
    "class ProxyWebScraper:\n",
    "    def __init__(self, dest_url, api_key, verbose=False):\n",
    "        self.verbose = verbose\n",
    "        self.api_key = api_key\n",
    "        self.dest_url = dest_url\n",
    "        self.ua = UserAgent()\n",
    "        self.headers = {\"User-Agent\": self.ua.random}\n",
    "\n",
    "    async def send_request(self, session):\n",
    "        try:\n",
    "            async with session.get(\n",
    "                url=\"https://app.scrapingbee.com/api/v1\",\n",
    "                params={\n",
    "                    \"url\": self.dest_url,\n",
    "                    \"api_key\": self.api_key,\n",
    "                    \"render_js\": 'false',\n",
    "                },\n",
    "                headers=self.headers\n",
    "            ) as response:\n",
    "                response.raise_for_status()\n",
    "                if self.verbose:\n",
    "                    print(f'Response HTTP Status Code: {response.status}')\n",
    "                    print(f'Response HTTP Response Body: {await response.text()}')\n",
    "                return await response.text()\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f'HTTP Request failed: {e}')\n",
    "            return None\n",
    "\n",
    "    def get_links(self, response_text):\n",
    "        if response_text is None:\n",
    "            return []\n",
    "        soup = BeautifulSoup(response_text, 'html.parser')\n",
    "        return soup.find_all('a', href=True)\n",
    "\n",
    "    def get_sub_links(self, links):\n",
    "        domain_name = urllib.parse.urlsplit(self.dest_url).netloc\n",
    "        return [link['href'] for link in links if domain_name in link['href']]\n",
    "    \n",
    "    def update_url(self, new_url):\n",
    "        self.dest_url = new_url\n",
    "        return None\n",
    "    \n",
    "def send_request(api_key, verbose=True):\n",
    "    response = requests.get(\n",
    "        url=\"https://app.scrapingbee.com/api/v1/usage\",\n",
    "        params={\n",
    "            \"api_key\": api_key,\n",
    "        },\n",
    "\n",
    "    )\n",
    "    # print('   Response HTTP Status Code: ', response.status_code)\n",
    "    data_dict = json.loads(response.content)\n",
    "    if verbose: \n",
    "        print(f'   API credits: {data_dict[\"used_api_credit\"]}/{data_dict[\"max_api_credit\"]}')\n",
    "        print(f'   Concurrency: {data_dict[\"current_concurrency\"]}/{data_dict[\"max_concurrency\"]}')\n",
    "        return None\n",
    "    else:\n",
    "        return data_dict\n",
    "\n",
    "async def main(concurrent_requests=4, hits=20, print_every=10):\n",
    "    counter = 0\n",
    "    start_time = time.time()\n",
    "    connector = aiohttp.TCPConnector(limit=concurrent_requests)\n",
    "    async with aiohttp.ClientSession(connector=connector) as session:\n",
    "        for i in range(hits):\n",
    "            scraper = ProxyWebScraper(dest_url=\"https://praxissolutions.com\", api_key=scrapingbee_key, verbose=False)\n",
    "            response_text = await scraper.send_request(session)\n",
    "            links = scraper.get_links(response_text)\n",
    "            sub_links = scraper.get_sub_links(links)\n",
    "            counter += 1\n",
    "            del response_text\n",
    "            await asyncio.sleep(random.randint(2, 5))  # Sleep after the main request\n",
    "            if np.random.choice([0, 1], p=[0.7, 0.3]) == 1:\n",
    "                while True:\n",
    "                    sub_link = random.choice(sub_links)\n",
    "                    full_url = urllib.parse.urljoin(scraper.dest_url, sub_link)\n",
    "                    scraper.update_url(full_url)\n",
    "                    await asyncio.sleep(random.randint(1, 3))  # Sleep before the sub-request\n",
    "                    _ = await scraper.send_request(session)\n",
    "                    counter += 1\n",
    "                    \n",
    "                    if np.random.choice([0, 1], p=[0.5, 0.5]) == 1:\n",
    "                        break\n",
    "            if i % print_every == 0:\n",
    "                end_time = time.time()\n",
    "                print(f\"Round {i}\\n   Time elapsed: {np.round((end_time-start_time)/60, 2)}.\\n   Time remaining: {np.round((end_time-start_time)/60/counter*(hits-i), 2)}\")\n",
    "                send_request(scrapingbee_key)\n",
    "                print()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    begin = send_request(scrapingbee_key, verbose=False)\n",
    "    asyncio.run(main())\n",
    "    fin = send_request(scrapingbee_key, verbose=False)\n",
    "    print()\n",
    "    print('--'*30)\n",
    "    print('Ending stats:')\n",
    "    print(f'  Concurrency: {fin[\"current_concurrency\"]}/{fin[\"max_concurrency\"]}')\n",
    "    print(f'  API credits: {fin[\"used_api_credit\"]}/{fin[\"max_api_credit\"]}')\n",
    "    print(f\"  Total credits used: {fin['used_api_credit']-begin['used_api_credit']}\")\n",
    "    print(f\"  Total Time elapsed: {np.round((time.time()-start_time)/60, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
