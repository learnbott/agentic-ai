{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for vast ai - enter in terminal\n",
    "!python3 -m pip install ipykernel -U --user --force-reinstall && apt update && apt install -y python3-pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip3 install flash-attn --no-build-isolation\n",
    "!pip3 install llama-index llama-parse llama-index-embeddings-huggingface llama-index-llms-huggingface dspy-ai openpyxl langchain chromadb\n",
    "!pip3 install sentencepiece protobuf evaluate rouge_score absl-py tensorboardX bitsandbytes peft accelerate\n",
    "!cp /workspace/repos/agentic-ai/MASTER\\ -\\ PYTHON\\ -\\ SCORING\\ MODEL\\ -\\ MCG\\ MADISON\\ RIDGE\\ DST\\ -\\ v2.0.xlsx /workspace/data\n",
    "!cp /workspace/repos/agentic-ai/PPM\\ -\\ MCG\\ MADISON\\ RIDGE\\ DST.pdf /workspace/data\n",
    "!pip3 uninstall -y torch\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "import dspy\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.datasets.hotpotqa import HotPotQA\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, StorageContext\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from llama_index.readers.file import PandasExcelReader\n",
    "CHROMA_COLLECTION_NAME = \"blockchain_and_ai\"\n",
    "CHROMADB_DIR = \"/workspace/data/db/\"\n",
    "\n",
    "from typing import List, Any, Callable, Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from dspy.retrieve.chromadb_rm import ChromadbRM\n",
    "\n",
    "from train_utils import get_csv_string, randomize_row_values, operators_dict, range_description_json, split_df_by_empty_columns, split_df_by_empty_rows, print_trainable_parameters\n",
    "from models import SpreadSheetAnalyzer\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/workspace/data/MASTER - PYTHON - SCORING MODEL - MCG MADISON RIDGE DST - v2.0.xlsx\"\n",
    "disposition_inputs = [\n",
    "  \"Selling Costs\",\n",
    "  \"Disposition Fee\",\n",
    "  \"Net Operating Income\",\n",
    "  \"Loan Assumption/Payoff\",\n",
    "  \"Return of Forecasted Reserves\",\n",
    "  \"CF Y 11\",\n",
    "  \"Return of Maximum Offering Amount\",\n",
    "  \"Projected Terminal Cap Rate\",\n",
    "  \"Cash Flows\"\n",
    "]\n",
    "dfs = pd.read_excel(filepath, sheet_name=\"5 - Disposition Analysis\", header=None)\n",
    "# Splitting the DataFrame by empty columns\n",
    "sub_dfs_by_columns = split_df_by_empty_columns(dfs)\n",
    "\n",
    "# Splitting each sub-DataFrame by empty rows\n",
    "final_split_dfs = []\n",
    "for sub_df in sub_dfs_by_columns:\n",
    "    split_sub_dfs = split_df_by_empty_rows(sub_df)\n",
    "    final_split_dfs.extend([get_csv_string(x) for x in split_sub_dfs if not x.empty])\n",
    "\n",
    "dfs.dropna(axis=0, how='all', inplace=True)\n",
    "dfs.dropna(axis=1, how='all', inplace=True)\n",
    "fee_columns = ['Disposition Fee', 'Selling Costs']\n",
    "cashflow_columns = [1,2,3,4,5,6,7,8,9]\n",
    "ground_truth = dfs[dfs[1].isin(disposition_inputs+cashflow_columns)].iloc[:, :2] # Get only the necessary columns\n",
    "ground_truth.drop(labels=[16, 17], axis=0, inplace=True) # drop the duplicate Selling and Disposition Costs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = os.getenv('HF_TOKEN')\n",
    "llama_api_key = os.getenv('LLAMA_API_KEY')\n",
    "\n",
    "print('first model load...')\n",
    "# model_name = \"EleutherAI/gpt-neo-125m\"\n",
    "# model_name = \"microsoft/Phi-3-mini-128k-instruct\" # 128K context window\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\" # 8K context window\n",
    "# model_name = \"clibrain/mamba-2.8b-instruct-openhermes\" # 8K context window\n",
    "model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.3\" # 32K context window\n",
    "llm = dspy.HFModel(model=model_name, hf_device_map='auto', token=access_token)\n",
    "llm.kwargs['max_new_tokens']=50\n",
    "# llm.kwargs['repetition_penalty']=1.1\n",
    "llm.kwargs['temperature']=None\n",
    "llm.kwargs['do_sample']=False\n",
    "llm.kwargs['top_k']=None\n",
    "# llm.kwargs['typical_p']=0.9\n",
    "\n",
    "print('deleting model...')\n",
    "llm.model=None\n",
    "gc.collect()\n",
    "print('reloading model...')\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "llm.model=AutoModelForCausalLM.from_pretrained(model_name, quantization_config=None, \n",
    "                                               trust_remote_code=True, device_map=\"auto\", \n",
    "                                               attn_implementation=\"flash_attention_2\",  \n",
    "                                               torch_dtype=torch.bfloat16)\n",
    "\n",
    "\n",
    "# from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=32,\n",
    "#     target_modules=[\"k_proj\", \"v_proj\", \"q_proj\", \"o_proj\"], # Mistral param names\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\", #\"none\", \"all\", \"lora_only\"\n",
    "#     task_type=\"CAUSAL_LM\", \n",
    "    \n",
    "# )\n",
    "\n",
    "# llm.model = prepare_model_for_kbit_training(llm.model)\n",
    "# llm.model = get_peft_model(llm.model, config)\n",
    "# print_trainable_parameters(llm.model)\n",
    "\n",
    "if model_name == 'mistralai/Mistral-7B-Instruct-v0.3':\n",
    "    llm.model.generation_config.pad_token_id = llm.tokenizer.eos_token_id\n",
    "    llm.tokenizer.pad_token_id = llm.tokenizer.eos_token_id\n",
    "\n",
    "# dspy.settings.configure(lm=llm)\n",
    "\n",
    "######## RAG model\n",
    "# chroma_client = chromadb.PersistentClient(path=CHROMADB_DIR)\n",
    "# collection = chroma_client.get_or_create_collection(name=CHROMA_COLLECTION_NAME)\n",
    "# # text_splitter = SentenceTransformersTokenTextSplitter(tokens_per_chunk=100)\n",
    "\n",
    "# ids = []\n",
    "# documents = []\n",
    "# metadatas = []\n",
    "# # dfs_str = get_csv_string(dfs)\n",
    "# # chunks = text_splitter.create_documents([dfs_str], )\n",
    "# for chunk_no, chunk in enumerate(final_split_dfs):\n",
    "#     ids.append(f\"{chunk_no}\")\n",
    "#     documents.append(chunk)\n",
    "#     # metadatas.append({\"title\":})\n",
    "# if ids:\n",
    "#     collection.upsert(ids=ids, documents=documents)#, metadatas=metadatas)\n",
    "\n",
    "# retriever = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "# default_ef = embedding_functions.HuggingFaceEmbeddingFunction(model_name='colbert-ir/colbertv2.0', api_key=access_token)\n",
    "# default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "# retriever = ChromadbRM(CHROMA_COLLECTION_NAME, CHROMADB_DIR, default_ef, k=3)\n",
    "\n",
    "# dspy.settings.configure(lm=llm, rm=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from dspy.retrieve.llama_index_rm import LlamaIndexRM\n",
    "\n",
    "embed_model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "retrieve = LlamaIndexRM(embed_model_name, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.settings.configure(lm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = os.getenv('HF_TOKEN')\n",
    "llama_api_key = os.getenv('LLAMA_API_KEY')\n",
    "parser = LlamaParse(\n",
    "    api_key=llama_api_key,\n",
    "        result_type=\"text\",\n",
    "        language=\"en\",\n",
    "        varbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!rm -rf /root/.cache/huggingface/hub/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip3 install llama-index-embeddings-text-embeddings-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.3\" # 32K context window\n",
    "# model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "# tokenizer_name = model_name\n",
    "# rm_llm = HuggingFaceLLM(model=dspy_llm.llm, tokenizer_name=tokenizer_name, is_chat_model=True, device_map='auto', max_new_tokens=50, context_window=8000)\n",
    "# rm_llm = HuggingFaceLLM(model_name=model_name, tokenizer_name=tokenizer_name, is_chat_model=True, device_map='auto', max_new_tokens=50, context_window=8000)\n",
    "filepath = \"/workspace/data/MASTER - PYTHON - SCORING MODEL - MCG MADISON RIDGE DST - v2.0.xlsx\"\n",
    "documents = PandasExcelReader(sheet_name=\"5 - Disposition Analysis\").load_data(filepath)\n",
    "# documents = parser.load_data(\"/workspace/data/PPM - MCG MADISON RIDGE DST.pdf\")\n",
    "print(\"Documents created\")\n",
    "\n",
    "# Settings.llm = rm_llm\n",
    "Settings.chunk_size = 200\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "embed_model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "# embed_model_name = \"Alibaba-NLP/gte-Qwen1.5-7B-instruct\"\n",
    "embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "\n",
    "# embed_model.num_workers = 1\n",
    "\n",
    "# KeywordTableSimpleRetriever\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "index.storage_context.persist(persist_dir=\"/workspace/data/storage/alpha\")\n",
    "query_engine = index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# index.set_index_id(\"vector_index\")\n",
    "# index.storage_context.persist(\"/workspace/data/storage\")\n",
    "# storage_context = StorageContext.from_defaults(persist_dir=\"/workspace/data/storage\")\n",
    "# index = load_index_from_storage(storage_context, index_id=\"vector_index\")\n",
    "# query_engine = index.as_query_engine(response_mode=\"tree_summarize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dspy_llm = DspyLlamaIndexWrapper(rm_llm, model_type='chat', max_new_tokens=30)\n",
    "dspy.settings.configure(lm=dspy_llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: Get the value for Return of Maximum Offering Amount.\n",
    "# Extracted values: Return of Maximum Offering Amount: 44386706.96773932\n",
    "# Question: What is the return on maximum offering amount? Please provide a floating point number less than zero.\n",
    "# Extracted values: Return of Maximum Offering Amount: -77670566.54709445"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_collect = {}\n",
    "for row,col in ground_truth.iterrows():\n",
    "    if isinstance(col.values[0], int):\n",
    "        name = f\"Cashflows {col.values[0]}\"\n",
    "    else:\n",
    "        name = col.values[0]\n",
    "    value = col.values[1]\n",
    "    gt_collect[name] = str(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# dfs_str = get_csv_string(dfs)\n",
    "num_rounds = 10\n",
    "train_data = []\n",
    "for _ in range(num_rounds):\n",
    "    # TODO: gradually increase n_samples, random fill in of values in range\n",
    "    # dfs_aug = randomize_row_values(dfs, ground_truth=ground_truth, n_samples=15)\n",
    "    # dfs_str = get_csv_string(dfs_aug)\n",
    "    # dfs_str = get_csv_string(dfs)\n",
    "    \n",
    "    for value_to_extract in gt_collect:\n",
    "\n",
    "        question = f\"Extract the value for the variable name '{value_to_extract}'?\"\n",
    "        answer = f\"{value_to_extract}: {gt_collect[value_to_extract]}\"\n",
    "        train_data.append(dspy.Example(question=question, answer=answer).with_inputs('question'))\n",
    "    \n",
    "random.shuffle(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_data = \"\"\n",
    "for string in final_split_dfs:\n",
    "    input_data += string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train_utils import operators_dict, range_description_json\n",
    "from models_testing import SpreadSheetAnalyzer\n",
    "spreadsheeet_ananlyst = SpreadSheetAnalyzer(range_description_json, operators_dict, query_engine=query_engine, num_passages=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in dspy.primitives.module.py\n",
    "# def reset_copy(self):\n",
    "#     import llama_index\n",
    "#     obj = copy.deepcopy(self)\n",
    "#     ######################################################\n",
    "#     for attribute_name in dir(obj):\n",
    "#         if not attribute_name.startswith('_'):\n",
    "#             attribute_value = getattr(obj, attribute_name)\n",
    "#             if isinstance(attribute_value, llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever):\n",
    "#                 setattr(obj, attribute_name, getattr(self, attribute_name))\n",
    "#     ######################################################\n",
    "#     for param in obj.parameters():\n",
    "#         param.reset()\n",
    "\n",
    "#     return obj\n",
    "\n",
    "############################or############################# \n",
    "\n",
    "# in llama_index/core/schema.py comment out line 84\n",
    "# 84  state[\"__private_attribute_values__\"] = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch, BootstrapFinetune, BootstrapFewShot\n",
    "perc_train = 0.7\n",
    "num_train = int(len(train_data) * perc_train)\n",
    "def validate_answer(pred, example, trace=None):\n",
    "    return example.answer.lower() == pred.answer.lower()\n",
    "metric = dspy.evaluate.metrics.answer_exact_match\n",
    "# metric = validate_answer\n",
    "\n",
    "#Configure model to finetune\n",
    "config = dict(target=model_name, epochs=10, bf16=True, bsize=1, accumsteps=3, lr=8e-5) #path_prefix=None\n",
    "\n",
    "#Compile program on BootstrapFinetune\n",
    "finetune_optimizer = BootstrapFinetune(metric=metric)\n",
    "# finetune_optimizer = BootstrapFewShot(metric=metric)\n",
    "\n",
    "finetune_program = finetune_optimizer.compile(spreadsheeet_ananlyst, trainset=train_data, **config)\n",
    "# finetune_program = finetune_optimizer.compile(spreadsheeet_ananlyst, trainset=train_data[:22])\n",
    "\n",
    "# finetune_program = spreadsheeet_ananlyst\n",
    "\n",
    "# #Load program and activate model's parameters in program before evaluation\n",
    "# ckpt_path = \"saved_checkpoint_path_from_finetuning\"\n",
    "# LM = dspy.HFModel(checkpoint=ckpt_path, model=model_name)\n",
    "\n",
    "# for p in finetune_program.predictors():\n",
    "#     p.lm = LM\n",
    "#     p.activated = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load program and activate model's parameters in program before evaluation\n",
    "ckpt_path = \"/workspace/repos/finetuning_ckpts/7AO0W3ZSHO9ZJ.all/checkpoint-70\"\n",
    "LM = dspy.HFModel(checkpoint=ckpt_path, model=model_name, hf_device_map='cuda:0')\n",
    "\n",
    "for p in spreadsheeet_ananlyst.predictors():\n",
    "    p.lm = LM\n",
    "    p.activated = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perc_train = 0.7\n",
    "# num_train = int(len(train_data) * perc_train)\n",
    "# metric = dspy.evaluate.metrics.answer_exact_match\n",
    "\n",
    "scores = []\n",
    "for x in train_data[num_train:num_train+34]:\n",
    "    pred = finetune_program(**x.inputs())\n",
    "    score = metric(x, pred)\n",
    "    scores.append(score)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saved_checkpoint_path_from_finetuning = '/workspace/repos/finetuning_ckpts/NFAI903XCHAMQ.all/checkpoint-53'\n",
    "llm.model=None\n",
    "llm.model=AutoModelForCausalLM.from_pretrained(saved_checkpoint_path_from_finetuning, quantization_config=None, \n",
    "                                               trust_remote_code=True, device_map=\"auto\", \n",
    "                                               attn_implementation=\"flash_attention_2\",  \n",
    "                                               torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train_utils import operators_dict, range_description_json\n",
    "from models_testing import SpreadSheetAnalyzer\n",
    "spreadsheeet_ananlyst = SpreadSheetAnalyzer(range_description_json, operators_dict, query_engine=query_engine, num_passages=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_data = \"\"\n",
    "for string in final_split_dfs:\n",
    "    input_data += string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_str = get_csv_string(dfs)\n",
    "collection = []\n",
    "for value_to_extract in gt_collect:\n",
    "    question = f\"Extract the value for the variable name '{value_to_extract}'?\"\n",
    "    print(question)\n",
    "    pred = spreadsheeet_ananlyst(question, verbose=True)\n",
    "    collection.append((pred, f\"{value_to_extract}: {gt_collect[value_to_extract]}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_str = get_csv_string(dfs)\n",
    "collection = []\n",
    "for value_to_extract in gt_collect:\n",
    "    question = f\"Extract the value for the variable name '{value_to_extract}'?\"\n",
    "    print(question)\n",
    "    pred = finetune_program(question, verbose=True)\n",
    "    collection.append((pred, f\"{value_to_extract}: {gt_collect[value_to_extract]}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in collection:\n",
    "    print(i[0].answer,\"---\", i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([i[0].answer == i[1] for i in collection])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline RAG and extractor only 0.35294117647058826\n",
    "# baseline RAG, extractor, float and format checks 0.35294117647058826"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt.signature_opt_typed import optimize_signature\n",
    "from dspy.evaluate.metrics import answer_exact_match\n",
    "from dspy.functional import TypedChainOfThought\n",
    "\n",
    "compiled_program = optimize_signature(\n",
    "    student=TypedChainOfThought(\"question -> answer\"),\n",
    "    evaluator=Evaluate(devset=devset, metric=answer_exact_match, num_threads=10, display_progress=True),\n",
    "    n_iterations=50,\n",
    ").program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch, BootstrapFinetune\n",
    "perc_train = 0.7\n",
    "num_train = int(len(train_data) * perc_train)\n",
    "metric = dspy.evaluate.metrics.answer_exact_match\n",
    "\n",
    "#Compile program on current dspy.settings.lm\n",
    "fewshot_optimizer = BootstrapFewShotWithRandomSearch(metric=metric, max_bootstrapped_demos=2, num_threads=1)\n",
    "your_dspy_program_compiled = tp.compile(spreadsheeet_ananlyst, trainset=train_data[:num_train], valset=train_data[num_train:])\n",
    "\n",
    "#Configure model to finetune\n",
    "config = dict(target=llm.model, epochs=2, bf16=True, bsize=1, accumsteps=2, lr=5e-5)\n",
    "\n",
    "#Compile program on BootstrapFinetune\n",
    "finetune_optimizer = BootstrapFinetune(metric=metric)\n",
    "finetune_program = finetune_optimizer.compile(spreadsheeet_ananlyst, trainset=some_new_dataset_for_finetuning_model, **config)\n",
    "\n",
    "finetune_program = spreadsheeet_ananlyst\n",
    "\n",
    "#Load program and activate model's parameters in program before evaluation\n",
    "ckpt_path = \"saved_checkpoint_path_from_finetuning\"\n",
    "LM = dspy.HFModel(checkpoint=ckpt_path, model=llm.model)\n",
    "\n",
    "for p in finetune_program.predictors():\n",
    "    p.lm = LM\n",
    "    p.activated = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this code implements a wrapper around the llama_index library to emulate a dspy llm\n",
    "\n",
    "this allows the llama_index library to be used in the dspy framework since dspy has limited support for LLMs\n",
    "\n",
    "This code is a slightly modified copy of dspy/dsp/modules/azure_openai.py\n",
    "\n",
    "The way this works is simply by creating a dummy openai client that wraps around any llama_index LLM object and implements .complete and .chat\n",
    "\n",
    "tested with python 3.12\n",
    "\n",
    "dspy==0.1.4\n",
    "dspy-ai==2.4.9\n",
    "llama-index==0.10.35\n",
    "llama-index-llms-openai==0.1.18\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from typing import Any, Literal\n",
    "\n",
    "from easydict import EasyDict\n",
    "from llama_index.core.base.llms.types import ChatMessage\n",
    "from llama_index.core.llms import LLM\n",
    "\n",
    "\n",
    "def LlamaIndexOpenAIClientWrapper(llm: LLM):\n",
    "    def chat(messages: list[ChatMessage], **kwargs) -> Any:\n",
    "        return llm.chat([ChatMessage(**message) for message in messages], **kwargs)\n",
    "\n",
    "    def complete(prompt: str, **kwargs) -> Any:\n",
    "        return llm.complete(prompt, **kwargs)\n",
    "\n",
    "    client = EasyDict(\n",
    "        {\n",
    "            'chat': EasyDict({'completions': EasyDict({'create': chat})}),\n",
    "            'completion': EasyDict({'create': complete}),\n",
    "            'ChatCompletion': EasyDict({'create': chat}),\n",
    "            'Completion': EasyDict({'create': complete}),\n",
    "        }\n",
    "    )\n",
    "    return client\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(message)s',\n",
    "    handlers=[logging.FileHandler('azure_openai_usage.log')],\n",
    ")\n",
    "\n",
    "import functools\n",
    "import json\n",
    "from typing import Any, Literal\n",
    "\n",
    "import backoff\n",
    "import dsp\n",
    "import openai\n",
    "from dsp.modules.cache_utils import CacheMemory, NotebookCacheMemory, cache_turn_on\n",
    "from dsp.modules.lm import LM\n",
    "\n",
    "try:\n",
    "    OPENAI_LEGACY = int(openai.version.__version__[0]) == 0\n",
    "except Exception:\n",
    "    OPENAI_LEGACY = True\n",
    "\n",
    "try:\n",
    "    import openai.error\n",
    "    from openai.openai_object import OpenAIObject\n",
    "\n",
    "    ERRORS = (\n",
    "        openai.error.RateLimitError,\n",
    "        openai.error.ServiceUnavailableError,\n",
    "        openai.error.APIError,\n",
    "    )\n",
    "except Exception:\n",
    "    ERRORS = (openai.RateLimitError, openai.APIError)\n",
    "    OpenAIObject = dict\n",
    "\n",
    "\n",
    "def backoff_hdlr(details):\n",
    "    \"\"\"Handler from https://pypi.org/project/backoff/\"\"\"\n",
    "    print(\n",
    "        'Backing off {wait:0.1f} seconds after {tries} tries ' 'calling function {target} with kwargs ' '{kwargs}'.format(**details),\n",
    "    )\n",
    "\n",
    "\n",
    "class DspyLlamaIndexWrapper(LM):\n",
    "    \"\"\"Wrapper around Azure's API for OpenAI.\n",
    "\n",
    "    Args:\n",
    "        api_base (str): Azure URL endpoint for model calling, often called 'azure_endpoint'.\n",
    "        api_version (str): Version identifier for API.\n",
    "        model (str, optional): OpenAI or Azure supported LLM model to use. Defaults to \"text-davinci-002\".\n",
    "        api_key (Optional[str], optional): API provider Authentication token. use Defaults to None.\n",
    "        model_type (Literal[\"chat\", \"text\"], optional): The type of model that was specified. Mainly to decide the optimal prompting strategy. Defaults to \"chat\".\n",
    "        **kwargs: Additional arguments to pass to the API provider.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: LLM,\n",
    "        model_type: Literal['chat', 'text'] = 'chat',\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(llm._model)\n",
    "        self.provider = 'openai'\n",
    "\n",
    "        self.llm = llm\n",
    "        self.client = LlamaIndexOpenAIClientWrapper(llm)\n",
    "        model = llm._model\n",
    "        self.model_type = model_type\n",
    "\n",
    "        # if not OPENAI_LEGACY and \"model\" not in kwargs:\n",
    "        #     if \"deployment_id\" in kwargs:\n",
    "        #         kwargs[\"model\"] = kwargs[\"deployment_id\"]\n",
    "        #         del kwargs[\"deployment_id\"]\n",
    "\n",
    "        #     if \"api_version\" in kwargs:\n",
    "        #         del kwargs[\"api_version\"]\n",
    "\n",
    "        if 'model' not in kwargs:\n",
    "            kwargs['model'] = model\n",
    "\n",
    "        self.kwargs = {\n",
    "            'temperature': 0.0,\n",
    "            'max_tokens': 150,\n",
    "            'top_p': 1,\n",
    "            'frequency_penalty': 0,\n",
    "            'presence_penalty': 0,\n",
    "            'n': 1,\n",
    "            **kwargs,\n",
    "        }  # TODO: add kwargs above for </s>\n",
    "\n",
    "        self.history: list[dict[str, Any]] = []\n",
    "\n",
    "    def _openai_client(self):\n",
    "        # if OPENAI_LEGACY:\n",
    "        #     return openai\n",
    "\n",
    "        return self.client\n",
    "\n",
    "    def log_usage(self, response):\n",
    "        \"\"\"Log the total tokens from the Azure OpenAI API response.\"\"\"\n",
    "        usage_data = response.get('usage')\n",
    "        if usage_data:\n",
    "            total_tokens = usage_data.get('total_tokens')\n",
    "            logging.info(f'{total_tokens}')\n",
    "\n",
    "    def basic_request(self, prompt: str, **kwargs):\n",
    "        raw_kwargs = kwargs\n",
    "\n",
    "        kwargs = {**self.kwargs, **kwargs}\n",
    "        if self.model_type == 'chat':\n",
    "            # caching mechanism requires hashable kwargs\n",
    "            kwargs['messages'] = [{'role': 'user', 'content': prompt}]\n",
    "            kwargs = {'stringify_request': json.dumps(kwargs)}\n",
    "            # response = chat_request(self.client, **kwargs)\n",
    "            # if OPENAI_LEGACY:\n",
    "            #     return _cached_gpt3_turbo_request_v2_wrapped(**kwargs)\n",
    "            # else:\n",
    "            return v1_chat_request(self.client, **kwargs)\n",
    "\n",
    "        else:\n",
    "            kwargs['prompt'] = prompt\n",
    "            response = self.completions_request(**kwargs)\n",
    "\n",
    "        history = {\n",
    "            'prompt': prompt,\n",
    "            'response': response,\n",
    "            'kwargs': kwargs,\n",
    "            'raw_kwargs': raw_kwargs,\n",
    "        }\n",
    "        self.history.append(history)\n",
    "\n",
    "        return response\n",
    "\n",
    "    @backoff.on_exception(\n",
    "        backoff.expo,\n",
    "        ERRORS,\n",
    "        max_time=1000,\n",
    "        on_backoff=backoff_hdlr,\n",
    "    )\n",
    "    def request(self, prompt: str, **kwargs):\n",
    "        \"\"\"Handles retrieval of GPT-3 completions whilst handling rate limiting and caching.\"\"\"\n",
    "        if 'model_type' in kwargs:\n",
    "            del kwargs['model_type']\n",
    "\n",
    "        return self.basic_request(prompt, **kwargs)\n",
    "\n",
    "    def _get_choice_text(self, choice: dict[str, Any]) -> str:\n",
    "        if self.model_type == 'chat':\n",
    "            return choice['message']['content']\n",
    "        return choice['text']\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        only_completed: bool = True,\n",
    "        return_sorted: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> list[dict[str, Any]]:\n",
    "        \"\"\"Retrieves completions from OpenAI Model.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): prompt to send to GPT-3\n",
    "            only_completed (bool, optional): return only completed responses and ignores completion due to length. Defaults to True.\n",
    "            return_sorted (bool, optional): sort the completion choices using the returned probabilities. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            list[dict[str, Any]]: list of completion choices\n",
    "        \"\"\"\n",
    "\n",
    "        assert only_completed, 'for now'\n",
    "        assert return_sorted is False, 'for now'\n",
    "\n",
    "        response = self.request(prompt, **kwargs)\n",
    "\n",
    "        try:\n",
    "            if dsp.settings.log_openai_usage:\n",
    "                self.log_usage(response)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        choices = response['choices']\n",
    "\n",
    "        completed_choices = [c for c in choices if c['finish_reason'] != 'length']\n",
    "\n",
    "        if only_completed and len(completed_choices):\n",
    "            choices = completed_choices\n",
    "\n",
    "        completions = [self._get_choice_text(c) for c in choices]\n",
    "        if return_sorted and kwargs.get('n', 1) > 1:\n",
    "            scored_completions = []\n",
    "\n",
    "            for c in choices:\n",
    "                tokens, logprobs = (\n",
    "                    c['logprobs']['tokens'],\n",
    "                    c['logprobs']['token_logprobs'],\n",
    "                )\n",
    "\n",
    "                if '<|endoftext|>' in tokens:\n",
    "                    index = tokens.index('<|endoftext|>') + 1\n",
    "                    tokens, logprobs = tokens[:index], logprobs[:index]\n",
    "\n",
    "                avglog = sum(logprobs) / len(logprobs)\n",
    "                scored_completions.append((avglog, self._get_choice_text(c)))\n",
    "\n",
    "            scored_completions = sorted(scored_completions, reverse=True)\n",
    "            completions = [c for _, c in scored_completions]\n",
    "\n",
    "        return completions\n",
    "\n",
    "    def completions_request(self, **kwargs):\n",
    "        # if OPENAI_LEGACY:\n",
    "        #     return cached_gpt3_request_v2_wrapped(**kwargs)\n",
    "        return v1_completions_request(self.client, **kwargs)\n",
    "\n",
    "\n",
    "def v1_chat_request(client, **kwargs):\n",
    "    @functools.lru_cache(maxsize=None if cache_turn_on else 0)\n",
    "    @NotebookCacheMemory.cache\n",
    "    def v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs):\n",
    "        @CacheMemory.cache\n",
    "        def v1_cached_gpt3_turbo_request_v2(**kwargs):\n",
    "            if 'stringify_request' in kwargs:\n",
    "                kwargs = json.loads(kwargs['stringify_request'])\n",
    "            return client.chat.completions.create(**kwargs)\n",
    "\n",
    "        return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
    "\n",
    "    response = v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs)\n",
    "\n",
    "    try:\n",
    "        response = response.model_dump()\n",
    "    except Exception:\n",
    "        response = response.raw\n",
    "        response['choices'] = [json.loads(x.json()) for x in response['choices']]\n",
    "        response['usage'] = json.loads(response['usage'].json())\n",
    "    return response\n",
    "\n",
    "\n",
    "def v1_completions_request(client, **kwargs):\n",
    "    @functools.lru_cache(maxsize=None if cache_turn_on else 0)\n",
    "    @NotebookCacheMemory.cache\n",
    "    def v1_cached_gpt3_request_v2_wrapped(**kwargs):\n",
    "        @CacheMemory.cache\n",
    "        def v1_cached_gpt3_request_v2(**kwargs):\n",
    "            return client.completions.create(**kwargs)\n",
    "\n",
    "        return v1_cached_gpt3_request_v2(**kwargs)\n",
    "\n",
    "    return v1_cached_gpt3_request_v2_wrapped(**kwargs).model_dump()\n",
    "\n",
    "\n",
    "## ======== test =========\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     print('Testing DspyLlamaIndexWrapper')\n",
    "#     import os\n",
    "\n",
    "#     import dspy\n",
    "#     from dspy.datasets.gsm8k import GSM8K, gsm8k_metric\n",
    "#     from llama_index.llms.openai import OpenAI\n",
    "\n",
    "#     llm = OpenAI(api_key=os.environ['OPENAI_API_KEY'], model='gpt-3.5-turbo')\n",
    "#     dspy_llm = DspyLlamaIndexWrapper(llm)\n",
    "\n",
    "#     # Load math questions from the GSM8K dataset.\n",
    "#     gsm8k = GSM8K()\n",
    "#     gsm8k_trainset, gsm8k_devset = gsm8k.train[:10], gsm8k.dev[:10]\n",
    "\n",
    "#     class CoT(dspy.Module):\n",
    "#         def __init__(self):\n",
    "#             super().__init__()\n",
    "#             self.prog = dspy.ChainOfThought('question -> answer')\n",
    "\n",
    "#         def forward(self, question):\n",
    "#             response = self.prog(question=question)\n",
    "#             return response\n",
    "\n",
    "#     ##\n",
    "\n",
    "#     dspy.settings.configure(lm=dspy_llm)\n",
    "\n",
    "#     from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "#     # Set up the optimizer: we want to \"bootstrap\" (i.e., self-generate) 4-shot examples of our CoT program.\n",
    "#     config = dict(max_bootstrapped_demos=4, max_labeled_demos=4)\n",
    "\n",
    "#     # Optimize! Use the `gsm8k_metric` here. In general, the metric is going to tell the optimizer how well it's doing.\n",
    "#     teleprompter = BootstrapFewShot(metric=gsm8k_metric, **config)\n",
    "#     optimized_cot = teleprompter.compile(CoT(), trainset=gsm8k_trainset)\n",
    "#     print(f'{optimized_cot=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this code implements a wrapper around the llama_index library to emulate a dspy llm\n",
    "\n",
    "this allows the llama_index library to be used in the dspy framework since dspy has limited support for LLMs\n",
    "\n",
    "This code is a slightly modified copy of dspy/dsp/modules/azure_openai.py\n",
    "\n",
    "The way this works is simply by creating a dummy openai client that wraps around any llama_index LLM object and implements .complete and .chat\n",
    "\n",
    "tested with python 3.12\n",
    "\n",
    "dspy==0.1.4\n",
    "dspy-ai==2.4.9\n",
    "llama-index==0.10.35\n",
    "llama-index-llms-openai==0.1.18\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from typing import Any, Literal\n",
    "\n",
    "from easydict import EasyDict\n",
    "from llama_index.core.base.llms.types import ChatMessage\n",
    "from llama_index.core.llms import LLM\n",
    "\n",
    "\n",
    "def LlamaIndexOpenAIClientWrapper(llm: LLM):\n",
    "    def chat(messages: list[ChatMessage], **kwargs) -> Any:\n",
    "        return llm.chat([ChatMessage(**message) for message in messages], **kwargs)\n",
    "\n",
    "    def complete(prompt: str, **kwargs) -> Any:\n",
    "        return llm.complete(prompt, **kwargs)\n",
    "\n",
    "    client = EasyDict(\n",
    "        {\n",
    "            'chat': EasyDict({'completions': EasyDict({'create': chat})}),\n",
    "            'completion': EasyDict({'create': complete}),\n",
    "            'ChatCompletion': EasyDict({'create': chat}),\n",
    "            'Completion': EasyDict({'create': complete}),\n",
    "        }\n",
    "    )\n",
    "    return client\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(message)s',\n",
    "    handlers=[logging.FileHandler('azure_openai_usage.log')],\n",
    ")\n",
    "\n",
    "import functools\n",
    "import json\n",
    "from typing import Any, Literal\n",
    "\n",
    "import backoff\n",
    "import dsp\n",
    "import openai\n",
    "from dsp.modules.cache_utils import CacheMemory, NotebookCacheMemory, cache_turn_on\n",
    "from dsp.modules.lm import LM\n",
    "\n",
    "try:\n",
    "    OPENAI_LEGACY = int(openai.version.__version__[0]) == 0\n",
    "except Exception:\n",
    "    OPENAI_LEGACY = True\n",
    "\n",
    "try:\n",
    "    import openai.error\n",
    "    from openai.openai_object import OpenAIObject\n",
    "\n",
    "    ERRORS = (\n",
    "        openai.error.RateLimitError,\n",
    "        openai.error.ServiceUnavailableError,\n",
    "        openai.error.APIError,\n",
    "    )\n",
    "except Exception:\n",
    "    ERRORS = (openai.RateLimitError, openai.APIError)\n",
    "    OpenAIObject = dict\n",
    "\n",
    "\n",
    "def backoff_hdlr(details):\n",
    "    \"\"\"Handler from https://pypi.org/project/backoff/\"\"\"\n",
    "    print(\n",
    "        'Backing off {wait:0.1f} seconds after {tries} tries ' 'calling function {target} with kwargs ' '{kwargs}'.format(**details),\n",
    "    )\n",
    "\n",
    "\n",
    "class DspyLlamaIndexWrapper(LM):\n",
    "    \"\"\"Wrapper around Azure's API for OpenAI.\n",
    "\n",
    "    Args:\n",
    "        api_base (str): Azure URL endpoint for model calling, often called 'azure_endpoint'.\n",
    "        api_version (str): Version identifier for API.\n",
    "        model (str, optional): OpenAI or Azure supported LLM model to use. Defaults to \"text-davinci-002\".\n",
    "        api_key (Optional[str], optional): API provider Authentication token. use Defaults to None.\n",
    "        model_type (Literal[\"chat\", \"text\"], optional): The type of model that was specified. Mainly to decide the optimal prompting strategy. Defaults to \"chat\".\n",
    "        **kwargs: Additional arguments to pass to the API provider.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: LLM,\n",
    "        model_type: Literal['chat', 'text'] = 'chat',\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(llm.model)\n",
    "        self.provider = 'openai'\n",
    "\n",
    "        self.llm = llm\n",
    "        self.client = LlamaIndexOpenAIClientWrapper(llm)\n",
    "        model = llm.model\n",
    "        self.model_type = model_type\n",
    "\n",
    "        # if not OPENAI_LEGACY and \"model\" not in kwargs:\n",
    "        #     if \"deployment_id\" in kwargs:\n",
    "        #         kwargs[\"model\"] = kwargs[\"deployment_id\"]\n",
    "        #         del kwargs[\"deployment_id\"]\n",
    "\n",
    "        #     if \"api_version\" in kwargs:\n",
    "        #         del kwargs[\"api_version\"]\n",
    "\n",
    "        if 'model' not in kwargs:\n",
    "            kwargs['model'] = model\n",
    "\n",
    "        self.kwargs = {\n",
    "            'temperature': 0.0,\n",
    "            'max_tokens': 150,\n",
    "            'top_p': 1,\n",
    "            'frequency_penalty': 0,\n",
    "            'presence_penalty': 0,\n",
    "            'n': 1,\n",
    "            **kwargs,\n",
    "        }  # TODO: add kwargs above for </s>\n",
    "\n",
    "        self.history: list[dict[str, Any]] = []\n",
    "\n",
    "    def _openai_client(self):\n",
    "        # if OPENAI_LEGACY:\n",
    "        #     return openai\n",
    "\n",
    "        return self.client\n",
    "\n",
    "    def log_usage(self, response):\n",
    "        \"\"\"Log the total tokens from the Azure OpenAI API response.\"\"\"\n",
    "        usage_data = response.get('usage')\n",
    "        if usage_data:\n",
    "            total_tokens = usage_data.get('total_tokens')\n",
    "            logging.info(f'{total_tokens}')\n",
    "\n",
    "    def basic_request(self, prompt: str, **kwargs):\n",
    "        raw_kwargs = kwargs\n",
    "\n",
    "        kwargs = {**self.kwargs, **kwargs}\n",
    "        if self.model_type == 'chat':\n",
    "            # caching mechanism requires hashable kwargs\n",
    "            kwargs['messages'] = [{'role': 'user', 'content': prompt}]\n",
    "            kwargs = {'stringify_request': json.dumps(kwargs)}\n",
    "            # response = chat_request(self.client, **kwargs)\n",
    "            # if OPENAI_LEGACY:\n",
    "            #     return _cached_gpt3_turbo_request_v2_wrapped(**kwargs)\n",
    "            # else:\n",
    "            return v1_chat_request(self.client, **kwargs)\n",
    "\n",
    "        else:\n",
    "            kwargs['prompt'] = prompt\n",
    "            response = self.completions_request(**kwargs)\n",
    "\n",
    "        history = {\n",
    "            'prompt': prompt,\n",
    "            'response': response,\n",
    "            'kwargs': kwargs,\n",
    "            'raw_kwargs': raw_kwargs,\n",
    "        }\n",
    "        self.history.append(history)\n",
    "\n",
    "        return response\n",
    "\n",
    "    @backoff.on_exception(\n",
    "        backoff.expo,\n",
    "        ERRORS,\n",
    "        max_time=1000,\n",
    "        on_backoff=backoff_hdlr,\n",
    "    )\n",
    "    def request(self, prompt: str, **kwargs):\n",
    "        \"\"\"Handles retrieval of GPT-3 completions whilst handling rate limiting and caching.\"\"\"\n",
    "        if 'model_type' in kwargs:\n",
    "            del kwargs['model_type']\n",
    "\n",
    "        return self.basic_request(prompt, **kwargs)\n",
    "\n",
    "    def _get_choice_text(self, choice: dict[str, Any]) -> str:\n",
    "        if self.model_type == 'chat':\n",
    "            return choice['message']['content']\n",
    "        return choice['text']\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        only_completed: bool = True,\n",
    "        return_sorted: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> list[dict[str, Any]]:\n",
    "        \"\"\"Retrieves completions from OpenAI Model.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): prompt to send to GPT-3\n",
    "            only_completed (bool, optional): return only completed responses and ignores completion due to length. Defaults to True.\n",
    "            return_sorted (bool, optional): sort the completion choices using the returned probabilities. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            list[dict[str, Any]]: list of completion choices\n",
    "        \"\"\"\n",
    "\n",
    "        assert only_completed, 'for now'\n",
    "        assert return_sorted is False, 'for now'\n",
    "\n",
    "        response = self.request(prompt, **kwargs)\n",
    "\n",
    "        try:\n",
    "            if dsp.settings.log_openai_usage:\n",
    "                self.log_usage(response)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        choices = response['choices']\n",
    "\n",
    "        completed_choices = [c for c in choices if c['finish_reason'] != 'length']\n",
    "\n",
    "        if only_completed and len(completed_choices):\n",
    "            choices = completed_choices\n",
    "\n",
    "        completions = [self._get_choice_text(c) for c in choices]\n",
    "        if return_sorted and kwargs.get('n', 1) > 1:\n",
    "            scored_completions = []\n",
    "\n",
    "            for c in choices:\n",
    "                tokens, logprobs = (\n",
    "                    c['logprobs']['tokens'],\n",
    "                    c['logprobs']['token_logprobs'],\n",
    "                )\n",
    "\n",
    "                if '<|endoftext|>' in tokens:\n",
    "                    index = tokens.index('<|endoftext|>') + 1\n",
    "                    tokens, logprobs = tokens[:index], logprobs[:index]\n",
    "\n",
    "                avglog = sum(logprobs) / len(logprobs)\n",
    "                scored_completions.append((avglog, self._get_choice_text(c)))\n",
    "\n",
    "            scored_completions = sorted(scored_completions, reverse=True)\n",
    "            completions = [c for _, c in scored_completions]\n",
    "\n",
    "        return completions\n",
    "\n",
    "    def completions_request(self, **kwargs):\n",
    "        # if OPENAI_LEGACY:\n",
    "        #     return cached_gpt3_request_v2_wrapped(**kwargs)\n",
    "        return v1_completions_request(self.client, **kwargs)\n",
    "\n",
    "\n",
    "def v1_chat_request(client, **kwargs):\n",
    "    @functools.lru_cache(maxsize=None if cache_turn_on else 0)\n",
    "    @NotebookCacheMemory.cache\n",
    "    def v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs):\n",
    "        @CacheMemory.cache\n",
    "        def v1_cached_gpt3_turbo_request_v2(**kwargs):\n",
    "            if 'stringify_request' in kwargs:\n",
    "                kwargs = json.loads(kwargs['stringify_request'])\n",
    "            return client.chat.completions.create(**kwargs)\n",
    "\n",
    "        return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
    "\n",
    "    response = v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs)\n",
    "\n",
    "    try:\n",
    "        response = response.model_dump()\n",
    "    except Exception:\n",
    "        response = response.raw\n",
    "        response['choices'] = [json.loads(x.json()) for x in response['choices']]\n",
    "        response['usage'] = json.loads(response['usage'].json())\n",
    "    return response\n",
    "\n",
    "\n",
    "def v1_completions_request(client, **kwargs):\n",
    "    @functools.lru_cache(maxsize=None if cache_turn_on else 0)\n",
    "    @NotebookCacheMemory.cache\n",
    "    def v1_cached_gpt3_request_v2_wrapped(**kwargs):\n",
    "        @CacheMemory.cache\n",
    "        def v1_cached_gpt3_request_v2(**kwargs):\n",
    "            return client.completions.create(**kwargs)\n",
    "\n",
    "        return v1_cached_gpt3_request_v2(**kwargs)\n",
    "\n",
    "    return v1_cached_gpt3_request_v2_wrapped(**kwargs).model_dump()\n",
    "\n",
    "\n",
    "## ======== test =========\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     print('Testing DspyLlamaIndexWrapper')\n",
    "#     import os\n",
    "\n",
    "#     import dspy\n",
    "#     from dspy.datasets.gsm8k import GSM8K, gsm8k_metric\n",
    "#     from llama_index.llms.openai import OpenAI\n",
    "\n",
    "#     llm = OpenAI(api_key=os.environ['OPENAI_API_KEY'], model='gpt-3.5-turbo')\n",
    "#     dspy_llm = DspyLlamaIndexWrapper(llm)\n",
    "\n",
    "#     # Load math questions from the GSM8K dataset.\n",
    "#     gsm8k = GSM8K()\n",
    "#     gsm8k_trainset, gsm8k_devset = gsm8k.train[:10], gsm8k.dev[:10]\n",
    "\n",
    "#     class CoT(dspy.Module):\n",
    "#         def __init__(self):\n",
    "#             super().__init__()\n",
    "#             self.prog = dspy.ChainOfThought('question -> answer')\n",
    "\n",
    "#         def forward(self, question):\n",
    "#             response = self.prog(question=question)\n",
    "#             return response\n",
    "\n",
    "#     ##\n",
    "\n",
    "#     dspy.settings.configure(lm=dspy_llm)\n",
    "\n",
    "#     from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "#     # Set up the optimizer: we want to \"bootstrap\" (i.e., self-generate) 4-shot examples of our CoT program.\n",
    "#     config = dict(max_bootstrapped_demos=4, max_labeled_demos=4)\n",
    "\n",
    "#     # Optimize! Use the `gsm8k_metric` here. In general, the metric is going to tell the optimizer how well it's doing.\n",
    "#     teleprompter = BootstrapFewShot(metric=gsm8k_metric, **config)\n",
    "#     optimized_cot = teleprompter.compile(CoT(), trainset=gsm8k_trainset)\n",
    "#     print(f'{optimized_cot=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
