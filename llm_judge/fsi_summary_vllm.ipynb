{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vllm serve \"Qwen/QwQ-32B-AWQ\" \\\n",
    "# vllm serve \"Valdemardi/DeepSeek-R1-Distill-Qwen-32B-AWQ\" \\\n",
    "# --enforce-eager \\\n",
    "# vllm serve /workspace/model_merged_daretie_quant/ \\\n",
    "# vllm serve \"Qwen/QwQ-32B-AWQ\" \\\n",
    "# --quantization awq_marlin \\\n",
    "vllm serve \"Valdemardi/DeepSeek-R1-Distill-Qwen-32B-AWQ\" \\\n",
    "--max_model_len 28000 \\\n",
    "--gpu-memory-utilization 0.8 \\\n",
    "--dtype float16 \\\n",
    "--port 8008 \\\n",
    "--host 0.0.0.0\n",
    "\n",
    "# vllm serve BAAI/bge-m3 --trust-remote-code --task embed --port 8008 --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip3 install --upgrade transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from federal_register.client import FederalRegister\n",
    "import requests\n",
    "\n",
    "# CIP 2024-10738\n",
    "# 2023-16377, 2019-12164, 2020-28868\n",
    "# Initialize the client.\n",
    "federal_register_client = FederalRegister()\n",
    "\n",
    "# Grab a specific document.\n",
    "# document_id = '2023-16377'\n",
    "document_id = '2024-10738'\n",
    "federal_document = federal_register_client.document_by_id(\n",
    "    document_id=document_id,\n",
    "    fields='all'\n",
    ")\n",
    "\n",
    "print(federal_document['full_text_xml_url'])\n",
    "response = requests.get(federal_document['full_text_xml_url'])\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Rule XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_rule import parse_xml_title\n",
    "\n",
    "rule_title = parse_xml_title(response.content[:5000].decode(\"utf-8\"))\n",
    "print(rule_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_rule import parse_and_clean_xml, clean_xml_text, get_page_numbers\n",
    "from llama_index.core import Document\n",
    "\n",
    "titles, sections = parse_and_clean_xml(response.content, first_section_title='Rule Introduction')\n",
    "assert len(titles) == len(sections)\n",
    "for title,section in zip(titles,sections):\n",
    "    first, last = get_page_numbers(section)\n",
    "    # print(f\"{title}: first page = {first}, last page = {last}, {len(section.split())*1.25} tokens\")\n",
    "    print(f\"{title}: first page = {first}, last page = {last}, tokens = {len(section.split())*1.25}\")\n",
    "print('All Titles\\n',titles)\n",
    "\n",
    "# Clean the text\n",
    "cleaned_rule_proposal = {title:clean_xml_text(section, remove_footnotes=True, remove_page_references=True, remove_xml_tags=True, remove_extra_whitespace=True) for title,section in zip(titles,sections)}\n",
    "\n",
    "# Get request for comments\n",
    "request_for_comments = None #cleaned_rule_proposal['III. Request for Comments']\n",
    "\n",
    "# Get only the sections for summarization. \n",
    "# omit_sections = ['Rule Introduction', 'Table of Contents', 'Text of Proposed Rules and Form Amendments']\n",
    "# omit_sections = ['Rule Introduction', 'Table of Contents', 'II. Discussion', 'III. Economic Analysis', 'IV. Paperwork Reduction Act', 'V. Initial Regulatory Flexibility Analysis', 'VI. Consideration of Impact on the Economy', 'Text of Proposed Rules and Form Amendments']\n",
    "omit_sections = ['III. Request for Comments', \"IX. FinCEN's Unfunded Mandates Reform Act Determination\", 'Authority and Issuance']\n",
    "relevant_sections_dict = {title:section for title,section in cleaned_rule_proposal.items() if title not in omit_sections}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, re, copy\n",
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    ")\n",
    "\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.openai import OpenAI as LIOpenAI\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/fsi/.env')\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class SummarizationEvent(Event):\n",
    "    result: dict\n",
    "\n",
    "class RefineSummariesEvent(Event):\n",
    "    result: dict\n",
    "\n",
    "class KeyPointsEvent(Event):\n",
    "    result: dict\n",
    "\n",
    "class KeyPointsGPEvent(Event):\n",
    "    result: dict\n",
    "\n",
    "class FactCheckingEvent(Event):\n",
    "    result: dict\n",
    "\n",
    "class FactUpdateEvent(Event):\n",
    "    result: dict\n",
    "\n",
    "class FormatSummaryOutputEvent(Event):\n",
    "    result: dict\n",
    "\n",
    "class ObjectiveVoiceEvent(Event):\n",
    "    result: dict\n",
    "\n",
    "\n",
    "from llama_index.core.prompts.base import BasePromptTemplate, PromptTemplate\n",
    "from summary_prompts import (\n",
    "                             LONG_SUMMARIZE_PROMPT_TMPL, \n",
    "                             SHORT_SUMMARIZE_PROMPT_TMPL, \n",
    "                             REFINE_SUMMARIES_PROMPT_TMPL,\n",
    "                             BULLET_POINTS_PROMPT_TMPL, \n",
    "                             GUIDING_PRINCIPLES_BULLET_POINTS_PROMPT_TMPL,\n",
    "                             BULLET_POINT_FACT_CHECKING_PROMPT_TMPL,\n",
    "                             BULLET_POINT_UPDATE_PROMPT_TMPL,\n",
    "                             FORMAT_SUMMARY_OUTPUT_PROMPT_TMPL\n",
    "                             )\n",
    "\n",
    "#TODO: use prompts not PromptTemplate\n",
    "long_summarize_prompt_template = PromptTemplate(LONG_SUMMARIZE_PROMPT_TMPL)\n",
    "short_summarize_prompt_template = PromptTemplate(SHORT_SUMMARIZE_PROMPT_TMPL)\n",
    "refine_summaries_prompt_template = PromptTemplate(REFINE_SUMMARIES_PROMPT_TMPL)\n",
    "bullet_points_prompt_template = PromptTemplate(BULLET_POINTS_PROMPT_TMPL)\n",
    "guiding_principles_bullet_points_prompt_template = PromptTemplate(GUIDING_PRINCIPLES_BULLET_POINTS_PROMPT_TMPL)\n",
    "bullet_point_fact_checking_prompt_template = PromptTemplate(BULLET_POINT_FACT_CHECKING_PROMPT_TMPL)\n",
    "bullet_point_update_prompt_template = PromptTemplate(BULLET_POINT_UPDATE_PROMPT_TMPL)\n",
    "format_summary_output_prompt_template = PromptTemplate(FORMAT_SUMMARY_OUTPUT_PROMPT_TMPL)\n",
    "\n",
    "def deepseek_llm_postprocesser(llm_response):\n",
    "    return llm_response.split('</think>')[-1].strip()\n",
    "\n",
    "def deepseek_summary_postprocesser(llm_response):\n",
    "    return re.sub(r'<think>.*?</think>', '', llm_response, flags=re.DOTALL).strip()\n",
    "\n",
    "def llm_postprocesser(llm_response):\n",
    "    return llm_response.strip()\n",
    "\n",
    "def summary_postprocesser(llm_response):\n",
    "    return llm_response.strip()\n",
    "\n",
    "\n",
    "def deepseek_llm_postprocesser(llm_response):\n",
    "    return llm_response.split('</think>')[-1].strip()\n",
    "\n",
    "def deepseek_summary_postprocesser(llm_response):\n",
    "    return re.sub(r'<think>.*?</think>', '', llm_response, flags=re.DOTALL).strip()\n",
    "\n",
    "def llm_postprocesser(llm_response):\n",
    "    return llm_response.strip()\n",
    "\n",
    "def summary_postprocesser(llm_response):\n",
    "    return llm_response.strip()\n",
    "\n",
    "\n",
    "class RuleSummarizationFlow(Workflow):\n",
    "\n",
    "    async def _get_nodes_for_section(self, ctx: Context, section_title: str):\n",
    "        return [x for x in await ctx.get(\"nodes\") if x.metadata['section_title'] == section_title]\n",
    "\n",
    "    async def _generate_llm_response(self, llm, template, **kwargs):\n",
    "        formatted_prompt = template.format(**kwargs)\n",
    "        response = await llm.acomplete(formatted_prompt)\n",
    "        return response.text\n",
    "\n",
    "    async def _process_and_postprocess(self, ctx: Context, llm, template, postprocessor, **kwargs):\n",
    "        response_text = await self._generate_llm_response(llm, template, **kwargs)\n",
    "        return postprocessor(response_text)\n",
    "\n",
    "    async def _summarize_section(self, ctx: Context, section):\n",
    "        print(f\"  Summarizing section: {section.metadata['section_title']}...\")\n",
    "        nodes = await self._get_nodes_for_section(ctx, section.metadata['section_title'])\n",
    "        summarizer = await ctx.get(\"tree_summarizer\")\n",
    "        summary_prompt = await ctx.get('summary_prompt')\n",
    "        compliance_guidance = await ctx.get(\"compliance_guidance\")\n",
    "        additional_guidance = await ctx.get(\"additional_guidance\")\n",
    "        postprocessor = await ctx.get(\"local_summary_postprocesser\")\n",
    "\n",
    "        template = summary_prompt.get_template()\n",
    "\n",
    "        response = await summarizer.aget_response(\n",
    "            template.format(\n",
    "                compliance_guidance=compliance_guidance,\n",
    "                additional_guidance=additional_guidance\n",
    "            ),\n",
    "            [doc.text for doc in nodes]\n",
    "        )\n",
    "        clean_response = postprocessor(response)\n",
    "        return section.metadata['section_title'], clean_response\n",
    "    \n",
    "    async def _refine_summary(self, ctx: Context, title: str, summary: str):\n",
    "        print(f\"  Refining summary section: {title}...\")\n",
    "        local_llm = await ctx.get(\"local_llm\")\n",
    "        postprocesser = await ctx.get(\"local_llm_postprocesser\")\n",
    "        template = refine_summaries_prompt_template.get_template()\n",
    "        refined_summary = await self._process_and_postprocess(ctx, local_llm, template, postprocesser, summaries=summary)\n",
    "        print(f\"  {title} Number of words: {len(refined_summary.split())}\")\n",
    "        return title, refined_summary\n",
    "\n",
    "    async def _extract_bullet_points(self, ctx: Context, title: str, summary: str, prompt_template, **kwargs):\n",
    "        print(f\"  Extracting key bullet points for section: {title}...\")\n",
    "        bullet_points_llm = await ctx.get(\"section_bullet_points_llm\")\n",
    "        compliance_guidance = await ctx.get(\"compliance_guidance\")\n",
    "        additional_guidance = await ctx.get(\"additional_guidance\")\n",
    "        postprocessor = await ctx.get(\"local_llm_postprocesser\")\n",
    "        \n",
    "        bullet_points = await self._process_and_postprocess(\n",
    "            ctx, \n",
    "            bullet_points_llm, \n",
    "            prompt_template.get_template(), \n",
    "            postprocessor,\n",
    "            compliance_guidance=compliance_guidance, \n",
    "            additional_guidance=additional_guidance, \n",
    "            section_str=summary,\n",
    "            **kwargs\n",
    "        )\n",
    "        return title, bullet_points\n",
    "\n",
    "    async def _check_facts(self, ctx: Context, title: str, summary: str, bullet_points: str):\n",
    "        print(f\"  Fact checking bullets: {title}...\")\n",
    "        fact_checker_llm = await ctx.get(\"fact_checker_llm\")\n",
    "        postprocessor = await ctx.get(\"expert_llm_postprocesser\")\n",
    "        template = bullet_point_fact_checking_prompt_template.get_template()\n",
    "        result = await self._process_and_postprocess(ctx, fact_checker_llm, template, postprocessor, section_str=summary, statement=bullet_points)\n",
    "        return title, result\n",
    "\n",
    "    async def _update_facts(self, ctx: Context, title: str, incorrect_bullet_points: str, corrected_bullet_points: str):\n",
    "         print(f\"  Fact update section: {title}...\")\n",
    "         fact_update_llm = await ctx.get(\"fact_update_llm\")\n",
    "         postprocessor = await ctx.get(\"expert_llm_postprocesser\")\n",
    "         template = bullet_point_update_prompt_template.get_template()\n",
    "         updated_bullets = await self._process_and_postprocess(ctx, fact_update_llm, template, postprocessor, incorrect_bullet_points=incorrect_bullet_points, corrected_bullet_points=corrected_bullet_points)\n",
    "         return title, updated_bullets\n",
    "\n",
    "    async def _format_summary(self, ctx: Context, title: str, summary: str):\n",
    "        print(f\"  Formatting summary output: {title}...\")\n",
    "        local_llm = await ctx.get(\"local_llm\")\n",
    "        postprocessor = await ctx.get(\"local_llm_postprocesser\")\n",
    "        template = format_summary_output_prompt_template.get_template()\n",
    "        formatted_summary = await self._process_and_postprocess(ctx, local_llm, template, postprocessor, summary_output=summary)\n",
    "        formatted_summary = formatted_summary.replace('**', '')\n",
    "        print(f\"  {title} Number of words: {len(formatted_summary.split())}\")\n",
    "        return title, formatted_summary\n",
    "\n",
    "    async def _configure_long_summary(self, local_model_name, ctx):\n",
    "        summary_style = 'compact_accumulate'\n",
    "        summary_prompt = long_summarize_prompt_template\n",
    "        summary_ctx_len = 7000\n",
    "        buffer_size = 5\n",
    "        breakpoint_percentile_threshold = 75\n",
    "        if local_model_name in ['deepseek-r1:32b', 'qwq']:\n",
    "            await ctx.set(\"local_summary_postprocesser\", deepseek_summary_postprocesser)\n",
    "        else:\n",
    "            await ctx.set(\"local_summary_postprocesser\", summary_postprocesser)\n",
    "        return summary_style, summary_prompt, summary_ctx_len, buffer_size, breakpoint_percentile_threshold\n",
    "\n",
    "    async def _configure_short_summary(self, local_model_name, ctx):\n",
    "        summary_style = 'tree_summarize'\n",
    "        summary_prompt = short_summarize_prompt_template\n",
    "        summary_ctx_len = 10000\n",
    "        buffer_size = 1\n",
    "        breakpoint_percentile_threshold = 95\n",
    "        await ctx.set('summary_response_split', None)\n",
    "        if local_model_name in ['deepseek-r1:32b', 'qwq']:\n",
    "            await ctx.set(\"local_summary_postprocesser\", deepseek_llm_postprocesser)\n",
    "        else:\n",
    "            await ctx.set(\"local_summary_postprocesser\", llm_postprocesser)\n",
    "        return summary_style, summary_prompt, summary_ctx_len, buffer_size, breakpoint_percentile_threshold\n",
    "\n",
    "\n",
    "    @step\n",
    "    async def initialize(self, ctx: Context, ev: StartEvent) -> SummarizationEvent:\n",
    "        # see https://github.com/run-llama/llama_index/blob/f7c5ee5efbb6172e819f26d1705fcdf6114b11a3/llama-index-core/llama_index/core/response_synthesizers/type.py#L4\n",
    "        # Summarization methods: \"accumulate\", \"compact_accumulate\", \"compact\", \"simple_summarize\", \"tree_summarize\", \"refine\"\n",
    "        # ollama start (defaults to \"http://127.0.0.1:11434\")\n",
    "        # OLLAMA_HOST=\"http://127.0.0.1:11435\" ollama start\n",
    "        embed_model_name = \"nomic-embed-text\"\n",
    "        # embed_model_name = \"bge-m3\"\n",
    "        local_model_name, local_ctx_len = \"deepseek-r1:32b\", 20000\n",
    "        # local_model_name, local_ctx_len = \"qwq\", 32000\n",
    "        # local_model_name, local_ctx_len = \"gpt-4o\", 16384\n",
    "        # expert_model_name, expert_ctx_len = \"gpt-4o\", 16384\n",
    "        \n",
    "        assert ev.summary_length in ['long', 'short'], \"Invalid summary length. Must be 'long' or 'short'.\"\n",
    "        # Models\n",
    "        \n",
    "        if ev.summary_length == 'long':\n",
    "            summary_style, summary_prompt, summary_ctx_len, buffer_size, breakpoint_percentile_threshold = await self._configure_long_summary(local_model_name, ctx)\n",
    "            if local_model_name in ['qwq', 'deepseek-r1:32b']: await ctx.set(\"local_summary_postprocesser\", deepseek_summary_postprocesser)\n",
    "            else: await ctx.set(\"local_summary_postprocesser\", summary_postprocesser)\n",
    "        else:\n",
    "            summary_style, summary_prompt, summary_ctx_len, buffer_size, breakpoint_percentile_threshold = await self._configure_short_summary(local_model_name, ctx)\n",
    "            if local_model_name in ['qwq', 'deepseek-r1:32b']: await ctx.set(\"local_summary_postprocesser\", deepseek_llm_postprocesser)\n",
    "            else: await ctx.set(\"local_summary_postprocesser\", llm_postprocesser)\n",
    "            \n",
    "        await ctx.set('summary_length', ev.summary_length)\n",
    "        await ctx.set('summary_prompt', summary_prompt)\n",
    "        \n",
    "        if local_model_name in ['qwq', 'deepseek-r1:32b']: \n",
    "            system_prompt = \"You are an AI assistant. Be helpful and informative. Provide accurate information. Be respectful and professional. Only answer in English.\"\n",
    "            await ctx.set(\"local_llm_postprocesser\", deepseek_llm_postprocesser)\n",
    "        else: \n",
    "            system_prompt = None\n",
    "            await ctx.set(\"local_llm_postprocesser\", llm_postprocesser)\n",
    "            await ctx.set(\"local_summary_postprocesser\", summary_postprocesser)\n",
    "        \n",
    "\n",
    "        # Embedding model\n",
    "        # OLLAMA_HOST=\"http://127.0.0.1:11435\" ollama start\n",
    "        await ctx.set('embed_model', OllamaEmbedding(embed_model_name, base_url=\"http://localhost:11435\"))\n",
    "        # TODO: add tokenizer to splitter\n",
    "        splitter = SemanticSplitterNodeParser(buffer_size=buffer_size, \n",
    "                                            embed_model=await ctx.get('embed_model'), \n",
    "                                            include_metadata=True, \n",
    "                                            breakpoint_percentile_threshold=breakpoint_percentile_threshold)\n",
    "        \n",
    "        # Build the documents (they will be split into chunks later)\n",
    "        documents_rule_proposal = [Document(text=section_text, metadata={'section_title':title}) for title, section_text in ev.rule_proposal_sections.items()]\n",
    "        nodes = splitter.get_nodes_from_documents(documents_rule_proposal, show_progress=True)\n",
    "        summary_ctx_len = max([len(node.text.split()) for node in nodes])*2.0\n",
    "        print(summary_ctx_len)\n",
    "        await ctx.set(\"documents\", documents_rule_proposal)\n",
    "        await ctx.set(\"nodes\", nodes)\n",
    "        \n",
    "        # Local LLM\n",
    "        # await ctx.set(\"local_llm\", LIOpenAI(model=local_model_name, max_tokens=local_ctx_len, api_key=openai_key, temperature=0.5))\n",
    "        # await ctx.set(\"summary_llm\", LIOpenAI(model=expert_model_name, max_tokens=summary_ctx_len, api_key=openai_key, temperature=0.5))\n",
    "\n",
    "        # additional_kwargs = {\"num_predict\": 10000,\n",
    "        #                      \"mirostat\":0}\n",
    "        # await ctx.set(\"local_llm\", Ollama(model=local_model_name, url=\"http://127.0.0.1:11434\", context_window=local_ctx_len, model_type=\"chat\", is_function_calling_model=True, \n",
    "        #                                   request_timeout=4000.0, additional_kwargs=additional_kwargs, keep_alive=0, system_prompt=system_prompt))\n",
    "        # await ctx.set(\"summary_llm\", Ollama(model=local_model_name, url=\"http://127.0.0.1:11434\", context_window=summary_ctx_len, model_type=\"chat\", is_function_calling_model=True, \n",
    "        #                                     request_timeout=4000.0, additional_kwargs=additional_kwargs, keep_alive=0, system_prompt=system_prompt))\n",
    "\n",
    "        vllm_model = \"/workspace/model_merged_daretie_quant/\"\n",
    "        # vllm_model = \"Valdemardi/DeepSeek-R1-Distill-Qwen-32B-AWQ\"\n",
    "        await ctx.set(\"local_llm\", OpenAILike(\n",
    "                                            model=vllm_model,\n",
    "                                            temperature=0.6,\n",
    "                                            system_prompt=system_prompt,\n",
    "                                            api_base=\"http://0.0.0.0:8008/v1\",\n",
    "                                            api_key=\"fake\",\n",
    "                                            is_chat_model=True,\n",
    "                                            is_function_calling_model=True,\n",
    "                                            context_window=local_ctx_len,\n",
    "                                            # max_tokens=10000,\n",
    "                                            timeout=4000.0\n",
    "                                        )\n",
    "                    )\n",
    "        await ctx.set(\"summary_llm\", OpenAILike(\n",
    "                                            model=vllm_model,\n",
    "                                            temperature=0.6,\n",
    "                                            system_prompt=system_prompt,\n",
    "                                            api_base=\"http://0.0.0.0:8008/v1\",\n",
    "                                            api_key=\"fake\",\n",
    "                                            is_chat_model=True,\n",
    "                                            is_function_calling_model=True,\n",
    "                                            context_window=summary_ctx_len,\n",
    "                                            # max_tokens=10000,\n",
    "                                            timeout=4000.0\n",
    "                                        )\n",
    "                    )\n",
    "        \n",
    "        # Expert LLM\n",
    "        # await ctx.set(\"expert_llm\", LIOpenAI(model=expert_model_name, max_tokens=expert_ctx_len, api_key=openai_key))\n",
    "        await ctx.set(\"expert_llm\", await ctx.get(\"local_llm\") )\n",
    "        await ctx.set(\"expert_llm_postprocesser\", await ctx.get(\"local_llm_postprocesser\"))\n",
    "        \n",
    "        await ctx.set(\"tree_summarizer\", get_response_synthesizer(llm=await ctx.get(\"summary_llm\"), \n",
    "                                                                  response_mode=summary_style))\n",
    "        await ctx.set(\"section_bullet_points_llm\", await ctx.get(\"local_llm\"))\n",
    "        await ctx.set(\"fact_checker_llm\", await ctx.get(\"expert_llm\"))\n",
    "        await ctx.set(\"fact_update_llm\", await ctx.get(\"expert_llm\"))\n",
    "        await ctx.set(\"section_guiding_principles_bullet_points_llm\", await ctx.get(\"local_llm\"))\n",
    "        await ctx.set(\"request_for_comments\", ev.request_for_comments)\n",
    "        await ctx.set(\"guiding_principles\", ev.guiding_principles)\n",
    "        await ctx.set(\"compliance_guidance\", ev.compliance_guidance)\n",
    "        await ctx.set(\"additional_guidance\", ev.additional_guidance)\n",
    "        #TODO: implement fact checking manager instead of status and num iterations\n",
    "        # await ctx.set(\"fact_checking_manager\", FactCheckingManager(ctx))\n",
    "        await ctx.set(\"fact_checking_status\", 1)\n",
    "        await ctx.set(\"fact_checking_num_iterations\", ev.fact_checking_num_iterations)\n",
    "\n",
    "        return SummarizationEvent(result={})\n",
    "    \n",
    "    @step\n",
    "    async def summarize(self, ctx: Context, ev: SummarizationEvent) -> KeyPointsEvent | RefineSummariesEvent:\n",
    "        documents = await ctx.get(\"documents\")\n",
    "        summary_length = await ctx.get('summary_length')\n",
    "        tasks = [self._summarize_section(ctx, section) for section in documents]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        summaries = dict(results)\n",
    "        await ctx.set(\"summaries\", summaries)\n",
    "        # ev.result['summaries'] = summaries\n",
    "        if summary_length == 'short':\n",
    "            return KeyPointsEvent(result=ev.result)\n",
    "        else:\n",
    "            return RefineSummariesEvent(result=ev.result)\n",
    "        \n",
    "    @step\n",
    "    async def refine_summaries(self, ctx: Context, ev: RefineSummariesEvent) -> KeyPointsEvent:\n",
    "        # Store raw summaries\n",
    "        summaries = await ctx.get(\"summaries\")\n",
    "        raw_summaries = copy.deepcopy(summaries)\n",
    "        await ctx.set(\"raw_summaries\", raw_summaries)\n",
    "        # ev.result['raw_summaries'] = copy.deepcopy(ev.result['summaries'])\n",
    "        tasks = [\n",
    "            self._refine_summary(ctx, title, summary)\n",
    "            for title, summary in raw_summaries.items()\n",
    "            # for title, summary in ev.result['raw_summaries'].items()\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        refined_summaries = dict(results)\n",
    "        await ctx.set(\"summaries\", refined_summaries)\n",
    "        # ev.result['summaries'] = refined_summaries\n",
    "        return KeyPointsEvent(result=ev.result)\n",
    "\n",
    "    @step\n",
    "    async def summary_bullet_points(self, ctx: Context, ev: KeyPointsEvent) -> FactCheckingEvent:\n",
    "        summaries = await ctx.get(\"summaries\")\n",
    "        tasks = [\n",
    "            self._extract_bullet_points(ctx, title, summary, bullet_points_prompt_template)\n",
    "            for title, summary in summaries.items()\n",
    "            # for title, summary in ev.result['summaries'].items()\n",
    "        ]\n",
    "\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        bullet_points = dict(results)        \n",
    "        await ctx.set(\"bullet_points\", bullet_points)\n",
    "        # ev.result['bullet_points'] = bullet_points\n",
    "        return FactCheckingEvent(result=ev.result)\n",
    "\n",
    "    @step\n",
    "    async def fact_checking(self, ctx: Context, ev: FactCheckingEvent) -> KeyPointsGPEvent | FactUpdateEvent:\n",
    "        # facts_updated = \"fact_updates\" in ev.result\n",
    "        facts_updated = await ctx.get(\"fact_updates\", False)\n",
    "        update_bullets = False\n",
    "        fact_checks = {}\n",
    "        fact_checking_status = await ctx.get(\"fact_checking_status\")\n",
    "        fact_checking_num_iterations = await ctx.get(\"fact_checking_num_iterations\")\n",
    "        summaries = await ctx.get(\"summaries\")\n",
    "\n",
    "        if fact_checking_status <= fact_checking_num_iterations:\n",
    "            tasks = []\n",
    "            fact_updates = await ctx.get(\"fact_updates\", {})\n",
    "            bullet_points = await ctx.get(\"bullet_points\")\n",
    "            for title, summary in summaries.items():\n",
    "            # for title, summary in ev.result['summaries'].items():\n",
    "                # Only check if not previously updated or if the previous update needed changes\n",
    "                # if not facts_updated or (title in ev.result.get('fact_updates', {}) and ev.result['fact_updates'][title] != 'None'):\n",
    "                if not facts_updated or (title in fact_updates and fact_updates[title] != 'None'):\n",
    "                    tasks.append(self._check_facts(ctx, title, summary, bullet_points[title]))\n",
    "                \n",
    "            if tasks:  # Only process if there are tasks\n",
    "                results = await asyncio.gather(*tasks)\n",
    "                fact_checks = dict(results)\n",
    "                \n",
    "                # Check if any updates are needed\n",
    "                update_bullets = any(\n",
    "                    check != 'None' or len(check) > 25 \n",
    "                    for check in fact_checks.values()\n",
    "                )\n",
    "                \n",
    "        if update_bullets:\n",
    "            fact_checking_status += 1\n",
    "            await ctx.set(\"fact_checking_status\", fact_checking_status)\n",
    "            \n",
    "        await ctx.set('fact_checks', fact_checks)\n",
    "        # ev.result['fact_checks'] = fact_checks\n",
    "        return FactUpdateEvent(result=ev.result) if update_bullets else KeyPointsGPEvent(result=ev.result)\n",
    "\n",
    "    @step\n",
    "    async def fact_update(self, ctx: Context, ev: FactUpdateEvent) -> FactCheckingEvent:\n",
    "        bullet_points = await ctx.get(\"bullet_points\")\n",
    "        fact_checks = await ctx.get(\"fact_checks\")\n",
    "        tasks = [\n",
    "            self._update_facts(ctx, title, bullet_updates, bullet_points[title])\n",
    "            # self._update_facts(ctx, title, bullet_updates, ev.result['bullet_points'][title])\n",
    "            # for title, bullet_updates in ev.result['fact_checks'].items()\n",
    "            for title, bullet_updates in fact_checks.items()\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        fact_updates = dict(results)\n",
    "        for title, updated_bullet_points in fact_updates.items():\n",
    "            bullet_points[title] = updated_bullet_points\n",
    "        await ctx.set('bullet_points', bullet_points)\n",
    "        await ctx.set('fact_updates', fact_updates)\n",
    "        # ev.result['fact_updates'] = fact_updates\n",
    "        return FactCheckingEvent(result=ev.result)\n",
    "\n",
    "    @step\n",
    "    async def guiding_principles_bullet_points(self, ctx: Context, ev: KeyPointsGPEvent) -> FormatSummaryOutputEvent:\n",
    "        guiding_principles = await ctx.get(\"guiding_principles\")\n",
    "        summaries = await ctx.get(\"summaries\")\n",
    "        tasks = [\n",
    "            self._extract_bullet_points(ctx, title, summary, guiding_principles_bullet_points_prompt_template, guiding_principles=guiding_principles)\n",
    "            for title, summary in summaries.items()\n",
    "            # for title, summary in ev.result['summaries'].items()\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        gp_bullet_points = dict(results)\n",
    "        # ev.result['guiding_principles_bullet_points'] = gp_bullet_points\n",
    "        await ctx.set('guiding_principles_bullet_points', gp_bullet_points)\n",
    "        return FormatSummaryOutputEvent(result=ev.result)\n",
    "    \n",
    "\n",
    "    @step\n",
    "    async def format_summary_output(self, ctx: Context, ev: FormatSummaryOutputEvent) -> StopEvent:\n",
    "        summaries = await ctx.get(\"summaries\")\n",
    "        tasks = [\n",
    "            self._format_summary(ctx, title, summary)\n",
    "            for title, summary in summaries.items()\n",
    "            # for title, summary in ev.result['summaries'].items()\n",
    "        ]\n",
    "\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        formatted_summaries = dict(results)        \n",
    "        await ctx.set('formatted_summaries', formatted_summaries)\n",
    "        # ev.result['formatted_summaries'] = formatted_summaries\n",
    "        return StopEvent(result=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "draw_all_possible_flows(RuleSummarizationFlow, filename=\"/workspace/data/rule_flows.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_relevant_section_dict = {k:v for k,v in relevant_sections_dict.items() if k in [\"IV. Analysis of the Costs and Benefits Associated With the Proposed Rule\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Runnining the workflow\n",
    "# Load the guiding principles\n",
    "with open('/workspace/repos/fsi/fsi_guiding_principles.txt', 'r') as f:\n",
    "    guiding_principles = f.read()\n",
    "\n",
    "c = RuleSummarizationFlow(timeout=12000, verbose=True)\n",
    "# Pass in the data\n",
    "result = await c.run(rule_proposal_sections=fake_relevant_section_dict, \n",
    "# result = await c.run(rule_proposal_sections=relevant_sections_dict, \n",
    "                     request_for_comments=request_for_comments, \n",
    "                     guiding_principles=guiding_principles,\n",
    "                     compliance_guidance=\"Do not include any information about requests for comments or instructions to commenters.\",\n",
    "                     additional_guidance='',\n",
    "                     fact_checking_num_iterations=3,\n",
    "                     summary_length='long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "formatted_summaries = await result.get('formatted_summaries')\n",
    "with open(f'/workspace/{rule_title}.json'.replace(' ','_'), 'w') as f:\n",
    "    json.dump(formatted_summaries, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bullets_dict={}\n",
    "for bullets in ['bullet_points', 'guiding_principles_bullet_points']:\n",
    "    result_bullets = await result.get(bullets)\n",
    "    bullets_dict[bullets] = {}\n",
    "    for section in result_bullets.keys():\n",
    "        bullet_collect=[]\n",
    "        for bullet in result_bullets[section].replace('\\n\\n','\\n').split('\\n'):\n",
    "            bullet_collect.append(bullet.replace('**', '').replace('-','').strip())\n",
    "        bullets_dict[bullets][section] = bullet_collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create main PDF with all sections summaries\n",
    "from utils import create_pdf\n",
    "formatted_summaries = await result.get('formatted_summaries')\n",
    "create_pdf(formatted_summaries,  bullets_dict, f'SUMMARY {rule_title}.pdf'.replace(' ','_'), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.core.prompts.base import PromptTemplate\n",
    "from summary_prompts import (\n",
    "                             LONG_SUMMARIZE_PROMPT_TMPL, \n",
    "                             SHORT_SUMMARIZE_PROMPT_TMPL, \n",
    "                             REFINE_SUMMARIES_PROMPT_TMPL,\n",
    "                             BULLET_POINTS_PROMPT_TMPL, \n",
    "                             GUIDING_PRINCIPLES_BULLET_POINTS_PROMPT_TMPL,\n",
    "                             BULLET_POINT_FACT_CHECKING_PROMPT_TMPL,\n",
    "                             BULLET_POINT_UPDATE_PROMPT_TMPL\n",
    "                             )\n",
    "\n",
    "\n",
    "long_summarize_prompt_template = PromptTemplate(LONG_SUMMARIZE_PROMPT_TMPL)\n",
    "short_summarize_prompt_template = PromptTemplate(SHORT_SUMMARIZE_PROMPT_TMPL)\n",
    "refine_summaries_prompt_template = PromptTemplate(REFINE_SUMMARIES_PROMPT_TMPL)\n",
    "bullet_points_prompt_template = PromptTemplate(BULLET_POINTS_PROMPT_TMPL)\n",
    "guiding_principles_bullet_points_prompt_template = PromptTemplate(GUIDING_PRINCIPLES_BULLET_POINTS_PROMPT_TMPL)\n",
    "bullet_point_fact_checking_prompt_template = PromptTemplate(BULLET_POINT_FACT_CHECKING_PROMPT_TMPL)\n",
    "bullet_point_update_prompt_template = PromptTemplate(BULLET_POINT_UPDATE_PROMPT_TMPL)\n",
    "embed_model_name=\"nomic-embed-text\"\n",
    "embed_model = OllamaEmbedding(embed_model_name, base_url=\"http://localhost:11435\")\n",
    "splitter = SemanticSplitterNodeParser(buffer_size=5, embed_model=embed_model, include_metadata=True, breakpoint_percentile_threshold=75)\n",
    "documents_rule_proposal = [Document(text=section_text, metadata={'section_title':title}) for title, section_text in relevant_sections_dict.items()]\n",
    "nodes = splitter.get_nodes_from_documents(documents_rule_proposal, show_progress=True)\n",
    "\n",
    "import os\n",
    "from llama_index.llms.openai import OpenAI as LIOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/fsi/.env')\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "expert_model_name, ctx_len = \"gpt-4o\", 3000\n",
    "\n",
    "llm = LIOpenAI(model=expert_model_name, max_tokens=30000, api_key=openai_key)\n",
    "        \n",
    "additional_kwargs = {\"num_predict\": 20000,\n",
    "                     \"mirostat\":0}\n",
    "local_model_name, ctx_len, tokenizer_name = \"deepseek-r1:32b\", 2048, \"\"\n",
    "llm = Ollama(model=local_model_name, url=\"http://127.0.0.1:11434\", context_window=ctx_len, model_type=\"chat\", is_function_calling_model=True, \n",
    "                            request_timeout=4000.0, additional_kwargs=additional_kwargs, keep_alive=0, system_prompt=None)\n",
    "\n",
    "tree_summarizer = get_response_synthesizer(llm=llm, \n",
    "                                            response_mode=\"compact_accumulate\", \n",
    "                                            ) \n",
    "test_nodes = [x for x in nodes if x.metadata['section_title'] == 'II. Discussion of Regulation Best Interest']\n",
    "len(test_nodes), len(\" \".join([x.text for x in test_nodes]).split())*1.25\n",
    "\n",
    "print(sorted([len(x.text.split())*1.25 for x in test_nodes]))\n",
    "# tree_response = tree_summarizer.get_response(long_summarize_prompt_template.get_template().format(compliance_guidance=\"None\", \n",
    "tree_response = tree_summarizer.get_response(long_summarize_prompt_template.get_template().format(compliance_guidance=\"Do not include any information about requests for comments or instructions to commenters.\", \n",
    "                                                                                                  additional_guidance=\"\"), \n",
    "                                             text_chunks=[doc.text for doc in test_nodes])\n",
    "# raw_summaries = \"\\n\\n\".join([x.split('</think>')[-1].strip() for x in tree_response.split(\"---------------------\\nResponse\")])\n",
    "# raw_summaries_list = [x.split('</think>')[-1].strip() for x in tree_response.split(\"---------------------\\nResponse\")]\n",
    "import re\n",
    "text = re.sub(r'<think>.*?</think>', '', tree_response, flags=re.DOTALL)\n",
    "print(text)\n",
    "refine_summaries_prompt = (\"You will be given text containing several independent reports. \"\n",
    "                           \"Each report begins with 'Response' and ends with '---------------------'. \"\n",
    "                           \"Your task is to rewrite all the responses (i.e. do not summarize) to form a coherent and accurate report. \"\n",
    "                           \"Remove any redundant information and redundant phrasing (e.g. 'The regulation specifies', 'In summary', 'The text outlines'). \"\n",
    "                           \"Preserve all of the details in the original report, while ensuring that the final rewrite has a clear narrative flow and logical structure. \"\n",
    "                           \"Write the report in paragraph form, using full sentences. \"\n",
    "                           \"Do not use any Markdown formatting. \"\n",
    "                           \"The final report should be at least 5000 words long.\\n\"\n",
    "                           \"\\n\"\n",
    "                           \"ORIGINAL REPORT:\\n\"\n",
    "                           \"{summaries}\\n\"\n",
    "                           \"\\n\"\n",
    "                           \"REFINED REPORT:\\n\"\n",
    "                        )\n",
    "additional_kwargs = {\"num_predict\": 10000,\n",
    "                     \"mirostat\":0}\n",
    "local_model_name, ctx_len, tokenizer_name = \"deepseek-r1:32b\", 20000, \"\"\n",
    "llm = Ollama(model=local_model_name, url=\"http://127.0.0.1:11434\", context_window=ctx_len, model_type=\"chat\", is_function_calling_model=True, \n",
    "                            request_timeout=4000.0, additional_kwargs=additional_kwargs, keep_alive=0, system_prompt=None)\n",
    "\n",
    "response = llm.acomplete(refine_summaries_prompt.format(summaries=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in bullets_dict[bullets][section]:\n",
    "    print(b.replace('**', '').replace('-','').strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "\n",
    "model_name, ctx_len, tokenizer_name = \"deepseek-r1:32b\", 40000, \"\"\n",
    "additional_kwargs = {\"num_predict\": 8000}\n",
    "system_prompt = None\n",
    "llm = Ollama(model=model_name, url=\"http://127.0.0.1:11434\", context_window=ctx_len, model_type=\"chat\", is_function_calling_model=True, \n",
    "             request_timeout=4000.0, additional_kwargs=additional_kwargs, keep_alive=0, system_prompt=system_prompt)\n",
    "# OLLAMA_HOST=\"http://127.0.0.1:11435\" ollama start\n",
    "embed_model = OllamaEmbedding('bge-m3', base_url=\"http://localhost:11435\", keep_alive=0)\n",
    "splitter = SemanticSplitterNodeParser(buffer_size=1, embed_model=embed_model, include_metadata=True)\n",
    "documents_rule_proposal = [Document(text=section_text, metadata={'section_title':title}) for title, section_text in relevant_sections_dict.items()]\n",
    "\n",
    "# nodes = splitter.get_nodes_from_documents([documents_rule_proposal[6]], show_progress=True)\n",
    "nodes = splitter.get_nodes_from_documents(documents_rule_proposal, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts.base import BasePromptTemplate, PromptTemplate\n",
    "from summary_prompts import TREE_PRAXIS_SUMMARIZE_PROMPT_TMPL, GLOBAL_PRAXIS_SUMMARIZE_PROMPT_TMPL, GLOBAL_PRAXIS_QA_PROMPT_TMPL, REQUEST_FOR_COMMENT_PRAXIS_SUMMARIZE_PROMPT_TMPL, DEF_EXTRACTION_PROMPT_TMPL\n",
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "compliance_guidance = \"None\"\n",
    "additional_guidance = \"None\"\n",
    "tree_summarize_prompt_template = PromptTemplate(TREE_PRAXIS_SUMMARIZE_PROMPT_TMPL)\n",
    "global_summarize_prompt_template = PromptTemplate(GLOBAL_PRAXIS_SUMMARIZE_PROMPT_TMPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_llm = get_response_synthesizer(llm=llm,\n",
    "                                        response_mode=\"tree_summarize\", \n",
    "                                        summary_template=tree_summarize_prompt_template) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compliance_guidance = \"None\"\n",
    "additional_guidance = \"None\"\n",
    "summary2 = summary_llm.get_response(\"\", \n",
    "                                   [doc.text for doc in nodes],\n",
    "                                   compliance_guidance=compliance_guidance, \n",
    "                                   additional_guidance=additional_guidance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM as a judge\n",
    "judge_prompt = \"\"\"You will be given two summaries. You are to decide which summary is more professional and more informative.\n",
    "Here are the criteria for a professional and informative summary:\n",
    "- The summary should be verbose, making sure to addresses all key points.\n",
    "- The summary should be free of grammatical errors.\n",
    "- The summary should be free of spelling errors.\n",
    "- The summary should be free of unnecessary information.\n",
    "- The summary should be free of personal opinions.\n",
    "- The summary should be free of conversational elements.\n",
    "- The summary should be free of pretext and posttext.\n",
    "- The summary should be free of any additional information not present in the original text.\n",
    "- The summary should not contradict the original text.\n",
    "Be sure to read both summaries carefully before making your decision.\n",
    "Please provide the reasoning for your choice.\n",
    "Format your response as follows:\n",
    "**Best Summary:** [Your choice between Summary 1 and Summary 2]\n",
    "**Reasoning:** [Your reasoning]\n",
    "\\n\\nOriginal text:\\n\\n{original_text}\\n\\nSummary 1:\\n\\n{summary}\\n\\nSummary 2:\\n\\n{summary2}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.complete(judge_prompt.format(original_text=\" \".join([doc.text for doc in nodes]), summary=summary.split('</think>')[-1].strip(), summary2=summary2.split('</think>')[-1].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.text.split('</think>')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary1=\"\"\"The section discusses considerations related to the impact of a proposed regulation on the economy, particularly in the context of the Small Business Regulatory Enforcement Fairness Act of 1996 (SBREFA). It explains that under SBREFA, a rule is considered \"major\" if it meets one or more of the following criteria: \n",
    "\n",
    "1. The regulation results in or is likely to result in an annual effect on the economy of $100 million or more.\n",
    "2. The regulation causes a major increase in costs or prices for consumers or individual industries.\n",
    "3. The regulation has significant adverse effects on competition, investment, or innovation.\n",
    "\n",
    "The text emphasizes that the regulatory body is required to advise the Office of Management and Budget (OMB) whether the proposed regulation qualifies as a \"major rule\" under these criteria. Additionally, it invites public comment on the potential economic impact of the proposed rule on an annual basis, any increases in costs or prices for consumers or industries, and any effects on competition, investment, or innovation. Commenters are encouraged to provide empirical data and factual support to substantiate their views. The section underscores the importance of gathering comprehensive input to assess whether the regulation would meet the thresholds for being classified as a \"major rule\" under SBREFA.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from graph_rag_utils import set_neo4j_password, add_lines_to_conf\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/fsi/.env')\n",
    "\n",
    "NEO4J_PWD = os.getenv('NEO4J_PWD')\n",
    "set_neo4j_password(password=NEO4J_PWD)\n",
    "add_lines_to_conf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* FIRST set password for database `neo4j-admin dbms set-initial-password <PASSWORD>`\n",
    "\n",
    "* THEN make sure the two lines have been added to neo4j.conf\n",
    "\n",
    "* FINALLY start neo4j in terminal or tmux screen `neo4j start`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# apoc plugin needs to go in /var/lib/neo4j/plugins\n",
    "#### NOTE #### neo4j and apoc versions must match!!!!!!\n",
    "# wget https://github.com/neo4j/apoc/releases/download/5.22.0/apoc-5.22.0-core.jar\n",
    "\n",
    "# in vim /etc/neo4j/neo4j.conf add the following lines\n",
    "# dbms.security.procedures.allowlist=apoc.*\n",
    "# dbms.security.procedures.unrestricted=apoc.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Neo4j Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a database dump file (the last argument is the database name)\n",
    "# neo4j-admin database load --from-path=/workspace/data/compliance/ --overwrite-destination neo4j\n",
    "\n",
    "# Start the neo4j service\n",
    "# neo4j console (or neo4j start)\n",
    "\n",
    "import os\n",
    "from typing import Literal\n",
    "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
    "from llama_index.core import PropertyGraphIndex, StorageContext\n",
    "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor, SimpleLLMPathExtractor\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/fsi/.env')\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "neo_url=\"bolt://localhost:7687\"\n",
    "NEO4J_PWD = os.getenv('NEO4J_PWD')\n",
    "graph_store = Neo4jPropertyGraphStore(\n",
    "        username=\"neo4j\",\n",
    "        password=NEO4J_PWD,\n",
    "        url=neo_url,\n",
    "        database=\"neo4j\",\n",
    "    )\n",
    "\n",
    "entities = Literal[\n",
    "    \"REGULATION\",          \n",
    "    \"AGENCY\",              \n",
    "    \"DEFINITION\",          \n",
    "    \"TERM\",                \n",
    "    \"SECTION\",             \n",
    "    \"STAKEHOLDER\",         \n",
    "    \"REQUIREMENT\",         \n",
    "    \"EXCEPTION\",           \n",
    "    \"TIMELINE\",            \n",
    "    \"PENALTY\",             \n",
    "    \"ECONOMIC_IMPACT\",     \n",
    "    \"JUSTIFICATION\",       \n",
    "    \"REQUEST_FOR_COMMENT\", \n",
    "]\n",
    "relations = Literal[\n",
    "    \"REPLACES\",            \n",
    "    \"AMENDS\",              \n",
    "    \"REFERS_TO\",           \n",
    "    \"DEFINES\",             \n",
    "    \"APPLIES_TO\",          \n",
    "    \"HAS_SECTION\",         \n",
    "    \"HAS_REQUIREMENT\",     \n",
    "    \"HAS_EXCEPTION\",       \n",
    "    \"HAS_TIMELINE\",        \n",
    "    \"HAS_PENALTY\",         \n",
    "    \"HAS_JUSTIFICATION\",   \n",
    "    \"HAS_REQUEST_FOR_COMMENT\",  \n",
    "    \"IMPACTS\",             \n",
    "    \"SUPPORTS\",            \n",
    "    \"OPPOSES\",             \n",
    "    \"HAS_ECONOMIC_IMPACT\",  \n",
    "    \"AFFECTS\",              \n",
    "    \"ESTIMATED_BY\",         \n",
    "    \"RELATES_TO\",\n",
    "]\n",
    "validation_schema = {\n",
    "    \"REGULATION\": [\"AMENDS\", \"REPLACES\", \"REFERS_TO\", \"HAS_SECTION\", \"HAS_REQUIREMENT\", \n",
    "        \"HAS_EXCEPTION\", \"HAS_TIMELINE\", \"HAS_PENALTY\", \"HAS_JUSTIFICATION\", \n",
    "        \"HAS_PUBLIC_COMMENT\", \"IMPACTS\", \"HAS_ECONOMIC_IMPACT\", \"AFFECTS\"],\n",
    "    \"AGENCY\": [\"REFERS_TO\"],\n",
    "    \"DEFINITION\": [\"DEFINES\", \"REFERS_TO\"],\n",
    "    \"TERM\": [\"DEFINES\", \"REFERS_TO\"],\n",
    "    \"SECTION\": [\"HAS_SECTION\", \"REFERS_TO\"],\n",
    "    \"REQUIREMENT\": [\"APPLIES_TO\", \"HAS_EXCEPTION\", \"HAS_TIMELINE\", \"HAS_PENALTY\", \"HAS_ECONOMIC_IMPACT\"],\n",
    "    \"ECONOMIC_IMPACT\": [\"AFFECTS\", \"ESTIMATED_BY\", \"REFERS_TO\"],\n",
    "    \"STAKEHOLDER\": [\"IMPACTS\", \"OPPOSES\", \"SUPPORTS\", \"AFFECTS\"],\n",
    "    \"EXCEPTION\": [\"APPLIES_TO\", \"RELATES_TO\"],\n",
    "    \"TIMELINE\": [\"RELATES_TO\"],\n",
    "    \"PENALTY\": [\"RELATES_TO\"],\n",
    "    \"JUSTIFICATION\": [\"SUPPORTS\", \"REFERS_TO\"],\n",
    "    \"REQUEST_FOR_COMMENT\": [\"REFERS_TO\", \"SUPPORTS\", \"OPPOSES\", \n",
    "                            \"AFFECTS\", \"HAS_REQUEST_FOR_COMMENT\", \"RELATES_TO\"],\n",
    "}\n",
    "\n",
    "# Create storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir='/workspace/data/compliance/graph_idx_schema_only',\n",
    "                                               property_graph_store=graph_store)\n",
    "\n",
    "# create graph index\n",
    "\n",
    "kg_extractor = SchemaLLMPathExtractor(\n",
    "    llm=llm,\n",
    "    possible_entities=entities,\n",
    "    possible_relations=relations,\n",
    "    kg_validation_schema=validation_schema,\n",
    "    strict=True,  # if false, will allow triples outside of the schema``\n",
    "    num_workers=7,\n",
    "    max_triplets_per_chunk=10,\n",
    ")\n",
    "\n",
    "graph_index = PropertyGraphIndex.from_existing(\n",
    "                                llm=llm,\n",
    "                                property_graph_store=graph_store, \n",
    "                                embed_model=embed_model, \n",
    "                                storage_context=storage_context,\n",
    "                                kg_extractors=[kg_extractor],\n",
    "                                show_progress=True,\n",
    "                                )\n",
    "\n",
    "# Create a graph engine\n",
    "query_engine = graph_index.as_query_engine(\n",
    "    # llm=llm,\n",
    "    similarity_top_k=20,\n",
    "    node_postprocessors=[\n",
    "        LLMRerank(\n",
    "            choice_batch_size=5,\n",
    "            top_n=10,\n",
    "        )\n",
    "    ],\n",
    "    # see https://github.com/run-llama/llama_index/blob/f7c5ee5efbb6172e819f26d1705fcdf6114b11a3/llama-index-core/llama_index/core/response_synthesizers/type.py#L4\n",
    "    response_mode=\"tree_summarize\", # \"accumulate\", \"compact_accumulate\", \"compact\", \"simple_summarize\", \"tree_summarize\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Neo4j Database from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph from scratch\n",
    "# GraphRAG Database\n",
    "import os\n",
    "from typing import Literal\n",
    "\n",
    "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "from rag_utils import create_neo4j_graph_store, dump_neo4j_database\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/fsi/.env')\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "graph_idx_persist_dir = \"/workspace/data/compliance/graph_idx_schema_only\"\n",
    "\n",
    "entities = Literal[\n",
    "    \"REGULATION\",          \n",
    "    \"AGENCY\",              \n",
    "    \"DEFINITION\",          \n",
    "    \"TERM\",                \n",
    "    \"SECTION\",             \n",
    "    \"STAKEHOLDER\",         \n",
    "    \"REQUIREMENT\",         \n",
    "    \"EXCEPTION\",           \n",
    "    \"TIMELINE\",            \n",
    "    \"PENALTY\",             \n",
    "    \"ECONOMIC_IMPACT\",     \n",
    "    \"JUSTIFICATION\",       \n",
    "    \"REQUEST_FOR_COMMENT\", \n",
    "]\n",
    "relations = Literal[\n",
    "    \"REPLACES\",            \n",
    "    \"AMENDS\",              \n",
    "    \"REFERS_TO\",           \n",
    "    \"DEFINES\",             \n",
    "    \"APPLIES_TO\",          \n",
    "    \"HAS_SECTION\",         \n",
    "    \"HAS_REQUIREMENT\",     \n",
    "    \"HAS_EXCEPTION\",       \n",
    "    \"HAS_TIMELINE\",        \n",
    "    \"HAS_PENALTY\",         \n",
    "    \"HAS_JUSTIFICATION\",   \n",
    "    \"HAS_REQUEST_FOR_COMMENT\",  \n",
    "    \"IMPACTS\",             \n",
    "    \"SUPPORTS\",            \n",
    "    \"OPPOSES\",             \n",
    "    \"HAS_ECONOMIC_IMPACT\",  \n",
    "    \"AFFECTS\",              \n",
    "    \"ESTIMATED_BY\",         \n",
    "    \"RELATES_TO\",\n",
    "]\n",
    "validation_schema = {\n",
    "    \"REGULATION\": [\"AMENDS\", \"REPLACES\", \"REFERS_TO\", \"HAS_SECTION\", \"HAS_REQUIREMENT\", \n",
    "        \"HAS_EXCEPTION\", \"HAS_TIMELINE\", \"HAS_PENALTY\", \"HAS_JUSTIFICATION\", \n",
    "        \"HAS_PUBLIC_COMMENT\", \"IMPACTS\", \"HAS_ECONOMIC_IMPACT\", \"AFFECTS\"],\n",
    "    \"AGENCY\": [\"REFERS_TO\"],\n",
    "    \"DEFINITION\": [\"DEFINES\", \"REFERS_TO\"],\n",
    "    \"TERM\": [\"DEFINES\", \"REFERS_TO\"],\n",
    "    \"SECTION\": [\"HAS_SECTION\", \"REFERS_TO\"],\n",
    "    \"REQUIREMENT\": [\"APPLIES_TO\", \"HAS_EXCEPTION\", \"HAS_TIMELINE\", \"HAS_PENALTY\", \"HAS_ECONOMIC_IMPACT\"],\n",
    "    \"ECONOMIC_IMPACT\": [\"AFFECTS\", \"ESTIMATED_BY\", \"REFERS_TO\"],\n",
    "    \"STAKEHOLDER\": [\"IMPACTS\", \"OPPOSES\", \"SUPPORTS\", \"AFFECTS\"],\n",
    "    \"EXCEPTION\": [\"APPLIES_TO\", \"RELATES_TO\"],\n",
    "    \"TIMELINE\": [\"RELATES_TO\"],\n",
    "    \"PENALTY\": [\"RELATES_TO\"],\n",
    "    \"JUSTIFICATION\": [\"SUPPORTS\", \"REFERS_TO\"],\n",
    "    \"REQUEST_FOR_COMMENT\": [\"REFERS_TO\", \"SUPPORTS\", \"OPPOSES\", \n",
    "                            \"AFFECTS\", \"HAS_REQUEST_FOR_COMMENT\", \"RELATES_TO\"],\n",
    "}\n",
    "\n",
    "kg_extractor = SchemaLLMPathExtractor(\n",
    "    llm=llm,\n",
    "    possible_entities=entities,\n",
    "    possible_relations=relations,\n",
    "    kg_validation_schema=validation_schema,\n",
    "    strict=True,  # if false, will allow triples outside of the schema\n",
    "    num_workers=7,\n",
    "    max_triplets_per_chunk=10,\n",
    ")\n",
    "\n",
    "# llm.is_function_calling_model = False\n",
    "# extract_prompt = None\n",
    "# kg_extractor = SimpleLLMPathExtractor(\n",
    "#         extract_prompt=extract_prompt,\n",
    "#         llm=llm,\n",
    "#         max_paths_per_chunk=10,\n",
    "#         num_workers=6,\n",
    "#     )\n",
    "\n",
    "print(\"Creating graph store...\")\n",
    "graph_store = create_neo4j_graph_store(neo_url=\"bolt://localhost:7687\", \n",
    "                                       password=os.getenv(\"NEO4J_PWD\"), \n",
    "                                       config={\"connection_timeout\": 1000, \"connection_acquisition_timeout\": 1000, \"max_connection_pool_size\": 1000})\n",
    "\n",
    "# if not os.path.exists(graph_idx_persist_dir):\n",
    "#     print(\"Deleting all nodes and relationships...\")\n",
    "#     neo4j_query(graph_store, query=\"\"\"MATCH n=() DETACH DELETE n\"\"\")\n",
    "\n",
    "print(\"Creating graphrag index...\")\n",
    "# graph_index = create_neo4j_graphrag(documents_rule_proposal, llm, embed_model, kg_extractor, graph_store, graph_idx_persist_dir=graph_idx_persist_dir, graph_store_persist_dir=graph_store_persist_dir)\n",
    "\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "# nodes are created from semantic splitter\n",
    "graph_index = PropertyGraphIndex(nodes,\n",
    "                                 llm=llm,\n",
    "                                 property_graph_store=graph_store,\n",
    "                                 kg_extractors=[kg_extractor],\n",
    "                                 embed_model=embed_model,\n",
    "                                 show_progress=True,\n",
    "                                 )\n",
    "\n",
    "graph_index.storage_context.persist(persist_dir=graph_idx_persist_dir)\n",
    "\n",
    "\n",
    "# dump_neo4j_database('neo4j', '/workspace/data/') # database needs to be stopped before running this command\n",
    "# http://localhost:7474/browser/\n",
    "# 22 mins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "query_engine = graph_index.as_query_engine(\n",
    "    # llm=llm,\n",
    "    similarity_top_k=20,\n",
    "    node_postprocessors=[\n",
    "        LLMRerank(\n",
    "            choice_batch_size=5,\n",
    "            top_n=10,\n",
    "        )\n",
    "    ],\n",
    "    # see https://github.com/run-llama/llama_index/blob/f7c5ee5efbb6172e819f26d1705fcdf6114b11a3/llama-index-core/llama_index/core/response_synthesizers/type.py#L4\n",
    "    response_mode=\"tree_summarize\", # \"accumulate\", \"compact_accumulate\", \"compact\", \"simple_summarize\", \"tree_summarize\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_neo4j_database('neo4j', '/workspace/data/compliance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt = \"\"\"Below is a section of a newly proposed federal regulation.\n",
    "Your task is to produce a precise summary of the given section, highlighting all major key points.\n",
    "Be very verbose in your summary, ensuring that you capture all the essential details and requirements.\n",
    "Use the context above to incorporate any relevant global document information.\n",
    "Use exact numeric values, specific costs, or other details where necessary.\n",
    "You must strictly follow the guidance provided by the expert compliance officer:\n",
    "\n",
    "Compliance Officer Guidelines:\n",
    "{compliance_guidance}\n",
    "{additional_guidance}\n",
    "\n",
    "SECTION TO SUMMARIZE:\n",
    "{section_str}\n",
    "\n",
    "\n",
    "SUMMARY:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_idx = 7\n",
    "compliance_guidance = \"\"\n",
    "additional_guidance = \"\"\n",
    "metadata_str = \"metadata: {metadata}\\n\".format(metadata=str(documents_rule_proposal[section_idx].metadata))\n",
    "# query = tree_summarize_prompt_template.template.format(compliance_guidance=compliance_guidance, additional_guidance=additional_guidance, section_str=documents_rule_proposal[section_idx].text)\n",
    "query = summary_prompt.format(compliance_guidance=compliance_guidance, additional_guidance=additional_guidance, section_str=metadata_str+documents_rule_proposal[section_idx].text)\n",
    "response = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents_rule_proposal[section_idx].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in response.source_nodes:\n",
    "    print(source.metadata)\n",
    "    print(source.text)\n",
    "    print('--'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with smaller text segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from rag_utils import create_llama_vector_index_rag\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser, SentenceSplitter\n",
    "\n",
    "# splitter = SemanticSplitterNodeParser(buffer_size=1, embed_model=embed_model, include_metadata=True)\n",
    "# TODO: add tokenizer\n",
    "persist_dir = '/workspace/data/compliance/vector_index'\n",
    "splitter = SentenceSplitter(chunk_size=256, chunk_overlap=32, include_metadata=True, tokenizer=None)\n",
    "documents_rule_proposal = [Document(text=section_text, metadata={'section_title':title}) for title, section_text in relevant_sections_dict.items()]\n",
    "nodes = splitter.get_nodes_from_documents(documents_rule_proposal, show_progress=True)\n",
    "vector_index = VectorStoreIndex(nodes, \n",
    "                                llm=llm,\n",
    "                                embed_model=embed_model, \n",
    "                                show_progress=True,\n",
    "                                )\n",
    "# vector_index = create_llama_vector_index_rag(llm, embed_model=embed_model, persist_dir='/workspace/data/compliance/vector_index', documents=documents_rule_proposal, vector_store_kwargs={'chunk_size':256, 'chunk_overlap':32})\n",
    "\n",
    "if not os.path.exists(persist_dir):\n",
    "    print(f\"Persisting vector index to {persist_dir}\")\n",
    "    vector_index.storage_context.persist(persist_dir=persist_dir)\n",
    "else:\n",
    "    print(f\"Vector index already exists at {persist_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores.types import MetadataFilters, ExactMatchFilter\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "\n",
    "# Define the metadata filter\n",
    "metadata_filters = MetadataFilters(\n",
    "    filters=[\n",
    "        ExactMatchFilter(key=\"section_title\", value=\"VIII. FinCEN's Regulatory Impact Analysis\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "rag_query_engine = vector_index.as_query_engine(\n",
    "    llm=llm,\n",
    "    similarity_top_k=10,\n",
    "    # node_postprocessors=[\n",
    "    #     LLMRerank(\n",
    "    #         llm=llm,\n",
    "    #         choice_batch_size=5,\n",
    "    #         top_n=10,\n",
    "    #     )\n",
    "    # ],\n",
    "    # response_mode=\"tree_summarize\", \n",
    "    filters=metadata_filters\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_for_checking = \"\"\"The proposed federal regulation under FinCEN's Regulatory Impact Analysis aims to assess the costs and benefits of the regulation. The regulation emphasizes the importance of quantifying costs and benefits, reducing costs, harmonizing rules, and promoting flexibility. It has been designated as a significant regulatory action and reviewed by the Office of Management and Budget. The primary costs of compliance with the proposed rule are detailed in the Analysis of the Costs and Benefits Associated with the Proposed Rule, with estimated annual internal time costs of $404,045,339.05 and external cost burden of $48,446,969.76. The benefits of the rule are expected to include reducing money laundering and terrorist financing in the U.S. financial system, aiding law enforcement in investigating and disrupting financial crimes. The rule would help in identifying high-risk customers and preventing criminal activities. While the economic losses prevented by reducing financial crimes are difficult to estimate, the rule would reduce both monetary and nonmonetary harms caused by such activities. The rule requires investment advisers to establish and implement a Customer Identification Program (CIP) based on their specific circumstances, with requirements similar to those for other financial institutions. Some advisers may have reduced costs if they already perform certain Anti-Money Laundering/Counter Financing of Terrorism (AML/CFT) functions or are affiliated with banks or broker-dealers. The costs incurred by the rule include establishing a CIP, verifying identifying information, checking customers against government lists, recordkeeping, and reliance on other financial institutions. FinCEN estimates that the average compliance costs for an ERA with three customers would be $1,675 internally and $654 externally, while for an RIA with 100 customers, it would be $26,468 internally and $4,088 externally. Overall, FinCEN believes that the benefits of the rule would outweigh the costs, contributing to a more secure financial system.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_fact_checking_prompt = \"\"\"Below is a summary that need to be fact-checked.\n",
    "ONLY use the information retrieved in the context above to determine if the facts presented in the summary are true.\n",
    "Verify all numeric values, numeric values in word form, and other factual information to ensure accuracy.\n",
    "Do not use any pre-existing knowledge.\n",
    "If there are any factual mistakes, rewrite the incorrect portion of the summary and include the correct information, and output in this format:\n",
    "- Original summary statement: [Incorrect information]\n",
    "- Corrected summary statement: [Corrected information] \n",
    "- Supporting Context: [Section context only]\n",
    "If the entire summary is factually correct, output \"None\".\n",
    "Only output the portions of the summary that are incorrect. Do not include any pre-text or post-text.\n",
    "\n",
    "SUMMARY TO FACT-CHECK:\n",
    "{statement}\n",
    "\n",
    "FACT-CHECKING RESPONSE:\"\"\"\n",
    "\n",
    "\n",
    "response = rag_query_engine.query(summary_fact_checking_prompt.format(statement=summary_for_checking))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.response.split('</think>')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in response.source_nodes:\n",
    "    print(source.metadata)\n",
    "    print(source.text)\n",
    "    print('--'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_for_checking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_idx=7\n",
    "print(documents_rule_proposal[section_idx].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
